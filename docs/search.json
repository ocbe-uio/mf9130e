[
  {
    "objectID": "course_material/course_material_overview.html",
    "href": "course_material/course_material_overview.html",
    "title": "Course material",
    "section": "",
    "text": "On this page you’ll find a list of material used in this course.\nFor the exercises and lab notes we use during the lab sessions, please check the R Lab and Code."
  },
  {
    "objectID": "course_material/course_material_overview.html#week-1",
    "href": "course_material/course_material_overview.html#week-1",
    "title": "Course material",
    "section": "Week 1",
    "text": "Week 1\n\n\n\nTime\nTopic\nLecture notes\nLab\n\n\n\n\nApril 24 PM\nCourse introduction\nSlides\n\n\n\n\nDescriptive statistics\nSlides, K&S chapter 2-4, Aalen chapter 1-2\nPaper 1, Paper 2, Paper 3\n\n\nApril 25 AM\nIntroduction to R and Rstudio\nSlides\nIntro to RStudio, Intro to R\n\n\n\nLab session\nSlides\nDescriptive statistics (EDA I)\n\n\nApril 25 PM\nProbability, diagnosistic tests\nIntro slides Probability Diagnostic tests\n\n\n\n\nStatistical distributions\nDistributions\n\n\n\nApril 26 AM\nLab session\n\nCOVID-19 tests, Simulations\n\n\nApril 26 PM\nStatistical inference, confidence intervals, t-test\nOverview, Sample mean, Confidence interval, Hypothesis testing\n\n\n\nApril 27 AM\nLab session\nSlides\nt-test\n\n\nApril 27 PM\nCategorical data, proportions, table analysis\nOverview, Proportions, Compare proportions, Table analysis\n\n\n\nApril 28 AM\nLab session\nSlides\nCategorical data analysis"
  },
  {
    "objectID": "course_material/course_material_overview.html#week-2",
    "href": "course_material/course_material_overview.html#week-2",
    "title": "Course material",
    "section": "Week 2",
    "text": "Week 2\n\n\n\nTime\nTopic\nLecture notes\nLab\n\n\n\n\nMay 8 AM\nExploratory data analysis (part II)\nLecture\nEDA II\n\n\n\nNon-parametric methods\nLecture\nNon-parametric tests\n\n\nMay 8 PM\nSample size, statistical power\nLecture\nExercises\n\n\nMay 9 AM\nStudy design, principles of clinical trials\nLecture\n\n\n\nMay 9 PM\nLinear regression I\nLecture\nLinear regression I\n\n\nMay 10 AM\nLinear regression II\nLecture\nLinear regression II\n\n\nMay 10 PM\nLinear regression III\nLecture Shmueli_2019_extract.pdf\nLinear regression III\n\n\nMay 11 AM\nLogistic regression\nLecture\nLogistic regression\n\n\nMay 11 PM\nSurvival analysis\nSurvival, Summary\nSurvival analysis"
  },
  {
    "objectID": "lab/lab_eda_part1.html",
    "href": "lab/lab_eda_part1.html",
    "title": "Exploratory data analysis (Part I)",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-1-weight",
    "href": "lab/lab_eda_part1.html#exercise-1-weight",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 1 (weight)",
    "text": "Exercise 1 (weight)\n\n1a\nGenerate a variable named weight, with the following measurements:\n50 75 70 74 95 83 65 94 66 65 65 75 84 55 73 68 72 67 53 65\n\n\n1b\nMake a simple descriptive analysis of the variable, what are the mean, standard deviation, maximum, minimum and quantiles?\nHow to interpret the data?\n\n\n1c\nMake a histogram.\n\n\n1d\nMake a boxplot. What do the two dots on the top represent?"
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-2-lung-function",
    "href": "lab/lab_eda_part1.html#exercise-2-lung-function",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nDownload and open PEFH98-english.dta into R.\nIf you have problem with .dta data format, you can also use PEFH98-english.csv.\nPay attention to how gender is coded. We might have to modify it.\n\n\n2b)\nHow many observations are there (number of subjects)? How do you get a list of variable names from your dataset?\nMake a histogram for each of the following variables. Compute means, and interpret the results.\nheight\nweight\nage\npefsitm\npefstam\n\n\n2c)\nMake histograms for the variables height and pefmean for men and women separately. Also try to make boxplots.\nWhat conclusion can you draw?\n\n\n2d)\nMake three scatterplots to compare\n\npefmean with height\npefmean with weight\npefmean with age\n\nWhat association do you see?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the course",
    "section": "",
    "text": "The aim of the course is to make the participants acquainted with basic statistical ideas and methods. No special previous knowledge of mathematics or statistics is assumed. The statistical software R and the RStudio environment will be used in many of the exercises. Analysis of examples from biomedical research will be emphasized.\n\n\n\n\n\n\nUse of the course material\n\n\n\nThis course website was built primarily with students taking MF9130E in mind, therefore the topics and examples are tailored to matche the existing teaching material we had been using at UiO.\nHowever, the course material and website are open source and fully publicly accessible. We advocate strongly for making learning statistics more accessible to everyone. Feel free to spread the words! And do let us know if you benefit from the course and the website :)\nPlease contact Chi Zhang chi.zhang@medisin.uio.no if you have questions and feedbacks.\n\n\n\nContact 2024 team\n\nValeria Vitelli: valeria.vitelli@medisin.uio.no\nChi Zhang: chi.zhang@medisin.uio.no\n\nPlease contact Chi regarding the development of the website."
  },
  {
    "objectID": "lab/overview.html",
    "href": "lab/overview.html",
    "title": "R Lab and Code",
    "section": "",
    "text": "Welcome! Here you will find material for R lab and code for this course."
  },
  {
    "objectID": "lab/overview.html#useful-resources",
    "href": "lab/overview.html#useful-resources",
    "title": "R Lab and Code",
    "section": "Useful resources",
    "text": "Useful resources\nList of commands that are useful for this course: list of commands\nBook (Wickham et al) R for Data Science (2e)\nDemo of webR"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MF9130E - Introductory course in Statistics",
    "section": "",
    "text": "Welcome!\nYou are on the course website for MF9130E - Introductory Course in Statistics at the Faculty of Medicine, University of Oslo.\n\nThe course is intended for students and researchers who are interested in statistics and R programming, with applications in medical and healthcare data. No previous programming experience is required to participate in this course.\nThis website is developed by the instructors of the course, hosted for free and public access on Github. The course github repository can be accessed here.\nYou can check the course page by UiO for information related to applications, evaluations and other administrative matters.\nAbout home exam: information regarding home exam will be talked about in the class on the first and last days. Assignment tasks for home exam will be put in Canvas, and you should submit your assignment in Inspera. More information about the exam\n\n\n\nSchedule\nYou can find the official course schedule provided by University of Oslo here. If there is an error in the time and place on this page, please refer to the official schedule.\n\nPreparation\nYou should have a working solution of R (either installed on your own laptop, or using Posit cloud) before the course.\nPlease familiarize yourself with the course website.\n\nGet Started provides some information about software installation, where to download data and code, and some resources.\nCourse material provides an overview of the material and corresponding links for each session.\nR Lab and Code hosts the lab session notes.\n\n\n\n\n\n\n\nUse of the course material\n\n\n\nThis course website was built primarily with students taking MF9130E in mind, therefore the topics and examples are tailored to matche the existing teaching material we had been using at UiO.\nHowever, the course material and website are open source and fully publicly accessible. We advocate strongly for making learning statistics more accessible to everyone. Feel free to spread the words! And do let us know if you benefit from the course and the website :)\nPlease contact Chi Zhang chi.zhang@medisin.uio.no if you have questions and feedbacks."
  },
  {
    "objectID": "course_material/notes_probability.html",
    "href": "course_material/notes_probability.html",
    "title": "Probability",
    "section": "",
    "text": "Topics:\nBooks and resources:"
  },
  {
    "objectID": "course_material/notes_probability.html#basic-concepts",
    "href": "course_material/notes_probability.html#basic-concepts",
    "title": "Probability",
    "section": "Basic concepts",
    "text": "Basic concepts\nA probability expresses a potential for something to happen.\nIt is an assessment of uncertainty in a situation or event.\nIt corresponds to the concept of risk in medicine.\n\nBrief history\nBlaise Pascal (17th century) was the founder of probability theory, the set of basic rules for doing probability calculations. His work was motivated by dice and card games.\nAndrey Kolmogorov formulated the exact probability rules as late as 1933.\n\n\nTwo definitions of probability\nFrequentist definition: Proportions of times (or frequency) that some event occurs in a large number of similar repeated trials.\nBayesian definition: Degree of belief in the occurrence of an event.\n\n\n\n\n\n\nObservation\n\n\n\n\n\nIn this course we focus in the frequentist definition, the most widely used. However, we will see at the end of this section that the Bayesian definition is important as it constitutes the foundation of the Bayesian approach to statistics, after Thomas Bayes (18th century).\n\n\n\n\n\nLaw of Large Numbers\n“As an experiment is repeated over and over, the observed frequency approaches the true probability”.\n\n\n\n\n\n\nExample: Coin tossing\n\n\n\n\n\nThis figure shows the frequency of heads in up to 200 simulated coin tosses. We can see that the frequency approaches the value 0.5 as the number of tosses grows.\n\n\n\n\nThe frequentist view of probability interprets the frequency of an event in a large number of experiments as its probability.\n\n\n\n\n\n\nExample: Births in Norway\n\n\n\n\n\nGiving birth to a girl or a boy is perceived as a random event. This table contains the frequencies of occurrence of the event “giving birth to a girl” in Norway during 2019-2022\n\nSource: Statistisk sentralbyrå\n\n\n\n\n\n\n\n\n\nYear\nTotal number of births\nNumber of boys\nNumber of girls\nPercentage of girls\n\n\n\n\n2022\n51480\n26445\n25035\n48.6\n\n\n2021\n56060\n28684\n27376\n48.8\n\n\n2020\n52979\n27063\n25916\n48.9\n\n\n2019\n54495\n28042\n26453\n48.5\n\n\n\nThe probability of giving birth to a girl in Norway is approximately 0.49"
  },
  {
    "objectID": "course_material/notes_probability.html#probability-calculations",
    "href": "course_material/notes_probability.html#probability-calculations",
    "title": "Probability",
    "section": "Probability calculations",
    "text": "Probability calculations\n\nStochastic trial, events and sample space.\nA stochastic trial is characterized by an uncertain outcome.\nAll possible outcomes in a stochastic trial make up the sample space.\nAn event can be a single outcome, or a collection of single outcomes.\nEach event has a probability of ocurrence between 0 and 1. A probability equal to 0 means that the event can never occur, and equal to 1 means that the event is certeinly occuring.\nThe sum of all probabilities in a sample space equals 1.\n\n\n\n\n\n\nExamples: Stochastic trials\n\n\n\n\n\nDice tossing\n\nSample space: {1,2,3,4,5,6}\nEvents:\n\nEven number of eyes: {2,4.6}\nMore than 3 eyes: {4,5,6}\n\n\nChild birth\n\nSample space: {B,G}\nEvents:\n\nHaving a girl: {G}\nNot having a girl: {B}\n\n\nDiastolic blood pressure\n\nSample space: {40,41,…,119,120}\nEvent:\n\nHypertension: {91,92, …, 120}\n\n\n\n\n\n\n\nVenn diagram\nVenn diagrams are often used to illustrate events. In the figures below A and B represent different events and S is the sample space.\n\n\n\n\n\n\n\n\n\n\n\n\nOperators on events:\n\nUnion: \\(A \\cup B\\)\nIntersection: \\(A \\cap B\\)\nComplement: \\(\\bar{A}\\)\n\n\n\n\n\n\n\n\n Union: \\(A \\cup B\\)\n Intersection: \\(A \\cap B\\)\n\n\n\n\n\n\n\n\n\n\n Complement: \\(\\bar{A}\\)\n Combining operators: \\(A \\cap \\bar{B}\\)\n\n\n\n\n\nProbability calculation rules\nThe probability of an event \\(A\\) is denoted by \\(P(A)\\). It has a value between 0 and 1. The probability over the whole sample space equals 1.\nComplement rule\n\\[P(A) + P(\\bar{A}) = 1\\]\nAdditive rule\nThe occurrence of at least one of the events \\(A\\) or \\(B\\) is \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] For disjoint events \\(A\\) and \\(B\\), \\(P(A \\cap B) = 0\\). Hence \\[P(A \\cup B) = P(A) + P(B)\\]\nMultiplicative rule\nProbability of independent events \\(A\\) and \\(B\\) can be multiplied\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\n\n\n\n\nExample: Child gender in two births\n\n\n\n\n\nWhat is the probability of giving birth to at least one girl?\n\nWe assume the probability of giving birth to a girl is 0.5.\nWe assume that the two births are independent events.\n\n\\[\n\\begin{aligned}\nP(\\text{at least one girl}) & =P(\\text{1st child is girl}) + P(\\text{2nd child is girl})-P(\\text{both are girls}) \\\\\n& = 1/2 + 1/2 - 1/2 \\times 1/2 = 3/4\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nExample: Tossing two dices\n\n\n\n\n\n\nThe two tosses are independent events.\nThere are 36 equally possible outcomes.\nOnly one outcome corresponds to the double 6 event.\n\nWhat is the probability of getting 6 in the first AND the second throw? \\[\nP(\\text{6 in the first AND the second throw}) = 1/6 \\times 1/6 = 1/36\n\\]\nWhat is the probability of getting at least one 6 in two throws? \\[\n\\begin{aligned}\n    P(\\text{at least one 6 in 2 trows}) &=P(\\text{6 in the first}) +         P(\\text{6 in the second}) - P(\\text{6 in both})\\\\\n    &=1/6 +1/6 - 1/36 =0.31\n\\end{aligned}\n\\]\nWhat is the probability of not getting any 6 in two throws? \\[\n\\begin{aligned}\n    P(\\text{not getting any 6 in 2 trows}) &= 1 - P(\\text{at least one 6 in 2 trows})\\\\\n    &= 1 - 0.31 = 0.69\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#conditional-probability",
    "href": "course_material/notes_probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhat is the probability of getting the outcome \\(A\\) given that the event \\(B\\) has occur? For example, what is the risk of becoming sick from COVID-19 given that your spouse already is?\nThe idea to define such a conditional probability of \\(A\\) given \\(B\\), denoted \\(P(A|B)\\), is to consider \\(B\\) as the new sample space and rescale the probability of events in \\(B\\), such that the new sample space has probability 1:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\n\n\nExercise: Weather forecast\n\n\n\n\n\nThis table contains the frequency of joint events: weather forecast and actual weather.\n\n\n\n\nsunny forecast\ncloudy forecast\nrainy forecast\n\n\nsunny weather\n0.3\n0.05\n0.05\n\n\ncloudy weather\n0.04\n0.2\n0.02\n\n\nrainy weather\n0.1\n0.04\n0.2\n\n\n\n\nWhich is the probability of a sunny day?\nWhich is the probability that the forecast is wrong?\nWhich is the probability of rain when the forecast is sunny?\n\nHint: remember to use the special case of the additive rule, holding when A and B are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\nStochastic independence\nThe events \\(A\\) and \\(B\\) are independent if \\(P(A|B) = P(A)\\)\nInterpretation: probability of \\(A\\) is the same if we also know that \\(B\\) has occurred.\n\n\n\n\n\n\nExamples: independence\n\n\n\n\n\nConsider the following:\n\n\\(A\\): Probability of me having the condition\n\\(B\\): Probability of my partner having the same condition\n\nCondition 1: Diabetes, \\(P(A|B) = P(A)\\)\nCondition 2: COVID-19, \\(P(A|B) \\neq P(A)\\)\n\n\n\nProbability calculations can be simplified if there is stochastic independence:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = P(A)\\]\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\n\n\n\n\nExample: Child birth (revisited)\n\n\n\n\n\nTwo children are born, which is the probability that genders are different?\n\\[\\begin{aligned}\nP(\\text{girl and boy}) & =P(\\text{girl then boy}) + P(\\text{boy then girl})\\\\\n& = P(\\text{first is girl}) \\times P(\\text{second is boy}) +\\\\\n&~~~~+ P(\\text{first is boy}) \\times P(\\text{second is girl})\\\\\n& = 1/2 \\times 1/2 + 1^/2 \\times 1/2 = 0.50\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nExample: Dices (revisited)\n\n\n\n\n\nThree dices are tossed, which is the probability of all showing sixes?\n\\[P(\\text{three 6})=1/6 + 1/6 + 1/6 = 1/216 = 0.004\\]\n\n\n\n\n\n\n\n\n\nExercise: Side effects of medication\n\n\n\n\n\nA treatment has a side effect which has a risk of 1/1000. Which is the probability that the side effect does not occur among 1000 patients?\nHint: think about independence."
  },
  {
    "objectID": "course_material/notes_probability.html#total-probability",
    "href": "course_material/notes_probability.html#total-probability",
    "title": "Probability",
    "section": "Total probability",
    "text": "Total probability\nThe law of total probability expresses the probability of an outcome, \\(P(A)\\), which can be realized via two distinct events \\(B\\) and \\(\\bar{B}\\):\n\\[P(A) = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\\]\n\n\n\n\n\n\nDerivation of the law of total probability\n\n\n\n\n\nAny event \\(A\\) can be divided in two with regard to another event \\(B\\):\n\\[A = (A \\cap B) \\cup (A \\cap \\bar{B})\\]\n\nBecase the two events \\((A \\cap B)\\) and \\((A \\cap \\bar{B})\\) are disjunct, we can write:\n\\[\nP(A) = P(A \\cap B) + P(A \\cap \\bar{B})\n\\] Using the multiplicative rule, we get the law of total probability:\n\\[\nP(A) = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\n\\]\n\n\n\n\n\n\n\n\n\nExample: Gender of twins\n\n\n\n\n\n\nWe want to find the probability of two twins having the same gender.\nMonozygotic twins have the same gender, while dizygotic twins are like any other siblings.\nWe have to take into consideration if the twins are monozygotic or not. For that we use the law of total probability.\n\n\\(A\\) = Both twins have the same gender.\n\\(B\\) = The twins are monozygotic.\n\nWe want to find \\(P(A)\\)\nWe assume that the probability of twins being monozygotic, \\(P(B)\\), is 1/3.\nThe law of total probability give us:\n\n\\[\n\\begin{aligned}\nP(A) & = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\\\\\n& = 1 \\cdot 1/3 + 1/2 \\cdot 2/3 = 0.67\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#bayes-theorem-also-called-bayes-law",
    "href": "course_material/notes_probability.html#bayes-theorem-also-called-bayes-law",
    "title": "Probability",
    "section": "Bayes’ theorem (also called Bayes’ law)",
    "text": "Bayes’ theorem (also called Bayes’ law)\nGiven two events \\(A\\) and \\(B\\), Bayes theorem states that:\n\\[\nP(B|A)=\\frac{P(A|B)P(B)}{P(A)}\n\\]\n\n\n\n\n\n\nExplain\n\n\n\n\n\nIt was first formulated by Thomas Bayes (1702-1761)\nBayes law relates the conditional probabilities \\(P(B|A)\\) and \\(P(A|B)\\)\n\n\n\n\n\n\n\n\n\nExample Gender of twins (revisited)\n\n\n\n\n\nWhat if we want to find the probability that two twins of the same gender are monozygotic?\nIn other words, what is \\(P(B|A)\\)?\nWe can use Bayes’ law and the law of total probability\n\\[\n\\begin{aligned}P(B|A) & =\\frac{P(A|B)P(B)}{P(A)}\\\\       & =\\frac{P(A|B)P(B)}{P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})}\\\\       & = \\frac{1 \\cdot 1/3}{1 \\cdot 1/3 + 1/2 \\cdot 2/3} = 0.5\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#bayesian-statistics",
    "href": "course_material/notes_probability.html#bayesian-statistics",
    "title": "Probability",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nIn the Bayesian definition of probability, the size of the probability of a given event represents ones degree of belief in the occurrence of the even.\nWhere does Bayes come in? Bayes’ law is used to calculate such probabilities base on on our prior belief and available data:\n\\[\nP(\\theta|data) =\\frac{P(data|\\theta)P(\\theta)}{P(data)}\n\\]\n\n\\(\\theta\\) refers to the parameters in your model (mean, variance, etc.)\nThe prior distribution \\(P(\\theta)\\) is where you put in your prior beliefs\nWhat you want to estimte is the so called posterior distribution \\(P(\\theta|data)\\), the probability distribution of the model parameters given your data.\nThe more data you have, the more will it dominate over your prior belief.\nWhen youy have prior knowledge about your problem, you get to actually use this information\nWhen you know little (or nothing) of your problme, there are anyway many methodological advantages in using Bayesian statistics\nBayesian statistics is not really relevant for simpler problems like in this course, but you will probaly at some point come across articles using Bayesian approaches."
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html",
    "href": "course_material/notes_diagnostic_tests.html",
    "title": "Evaluation of Diagnostic Tests",
    "section": "",
    "text": "Topics:\nBook and resources:"
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html#confusion-matrix",
    "href": "course_material/notes_diagnostic_tests.html#confusion-matrix",
    "title": "Evaluation of Diagnostic Tests",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nDenote the subjects with the condition as \\(P\\), and subjects without the condition as \\(N\\).\nTotal population: \\(P+N\\)\nPrevalence (of the condition) is defined as: \\(\\frac{P}{P+N}\\)\n\n\n\n\n\n\n\n\n\n\nPredicted Positive\nPredicted Negative\nTotal\n\n\n\n\nWith condition\nTrue Positive TP\nFalse Negative FN\n\\(P = TP + FN\\)\n\n\nWithout condition\nFalse Positive FP\nTrue Negative TN\n\\(N=FP + TN\\)\n\n\nTotal\n\\(P_{\\text{predicted}}=TP+FP\\)\n\\(N_{\\text{predicted}}=FN + TN\\)\n\n\n\n\nSensitivity of a diagnostic test is the probability of revealing that a person has the condition. It is also known as true positive rate (TPR), as it is the proportion of true positives.\n\\[\\text{Sensitivity}= \\frac{TP}{P} = \\frac{TP}{TP+FN}\\]\nSpecificity is the probability of revealing that a person does not have the condition (i.e. healthy). It is also known as true negative rate (TNR), as it is the proportion of true negatives.\n\\[\\text{Specificity} = \\frac{TN}{N} = \\frac{TN}{TN + FP}\\]\nPositive predictive value (PPV): probability that the person with the condition was given a positive test result\n\\[PPV = \\frac{TP}{P_{predicted}} = \\frac{TP}{TP + FP}\\]\nNegative predictive value (NPV): probability that the person without the condition (i.e. healthy) was give a negative test result.\n\\[NPV = \\frac{TN}{N_{predicted}} = \\frac{TN}{TN + FN}\\]\n\n\n\n\n\n\nExample: Mammography\n\n\n\n\n\n(From the Norwegian Medical Journal, 1990) 372 women with a lump in the breast initially classified as malign or benign were referred to a surgical clinic for a final diagnosis.\n\n\n\n\nMammography malign\nMammography benign\n\n\n\n\nFinal diagnosis malign\n22\n3\n\n\nFinal diagnosis benign\n16\n331\n\n\n\nWe identify the positive test result (mammography malign), and the positive condition (final diagnosis malign). Then we can compute the four following probabilities from the table:\nSensitivity: \\(22/(22+3) = 0.88\\)\nSpecificity: \\(331/(331+16) = 0.95\\)\nPositive predictive value: \\(22/(22+16) = 0.58\\)\nNegative predictive value: \\(331/(331+3) = 0.99\\)"
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html#diagnostic-tests-and-prevalence",
    "href": "course_material/notes_diagnostic_tests.html#diagnostic-tests-and-prevalence",
    "title": "Evaluation of Diagnostic Tests",
    "section": "Diagnostic tests and prevalence",
    "text": "Diagnostic tests and prevalence\nThe concepts in diagnostic testing can be formulated in the form of conditional probabilities:\n\nSensitivity: \\(P(\\text{pos}|\\text{ill})\\)\nSpecificity: \\(P(\\text{neg}|\\text{healthy})\\)\nPositive predictive value: \\(P(\\text{ill}|\\text{pos})\\)\nNegative predictive value: \\(P(\\text{healthy}|\\text{neg})\\)\n\nBayes’ theorem can be applied to compute PPV and NPV from sensitivity, specificity and prevalence:\n\\[PPV = \\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})}\\]\n\\[NPV = \\frac{\\text{spec} \\cdot (1-\\text{prev}) }{(1-\\text{sens}) \\cdot \\text{prev} + \\text{spec} \\cdot (1-\\text{prev})}\\]\n\n\n\n\n\n\nDerivation of the PPV formula using Bayes’ law\n\n\n\n\n\n\\[\n\\begin{aligned}PPV = P(\\text{ill}|\\text{pos}) & = \\frac{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill})}{P(\\text{pos})}\\\\                               & = \\frac{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill})}{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill}) +  P(\\text{pos}|\\text{healthy}) \\cdot P(\\text{healthy})}\\\\ & =\\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})}\\end{aligned}\n\\]\nExercise: Try to derive the formula for \\(NPV\\) in a similar way.\n\n\n\n\n\n\n\n\n\nExample: HIV testing\n\n\n\n\n\nIn a test for the HIV virus, the result can be:\n\nPositive: the test shows antibodies.\nNegative: the test does not show antibodies.\n\nBut the test result may be wrong:\n\nA false positive might come from antibodies from related virus, but not HIV.\nA false negative might be due to the fact that antibodies are not yet produced in sufficient quantity, hence are not detected by the test.\n\nWe assume that the sensitivity of the HIV test is 98%. We know that the specificity of the HIV test is 99.8%. We assume that the prevalence of HIV in a given population is 0.1%.\nWhat is the probability of a person having HIV, if he got a positive test result?\n\\(PPV = \\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})} = \\frac{0.98 \\times 0.001}{0.98 \\times 0.001 + 0.002 \\times 0.999} = 0.329\\)\nWhat is the probability of a person not having HIV, if he got a negative test result?\n\\(NPV = \\frac{\\text{spec} \\cdot (1-\\text{prev}) }{(1-\\text{sens}) \\cdot \\text{prev} + \\text{spec} \\cdot (1-\\text{prev})} = \\frac{0.998 \\times 0.999}{0.98 \\times 0.001 + 0.998 \\times 0.999} = 0.999\\)\nLet us see from 100 000 persons, what are the theoretical results of the test?\n\nNumber of HIV infected (positive condition): \\(100000 \\times 0.001 = 100\\)\nTrue positives: \\(100 \\times 0.98 = 98\\)\nFalse negatives: 2\nNumber of not HIV infected (negative condition): \\(100000-100 = 99900\\)\nTrue negatives: \\(99900 \\times 0.998 = 99700\\)\nFalse positives: \\(200\\)\n\nNote that from 298 positive tests, only 98 persons have HIV. How would these numbers change if the prevalence would be lower? and if it would be higher?"
  },
  {
    "objectID": "lab/lab_survival.html",
    "href": "lab/lab_survival.html",
    "title": "Survival analysis",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_ttest.html",
    "href": "lab/lab_ttest.html",
    "title": "Statistical inference, t-test",
    "section": "",
    "text": "Datasets\n\nExercise 1: no data needed, we type in data directly.\nExercise 2: PEFH98-english (rda link, csv link)\n\nR script\n\n\nShort summary\n\nProperties of the sample mean\nThe sample mean of a continuous variable \\(X\\) is defined as\n\\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\).\n\nExpectation of the sample mean: \\(E(\\bar{X}) = \\mu\\)\nVariance of the sample mean: \\(Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\)\nStandard error (standard deviation of the sample mean): \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nThe distribution of the sample mean: \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\)\n\n\n\n\n\n\nPopulation vs sample\n\n\n\nNote that \\(\\sigma\\) and \\(\\mu\\) are parameters for the population: which are unknown in practice! That is why we need to estimate them based on samples we have collected.\n\n\n\n\nConfidence interval for sample mean\n\nKnown population standard deviation (sd)\nWhen the population standard deviation \\(\\sigma\\) (or variance \\(\\sigma^2\\)) is known, \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\)\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + 1.96 \\times \\frac{\\sigma}{\\sqrt{n}})\\)\n\n\nUnknown sd: use empirical sd and t-distribution\nHowever, in practice it’s hard to know \\(\\sigma\\) because we do not measure every subject in the population. Instead, we replace \\(\\sigma\\) with the empirical standard deviation \\(s\\). Instead of normal distribution, this quantity is Student t-distributed with \\(n-1\\) degrees of freedom:\n\\(\\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\)\nThe more samples you have (i.e. \\(n\\) becomes larger), the closer t-distribution is to the standard normal distribution \\(N(0,1)\\).\n95% confidence interval for the sample mean \\(\\bar{X}\\) is then\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\n\n\n\n\n\n\nQuantile for t-distribution using R\n\n\n\n\n\nFor 0.05 significance level (95% confidence interval), you can find different values of \\(t'\\) using R by qt(p = 0.975, df = n-1). For example, \\(t' = 2.09\\) when your sample size is 20, and \\(t' = 1.97\\) when your sample size is 200 (close to normal distribution: 1.96).\n\n\n\n\n\n\nHypothesis testing for sample mean\nOne sample t-test: tests whether sample mean equals to a certain value.\nt-test for paired samples: can be transformed into a one-sample t-test, by taking the differences between each pair.\nt-test for two independent samples: tests whether the sample means for two groups equal.\n\n# one sample (default tests against 0, conf.level 0.95)\nt.test(x)\n\n# one sample\nt.test(x, mu = your_value, conf.level = 0.95)\n\n# paired samples\nt.test(x1, x2, paired = T, conf.level = 0.95)\nt.test(x1-x2, conf.level = 0.95) # equivalent to one sample\n\n# two independent samples\nt.test(x, y, conf.level = 0.95)\n\n# check normal assumption\nqqnorm(x)\nqqline(x)\n\n\nAssumptions\n\nIndependent individuals\nSample mean is normally distributed\n\neither observations are normally distributed\nor large sample so that averages become normally distributed\n\n\nFor t-test of two independent samples, in addition to above:\n\nEquality of population standard deviation for both samples\nif not: Welch test or Fisher-Behrens test\n\n\n\n\n\n\nExercise 1 (heart)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn &lt;- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 &lt;- qt(p = 0.025, df = n-1) # -2.09\nt975 &lt;- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n# plug in the formula\nci_lower &lt;- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper &lt;- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals).\n\n\n\nExercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference &lt;- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?\n\n\n\n\nSolution\n\nExercise 1 (heart data)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n# enter the data\nheart &lt;- c(11.5, 14.75, 13.75, 10.5, 14.75,\n           13.5, 10.75, 9.5, 11.75, 12, \n           10.5, 11.75, 10, 14.5, 12, \n           11, 14, 15, 11.5, 10.25)\n\nYou can either compute the sample mean by summing each data point, and divide by sample size; or use R command mean().\n\n# compute the mean\nsum_heart &lt;- 11.5 + 14.75 + 13.75 + 10.5 + 14.75 + \n  13.5 + 10.75 + 9.5 + 11.75 + 12 + \n  10.5 + 11.75 + 10 + 14.5 + 12 + \n  11 + 14 + 15 + 11.5 + 10.25\n\n# this is the sum \nsum_heart\n\n[1] 243.25\n\n# sample size: 20\nn &lt;- 20\n# if we do not know the size, can find out with length(heart)\nsum_heart/n\n\n[1] 12.1625\n\n# formula: sum(heart)/length(heart)\nmean(heart)  # should be the same as above\n\n[1] 12.1625\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn &lt;- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n[1] 1.779405\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n[1] 1.779405\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 &lt;- qt(p = 0.025, df = n-1) # -2.09\nt975 &lt;- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n[1] -2.093024  2.093024\n\n# plug in the formula\nci_lower &lt;- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper &lt;- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n[1] 11.32971 12.99529\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 30.568, df = 19, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\npar(mfrow = c(1, 2)) # this code puts two plots side by side\n# histogram\nhist(heart, breaks = 10, main = 'Histogram of heart data')\nabline(v = mean(heart), col = 'red', lwd = 2)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\n\n# qqplot \nqqnorm(heart, pch = 20)\nqqline(heart, lwd = 2)\n\n\n\n\n\n\n\n\n\n# boxplot \npar(mfrow = c(1, 2))\n# compare with 11\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11, col = 'forestgreen', lwd = 3)\n\n\n# compare with 11.5\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11.5')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11.5, col = 'forestgreen', lwd = 3)\n\n\n\n\n\n\n\n\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals).\n\n# H0: mu = 11; H1: mu != 11\nt.test(heart, mu = 11, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 2.9217, df = 19, p-value = 0.008751\nalternative hypothesis: true mean is not equal to 11\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n# H0: mu = 11.5; H1: mu != 11.5\nt.test(heart, mu = 11.5, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 1.665, df = 19, p-value = 0.1123\nalternative hypothesis: true mean is not equal to 11.5\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n\n\n\n\nExercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n# if you use rda data file: \n# load('./lab/data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data &lt;- read.csv('data/PEFH98-english.csv', sep = ',')\nhead(lung_data)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n4  21   male    179     74     520     530     540     480     510     500\n5  20   male    196     95     740     750     750     700     710     700\n6  20   male    189     83     600     575     600     600     600     640\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n4 530.0000 496.6667 513.3333\n5 746.6667 703.3333 725.0000\n6 591.6667 613.3333 602.5000\n\n# we can focus on height and gender variable only\nhead(lung_data[, c('height', 'gender')], 10)\n\n   height gender\n1     165 female\n2     185   male\n3     178   male\n4     179   male\n5     196   male\n6     189   male\n7     173   male\n8     196   male\n9     173 female\n10    173 female\n\n\nNow we need to separate the height data for based on gender. First, we do it for gender == 'female'.\n\n# for convenience, we create a variable names 'gender'\ngender &lt;- lung_data$gender\n# height for female\nheight_f &lt;- lung_data$height[gender == 'female']\n\nYou should always check whether your newly created variable is correct: for example, you can compare the first several values of height_f with the original data, to see if it is really only selecting height for females.\nAnother useful thing to do is to check how many data poinnts have been selected.\n\n# check the first few values, is it only selecting female heights?\nhead(height_f)\n\n[1] 165 173 173 169 170 172\n\n# number of females\nnf &lt;- length(height_f)  # 54\nnf\n\n[1] 54\n\n\nNow we can compute the mean and confidence interval on the newly created variable, height_f.\n\nmean(height_f) # 169.57\n\n[1] 169.5741\n\nsd(height_f) # 5.69\n\n[1] 5.692106\n\n# se_f &lt;- sd(height_f)/sqrt(54) 0.774\n\n# quantile for t distribution: pay attention to df!\nt975 &lt;- qt(p = 0.975, df = nf-1) # 2.005\n\n# 95% CI \nci_upper_f &lt;- mean(height_f) + t975 * sd(height_f)/sqrt(nf) # 171.1277\nci_lower_f &lt;- mean(height_f) - t975 * sd(height_f)/sqrt(nf) # 168.0204\nc(ci_lower_f, ci_upper_f)\n\n[1] 168.0204 171.1277\n\n# double check by running a t.test\nt.test(height_f)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 218.92, df = 53, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nBased on our calculation, the mean height for females is 169.57 cm (95% confidence interval (168.02, 171.13)).\nFor gender == 'male', we can do the same thing. Pay attention to the different degrees of freedom, because the sample size has changed.\n\n# height for male \nheight_m &lt;- lung_data$height[gender == 'male']\n# number of males\nnm &lt;- length(height_m)  # 52\n\nmean(height_m) # 181.87\n\n[1] 181.8654\n\nsd(height_m) # 5.67\n\n[1] 5.667343\n\n# se_m &lt;- sd(height_m)/sqrt(52) # 0.786\n\n# find quantile for males (pay attention to df)\nt975 &lt;- qt(p = 0.975, df = nm-1) # 2.007\n# 95% CI\nci_upper_m &lt;- mean(height_m) + t975 * sd(height_m)/sqrt(nm) # 183.44\nci_lower_m &lt;- mean(height_m) - t975 * sd(height_m)/sqrt(nm)  # 180.28\nc(ci_lower_m, ci_upper_m)\n\n[1] 180.2876 183.4432\n\n# verify by t.test\nt.test(height_m)\n\n\n    One Sample t-test\n\ndata:  height_m\nt = 231.4, df = 51, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 180.2876 183.4432\nsample estimates:\nmean of x \n 181.8654 \n\n\nBased on our calculation, the mean height for males is 181.87 cm (95% confidence interval (180.28, 183.44)).\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\nLet \\(\\mu_F\\) indicate the average height for the female students. We test the hypothesis\n\\(H_0: \\mu_F = 167\\) against \\(H_a: \\mu_F \\neq 167\\) (two sided test).\n\n# use t.test command\nt.test(height_f, mu = 167)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 3.3231, df = 53, p-value = 0.001619\nalternative hypothesis: true mean is not equal to 167\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nWe can conclude that the average height for female students differ significantly from 167 (p = 0.0016). We reject the null hypothesis.\nIf you want to verify the p-value by hand: follow the procedure below.\n\n# (optional) calculate from the formula\nt_stat &lt;- (mean(height_f) - 167)/(sd(height_f)/sqrt(nf))\nt_stat  # 3.323\n\n[1] 3.323112\n\n# compare this with t distribution with nf-1 degrees of freedom\npval_twosided &lt;- pt(q = t_stat, df = nf-1, lower.tail = F)*2\npval_twosided\n\n[1] 0.001618751\n\n\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\nHere we use paired t-test, because these two measurements are on the same subject.\n\n\n\n\n\n\nMissing data in pefsit2\n\n\n\n\n\nWhen you run mean(lung_data$pefsit2), it might return NA as a result. This is caused by missing values in this variable. You can check this from the data (click on lung_data in your environment)(row 66)\nThis does not affect t.test() as it will remove NA automatically. However this might affect other functions, such as mean().\nYou can do mean(pefsit2, na.rm = T) (remove NA). This does not remove NA from your data forever; but only for your mean computation.\nYou should check whether there are NA in your data. One option is summary(pefsit2).\n\n\n\n\npefsit1 &lt;- lung_data$pefsit1\npefsit2 &lt;- lung_data$pefsit2\n# hist(pefsit1)\n# hist(pefsit2)\n# compute mean:\nmean(pefsit1)\n\n[1] 504.6038\n\nmean(pefsit2) # this returns NA, bec subject 66 has missing data\n\n[1] NA\n\nlung_data[66, ]\n\n   age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n66  19 female    166     56     450      NA      NA      NA     425      NA\n   pefsitm pefstam pefmean\n66      NA      NA      NA\n\n# to remove this when computing mean by mean(): \nmean(pefsit2, na.rm = T) # remove NA when computing mean\n\n[1] 509.5238\n\n# (how to check if my data has NA: is.na(pefsit2); summary(pefsit2))\nsummary(pefsit2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  280.0   415.0   500.0   509.5   580.0   800.0       1 \n\n# t-test will automatically remove NA \nt.test(pefsit1, pefsit2, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsit1 and pefsit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean difference \n           -4.4 \n\n# alternatively, you can test whether the difference is equal to 0\ndiff_sit1_sit2 &lt;- pefsit1 - pefsit2\nt.test(diff_sit1_sit2, mu = 0)\n\n\n    One Sample t-test\n\ndata:  diff_sit1_sit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean of x \n     -4.4 \n\n\nWe can conclude that there is no significant difference (p = 0.12) between the first and second pef measurement.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\nWe still use paired test as both measurements are on the same subject.\n\n# compare pefsitm, pefstam (paired t-test)\npefsitm &lt;- lung_data$pefsitm\npefstam &lt;- lung_data$pefstam\n\nt.test(pefsitm, pefstam, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsitm and pefstam\nt = -3.6974, df = 104, p-value = 0.0003498\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -14.656161  -4.423204\nsample estimates:\nmean difference \n      -9.539683 \n\n\nWe can conclude that that there is a significant difference (p &lt; 0.001) between pef measured in a sitting and standing position.\n\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference &lt;- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n# create a variable\ndiff_sitm_stam &lt;- pefsitm - pefstam\nqqnorm(diff_sitm_stam, pch = 20)\nqqline(diff_sitm_stam, col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nWhen a Q-Q plot looks like this, we can say that the normality assumption is reasonably fulfilled.\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n# this is independent two samples t-test\n# we need two variables: pefmean for men, pefmean for women\n\npefmean_f &lt;- lung_data$pefmean[gender == 'female']\npefmean_m &lt;- lung_data$pefmean[gender == 'male']\n\n# visually spot whether there is a difference\n# NOTE: there is a NA in pefmean_f; use na.rm = T to remove it \npar(mfrow = c(1, 2))\nhist(pefmean_f, main = 'pefmean (F)')\nabline(v = mean(pefmean_f, na.rm = T), col = 'red', lwd = 2)\nhist(pefmean_m, main = 'pefmean (M)')\nabline(v = mean(pefmean_m), col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nFrom the histogram for female and male students, it can be seen that the difference between the two mean pef measurements differ. We can test it using t-test for two independent samples.\n\n\n\n\n\n\nEqual variance assumption\n\n\n\n\n\nFor two sample t-test, there is an assumption of equal variance in two groups. In R, t.test() automatically checks whether this is fulfilled. If not, it returns result from Welch’s t-test.\nIf you want to force t.test() to use equal variance, specify var.equal = T.\nThe ways to interpret results are the same.\n\n\n\n\n# two sample t-test (Welch)\nt.test(pefmean_f, pefmean_m, paired = F)\n\n\n    Welch Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.425, df = 90.28, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.6366 -150.3941\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n# force equal variance\nt.test(pefmean_f, pefmean_m, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.468, df = 103, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.4907 -150.5401\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n\nFrom both results you can see that the difference in means between two genders is very big. The p-values are very small (p &lt; 0.001), can reject the null hypothesis that the sample means from two groups are equal.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?\n\npar(mfrow = c(1, 2))\nqqnorm(pefmean_f, pch = 20, main = 'Q-Q plot: pefmean (F)')\nqqline(pefmean_f, col = 'red', lwd = 2)\n\nqqnorm(pefmean_m, pch = 20, main = 'Q-Q plot: pefmean (M)')\nqqline(pefmean_m, col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nThe normality assumption is fulfilled."
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-1-heart-data",
    "href": "lab/lab_ttest.html#exercise-1-heart-data",
    "title": "t-test",
    "section": "Exercise 1 (heart data)",
    "text": "Exercise 1 (heart data)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n# enter the data\nheart &lt;- c(11.5, 14.75, 13.75, 10.5, 14.75,\n           13.5, 10.75, 9.5, 11.75, 12, \n           10.5, 11.75, 10, 14.5, 12, \n           11, 14, 15, 11.5, 10.25)\n\nYou can either compute the sample mean by summing each data point, and divide by sample size; or use R command mean().\n\n# compute the mean\nsum_heart &lt;- 11.5 + 14.75 + 13.75 + 10.5 + 14.75 + \n  13.5 + 10.75 + 9.5 + 11.75 + 12 + \n  10.5 + 11.75 + 10 + 14.5 + 12 + \n  11 + 14 + 15 + 11.5 + 10.25\n\n# this is the sum \nsum_heart\n\n[1] 243.25\n\n# sample size: 20\nn &lt;- 20\n# if we do not know the size, can find out with length(heart)\nsum_heart/n\n\n[1] 12.1625\n\n# formula: sum(heart)/length(heart)\nmean(heart)  # should be the same as above\n\n[1] 12.1625\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn &lt;- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n[1] 1.779405\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n[1] 1.779405\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 &lt;- qt(p = 0.025, df = n-1) # -2.09\nt975 &lt;- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n[1] -2.093024  2.093024\n\n# plug in the formula\nci_lower &lt;- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper &lt;- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n[1] 11.32971 12.99529\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 30.568, df = 19, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\npar(mfrow = c(1, 2)) # this code puts two plots side by side\n# histogram\nhist(heart, breaks = 10, main = 'Histogram of heart data')\nabline(v = mean(heart), col = 'red', lwd = 2)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\n\n# qqplot \nqqnorm(heart, pch = 20)\nqqline(heart, lwd = 2)\n\n\n\n\n\n\n\n\n\n# boxplot \npar(mfrow = c(1, 2))\n# compare with 11\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11, col = 'forestgreen', lwd = 3)\n\n\n# compare with 11.5\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11.5')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11.5, col = 'forestgreen', lwd = 3)\n\n\n\n\n\n\n\n\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals).\n\n# H0: mu = 11; H1: mu != 11\nt.test(heart, mu = 11, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 2.9217, df = 19, p-value = 0.008751\nalternative hypothesis: true mean is not equal to 11\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n# H0: mu = 11.5; H1: mu != 11.5\nt.test(heart, mu = 11.5, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 1.665, df = 19, p-value = 0.1123\nalternative hypothesis: true mean is not equal to 11.5\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625"
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-2-lung-function",
    "href": "lab/lab_ttest.html#exercise-2-lung-function",
    "title": "t-test",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference &lt;- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?"
  },
  {
    "objectID": "lab/lab_categorical.html",
    "href": "lab/lab_categorical.html",
    "title": "Categorical data analysis",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-1-lung-function",
    "href": "lab/lab_categorical.html#exercise-1-lung-function",
    "title": "Categorical data analysis",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe investigate whether having a high value of pefmean is associated with gender. First, create a new variable highpef that indicates whether pefmean is above 500.\n\n# if you use rda data file: \n# load('./data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data <- read.csv('data/PEFH98-english.csv', sep = ',')\n# head(lung_data)\n\n# examine variable pefmean\n# visualise the distribution\nhist(lung_data$pefmean)\nabline(v = 500, col = 'red', lwd = 2)\n\n\n\n\nThere are multiple ways to do it. Here we show two of them.\nIn option 1, we use the function ifelse(). It will create a binary vector, and fill in different output based on whether the condition was true or not.\n\n# option 1\n# code new variable: highpef with cutoff = 500\n# this is a binary variable, 1 means yes (higher than 500), 0 means no\nhighpef <- ifelse(lung_data$pefmean > 500, '1', '0')\n\n# check if it makes sense\nhead(lung_data$pefmean)\n\n[1] 405.0000 491.6667 505.0000 513.3333 725.0000 602.5000\n\nhead(highpef)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\nIn option 2, we create a vector with a pre-specified value, then replace the elements for given indices.\n\n# option 2 \n# create a vector with all '0's\n# then set the elements above 500 in 'pefmean' as '1'\n# (replacement with index)\n\nn <- length(lung_data$pefmean) # 106\nhighpef_alt <- rep('0', n) # repeat '0' 106 times\nhighpef_alt[lung_data$pefmean > 500] <- '1'\nhead(highpef_alt)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\n\n\n1b)\nWe investigate the association between these two variables via an appropriate table analysis. Make a table of highpef vs gender, which counts the number of subjects having each one of the four combinations in a 2 by 2 contingency table.\nYou can do this with table() command: it counts how many observations equals to each unique value in the data. When the data takes 2 values (1/0, yes or no), table() counts the numbers in each category.\n\n# take out gender variable so we don't need to use $ any more\ngender <- lung_data$gender\n\n# we can count how many subjects are in each category for the two variables\ntable(gender)\n\ngender\nfemale   male \n    54     52 \n\ntable(highpef)\n\nhighpef\n 0  1 \n54 51 \n\n\nUsing table() for two variables produces the cross tabulation: \\(2 \\times 2\\) combinations. The first element is put as rows, and second element is put as columns.\n\n# now we count how many subjects are in the combinations \ntable_gender_highpef <- table(gender, highpef)\ntable_gender_highpef\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\n\n\n\n\n\n\n\nCross tabulation, order of exposure and outcome\n\n\n\n\n\nYou should be aware that table() does not automatically produce the contingency table which puts exposed (group 1) for two outcomes at the first row; neither does it put positive outcome (outcome = yes) for exposed/unexposed group in the first column.\ntable() only counts the combination. It is up to you to make sense which numbers are \\(d1, d0, h1, h0\\)!\n\n\n\n\n\n1c)\nCompute risk ratio and odds ratio (only the point estimates, no confidence interval). Here the exposure is gender (we assume that the reference group is female: unexposed), and outcome is highpef. Interpret your results.\n\n\n\n\n\n\nRisk ratio and odds ratio\n\n\n\n\n\n\n\n\n\nOutcome (yes)\nOutcome (no)\n\n\n\n\nExposed\nd1\nh1\n\n\nUnexposed\nd0\nh0\n\n\n\nRisk ratio: \\(\\frac{d1/(d1+h1)}{d0/(d0+h0)}\\)\nOdds ratio: \\(\\frac{d1/h1}{d0/h0} = \\frac{d1 \\times h0}{d0 \\times h1}\\)\n\n\n\nIt could help by drawing a table before computing the metrics.\n\n\n\n\nOutcome (highpef = 1)\nOutcome (highpef = 0)\n\n\n\n\nExposed (gender = male)\n46\n6\n\n\nUnexposed (gender = female)\n5\n48\n\n\n\n\n# first use formula (only point estimates)\n# risk ratio (relative risk)\n# risk in exposed / risk in unexposed\n\nrisk_exposed <- 46/(46+6)  # 0.885\nrisk_unexposed <- 5/(5+48) # 0.094\nrr <- risk_exposed/risk_unexposed\nrr  # 9.37\n\n[1] 9.376923\n\n# odds ratio\n# odds of event in exposed group / odds of event in non-exposed group\nodds_exposed <- 46/6  # 7.667\nodds_unexposed <- 5/48  # 0.104\nor <- odds_exposed/odds_unexposed\nor # 73.6\n\n[1] 73.6\n\n\n\n\n1d)\nCarry out a chi-square analysis to assess the strength of association. Interpret your results.\nWe do a chi-square test with chisq.test() function. To read more about what is required as input, you can use ?chisq.test() to get the documentation.\n\n# note: \n# we have not distinguished between exposure and outcome\n# by default, 'table' function puts the first input in rows (gender)\n# the order does not affect the result of chi-square test\n\nchisq.test(table_gender_highpef)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_gender_highpef\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\nIf you want to know what are the observed and expected numbers used for the chi-square test, you can create a variable (let us call it ctest), and access the elements using $.\n\nctest <- chisq.test(table_gender_highpef)\n# you can extract elements of ctest using $\nctest$observed # observed data\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\nctest$expected # expected data (under null, no association)\n\n        highpef\ngender          0        1\n  female 27.25714 25.74286\n  male   26.74286 25.25714\n\n\n\n\n\n\n\n\nChi-square test measures strength of association\n\n\n\nIt is worth pointing out that chisq.test() does not distinguish between exposure and outcome variables. If you reverse the variable order, the test-statistic and p-value is exactly the same.\nThis is why you should always be clear in your mind what is your exposure and outcome, and report risk ratio and/or odds ratio.\n\n\n\n# reverse the order of the two variables: highpef firsst, gender second\ntable_highpef_gender <- table(highpef, gender)\nchisq.test(table_highpef_gender)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_highpef_gender\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\n\n\nOptional: use R package to compute RR and OR\n(This is for people who are interested, and able to load additional R packages and read the documentations. If this is too much: skip it)\nYou can verify if this is correct with some R packages that implement risk ratio and odds ratio.\n\n\n\n\n\n\nBe careful with the input format when using packages\n\n\n\nDifferent packages can have different requirements for how your data input should look like. Always check documentation to get the correct result.\nThis is also true with other softwares like STATA.\nComparison of different packages in computing risk ratio: read this discussion\nDocumentation: epitools Documentation: epiR\n\n\nThe first function we use is epitab from epitools package.\n\n# install.packages('epitools')\n\n# match the expected data format\n# col: outcome -, outcome +\n# row: exposure -, exposure +\ntb1 <- matrix(c(48, 5, 6, 46), byrow = T, ncol = 2)\n# tb1\nepitools::epitab(tb1, method = 'riskratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 riskratio    lower    upper\n  Exposed1       48 0.9056604        5 0.09433962  1.000000       NA       NA\n  Exposed2        6 0.1153846       46 0.88461538  9.376923 4.048485 21.71842\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\nepitools::epitab(tb1, method = 'oddsratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 oddsratio    lower    upper\n  Exposed1       48 0.8888889        5 0.09803922       1.0       NA       NA\n  Exposed2        6 0.1111111       46 0.90196078      73.6 21.00627 257.8735\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\nThe second function we use is epi.2by2 from epiR package.\n\n# install.packages('epiR')\n\n# match the expected data format\n# col: outcome +, outcome -\n# row: exposure +, exposure -\ntb2 <- matrix(c(46, 6, 5, 48), byrow = T, ncol = 2)\n# tb2\nepiR::epi.2by2(tb2, method = 'cohort.count')\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           46            6         52     88.46 (76.56 to 95.65)\nExposed -            5           48         53       9.43 (3.13 to 20.66)\nTotal               51           54        105     48.57 (38.70 to 58.53)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 9.38 (4.05, 21.72)\nOdds ratio                                     73.60 (21.01, 257.87)\nAttrib risk in the exposed *                   79.03 (67.31, 90.75)\nAttrib fraction in the exposed (%)            89.34 (75.30, 95.40)\nAttrib risk in the population *                39.14 (26.76, 51.52)\nAttrib fraction in the population (%)         80.58 (57.91, 91.04)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 65.624 Pr>chi2 = <0.001\nFisher exact test that OR = 1: Pr>chi2 = <0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "lab/lab_distributions.html",
    "href": "lab/lab_distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "In this R lab, we will practice with probability distributions."
  },
  {
    "objectID": "lab/lab_distributions.html#binomial-distribution",
    "href": "lab/lab_distributions.html#binomial-distribution",
    "title": "Probability distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA random variable \\(X\\) is said to be a binomial variable with parameters \\(n\\) and \\(p\\) if\n\\[ P(X=x) = \\binom n x p^{x}(1-p)^{n-x}\\]\nThis is often writen \\(X \\sim \\text{Binom}(n,p)\\). Let us compute the probabilities of a binomial distribution \\(X \\sim \\text{Binom}(5,0.2)\\) using R.\n\n# first, we store the parameters of the distribution in variables n and p\nn = 5\np = 0.2\n\nx = 0:n # we create a vector with discrete values from 0 to n\n\nbinom_coef=choose(n, x) # we compute the binomial coefficients for each x \n\n# we compute binomial probabilities for all x values\nbinom_prob= binom_coef* p^x * (1 - p)^{n-x} \nprint(binom_prob) # we print all probabilities\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\nThis was a good exercise, but R has a built-in function to compute the probabilities of a binomial distribution directly: dbinom(x,n,p). Let us use that function to verify that the probabilities that we computed were correct.\n\nbinom_prob = dbinom(x,n,p)\nprint(binom_prob)\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\nWe can plot those probabilities with the barplot command to see the probability density function (PDF).\n\nbarplot(names= x,\n        height = binom_prob,\n        main = \"PDF of X~Binom(5,0.2)\",\n        xlab='X',\n        ylab='Probability')"
  },
  {
    "objectID": "lab/lab_distributions.html#simulations-of-a-binomial-distribution",
    "href": "lab/lab_distributions.html#simulations-of-a-binomial-distribution",
    "title": "Probability distributions",
    "section": "Simulations of a binomial distribution",
    "text": "Simulations of a binomial distribution\nNext we will simulate N samples from a binomial distribution with the command rbinom(N, n, p) and look at the frequency distribution of the outcomes.\n\nN=100\n# we draw N samples from a binomial distribution with parameters n and p and store them in variable binom_sim\nbinom_sim = rbinom(N,n,p)\n\n# we compute the frequency distribution of the samples and store them in a variable\ndata = table(binom_sim)\nprint(data)\n\nbinom_sim\n 0  1  2  3 \n44 35 18  3 \n\n# we can plot the frequencies in a bar plot\nbarplot(data,ylab=\"frequencies\")\n\n\n\n# we can plot the frequencies in a pie chart\npie(data)\n\n\n\n\nNext, we will compute the relative frequencies by dividing the observed frequencies by the total number of draws N. For high N, the relative frequencies should approach the theoretical probabilities of the binomial distribution.\n\nbinom_sim_relfreq=data/N\nprint(binom_sim_relfreq)\n\nbinom_sim\n   0    1    2    3 \n0.44 0.35 0.18 0.03 \n\nbarplot(binom_sim_relfreq,ylab=\"relative frequency\")"
  },
  {
    "objectID": "lab/lab_distributions.html#normal-distribution",
    "href": "lab/lab_distributions.html#normal-distribution",
    "title": "Probability distributions",
    "section": "Normal distribution",
    "text": "Normal distribution\nThe probability density function of the normal distribution \\(X \\sim \\text{N}(\\mu,\\sigma)\\), where \\(\\mu\\) denotes the mean and \\(\\sigma\\) the standard deviation, is given by:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\text{exp}(- \\frac{(x-\\mu)^2}{2\\sigma^2})\n\\]\nWe can define and plot this function in R to see the characteristic bell shape of the normal distribution.\n\n# first we set the values for the mean and the sd \nmu=0\nsigma=1\n\n# next, we define the function f\nf_norm=function(x){(1/(sigma*sqrt(2*pi))) * exp((-(x-mu)^2)/(2*sigma^2))}\n#note that this function is already defined in R as dnorm(x), we could use that instead\n\n# we plot the function f\ncurve(f_norm,\n      xlim=c(-5,5),\n      main=\"Normal density plot\",\n      xlab=\"X values\",\n      ylab=\"Density\"\n      )\n\n\n\n\nNext, we will compute \\(P(X \\leq 0)\\). For that we can compute the area under the curve from \\(-\\infty\\) to \\(1\\) using the function integrate.\n\nintegrate(f_norm,-Inf, 0)\n\n0.5 with absolute error < 4.7e-05\n\n\nUnsurprisingly, R has a built-in function to compute such probabilities: pnorm(x,mean, sd). Let us use that function to verify that the probability that we computed was correct.\n\npnorm(0, mean=0, sd=1)\n\n[1] 0.5\n\n\nConversely, we can find the value of \\(x\\) so that \\(P(X\\leq x) = 0.5\\) using the command qnorm(prob,mean,sd).\n\nqnorm(0.5,mean=0,sd=1)\n\n[1] 0\n\n\nNow let us find the value of \\(x\\) so that \\(P(-x\\leq X\\leq x) = 0.95\\). As the distribution is symmetric with respect to zero, we could instead find the value of \\(x\\) so that \\(P( X\\leq x) = 0.025\\), or equivalently, the value of \\(x\\) so that \\(P( X\\geq x) = 0.025\\). We can compute those values again using the command pnorm.\n\nqnorm(0.025,mean=0,sd=1) # find x so that P(X<=x) = 0.025\n\n[1] -1.959964\n\nqnorm(0.025,mean=0,sd=1, lower.tail = FALSE) # find x so that P(X>=x) = 0.025\n\n[1] 1.959964\n\n\nLet us verify that \\(P(-1.96\\leq X\\leq 1.96) = 0.95\\).\n\npnorm(1.96,0,1) - pnorm(-1.96,0,1)\n\n[1] 0.9500042\n\n\nIt is important to remember where the value 1.96 comes from, we will see it later in the course!"
  },
  {
    "objectID": "lab/lab_distributions.html#simulations-of-the-normal-distribution",
    "href": "lab/lab_distributions.html#simulations-of-the-normal-distribution",
    "title": "Probability distributions",
    "section": "Simulations of the normal distribution",
    "text": "Simulations of the normal distribution\nTo simulate samples of the normal distribution in R, we use the command rnorm(N, mean, sd). If we want to simulate samples of the standard normal distribution \\(N(0,1)\\), we can simply use the command rnorm(N).\n\nnorm_sim=rnorm(500)\nhist(norm_sim, \n     freq = FALSE, #This is needed to plot the relative frequencies instead of the frequencies, so that the total area of the histogram is one.\n     main= \"Histrogram of normal data\",\n     xlab=\"X values\")\n\n\n\n\nThe larger the number of samples, the the closer the histogram would look to a bell shape. We can also check if our data are normally distributed using the command qqnorm(data) that generates a QQPlot. These plots display the data quantiles against the theoretical quantiles of the normal distribution. Therefore, normally distributed data should lie close to the identity line \\(y=x\\).\n\nqqnorm(norm_sim) # we generate the QQPlot for the data simulated data"
  },
  {
    "objectID": "lab/lab_distributions.html#approximation-of-the-binomial-distribution",
    "href": "lab/lab_distributions.html#approximation-of-the-binomial-distribution",
    "title": "Probability distributions",
    "section": "Approximation of the binomial distribution",
    "text": "Approximation of the binomial distribution\nHere we will compute the probability density function of binomial distributions with parameters \\(p=0.2\\) and parameter \\(n\\) variying from 2 to 30.\n\np=0.3\n\npar(mfrow=c(2,2))    # set the plotting area into a 2*2 matrix\n#first plot with n=2\nx = 0:2\nbarplot(names= x,\n        height = dbinom(x,2,p),\n        main = \"PDF of X~Binom(2,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#second plot with n=5\nx = 0:5\nbarplot(names= x,\n        height = dbinom(x,5,p),\n        main = \"PDF of X~Binom(5,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#third plot with n=15\nx = 0:15\nbarplot(names= x,\n        height = dbinom(x,15,p),\n        main = \"PDF of X~Binom(15,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#fourth plot with n=30\nx = 0:30\nbarplot(names= x,\n        height = dbinom(x,30,p),\n        main = \"PDF of X~Binom(30,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n\n\n\nWe observe that the higher \\(n\\) is, the closer the PDF of the binomial distribution looks to a bell shape. From the graphs above, it seems that \\(X_1 \\sim \\text{Binom}(30,0.3)\\) can be approximated by a normal distribution \\(X_2 \\sim \\text{N}(30*0.3,\\sqrt{30*0.3*0.7})\\). But do the probabilities of anologous events also match? Next, we compare \\(P(X_1 = 10)\\) and \\(P(9.5 \\leq X_2 \\leq 10.5)\\).\n\ndbinom(x=10,30,0.3)\n\n[1] 0.1415617\n\nmu=30*0.3\nsigma=sqrt(30*0.3*0.7) \npnorm(10.5,mu,sigma) - pnorm(9.5,mu,sigma)\n\n[1] 0.1460026"
  },
  {
    "objectID": "lab/lab_distributions.html#exercise-when-the-normal-distribution-wont-work",
    "href": "lab/lab_distributions.html#exercise-when-the-normal-distribution-wont-work",
    "title": "Probability distributions",
    "section": "Exercise: When the normal distribution won’t work",
    "text": "Exercise: When the normal distribution won’t work\nCheck if a Binomial distribution with \\(p=0.01\\) and \\(n=30\\) can be approximated by a normal distribution. Why?"
  },
  {
    "objectID": "lab/lab_distributions.html#central-limit-theorem",
    "href": "lab/lab_distributions.html#central-limit-theorem",
    "title": "Probability distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nWe will simulate 1000 samples of size \\(N=100\\) of the binomial distribution with parameters \\(p=0.01\\) and \\(n=30\\) (it could be any other distribution). We will compute the mean of each sample and see that the sample mean is a random variable that is normally distributed.\n\nsample_means=replicate(n = 1000, mean(rbinom(100,30,0.01)))\nhist(sample_means, breaks=20)\n\n\n\nqqnorm(sample_means)"
  },
  {
    "objectID": "lab/lab_distributions.html#exercise-sample-mean-with-small-sample-size",
    "href": "lab/lab_distributions.html#exercise-sample-mean-with-small-sample-size",
    "title": "Probability distributions",
    "section": "Exercise: Sample mean with small sample size",
    "text": "Exercise: Sample mean with small sample size\nRepeat the previous simulations, but this time take a very small sample size. Is the sample mean also normally distributed? Why?"
  },
  {
    "objectID": "lab/ex_categorical.html",
    "href": "lab/ex_categorical.html",
    "title": "Exercise - Categorical data analysis",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_categorical.html#exercise-1-lung-function",
    "href": "lab/ex_categorical.html#exercise-1-lung-function",
    "title": "Exercise - Categorical data analysis",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sitting position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe investigate whether having a high value of pefmean is associated with gender.\nFirst, create a new variable highpef that indicates whether pefmean is above 500.\n\n\n1b)\nWe investigate the association between these two variables via an appropriate table analysis. Make a table of highpef vs gender, which counts the number of subjects having each one of the four combinations in a 2 by 2 contingency table.\n\n\n1c)\nCompute risk ratio and odds ratio (only the point estimates, no confidence interval). Here the exposure is gender (we assume that the reference group is female: unexposed), and outcome is highpef. Interpret your results.\n\n\n\n\n\n\nRisk ratio and odds ratio\n\n\n\n\n\n\n\n\n\nOutcome (yes)\nOutcome (no)\n\n\n\n\nExposed\na\nb\n\n\nUnexposed\nc\nd\n\n\n\nRisk ratio: \\(\\frac{a/(a+b)}{c/(c+d)}\\)\nOdds ratio: \\(\\frac{a/b}{c/d} = \\frac{ad}{bc}\\)\n\n\n\n\n\n1d)\nCarry out a chi-square analysis to assess the strength of association. Interpret your results."
  },
  {
    "objectID": "lab/ex_categorical.html#exercise-2-birth-data",
    "href": "lab/ex_categorical.html#exercise-2-birth-data",
    "title": "Exercise - Categorical data analysis",
    "section": "Exercise 2 (birth data)",
    "text": "Exercise 2 (birth data)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n2a)\nDoes smoking influence the birth weight of a baby?\nMake a histogram and a box plot, and find the median and mean for birth weight in grams (bwt) for the two groups of mothers: one group that smokes, and one that does not smoke durinng the pregnancy. (Hint: use variable smk for this task)\n\n\n2b)\nAre the distributions for the two groups normal? Can we conclude that smoking has an effect on the weight of a newborn? Use a t-test.\n\n\n2c)\nDo mothers of hypertension have a tendency to have babies with a lower birth weight? (The variable for hypertension is ht)\nMake a histogram of birth weight for the two groups of mothers with and without hypertension. What do you observe?\n\n\n2d)\nAnalyse the effect of smoking on birth weight in a table (categorical analysis): compute the relative risk (risk ratio) and carry out a test to assess the strength of association.\nAlso do it for hypertension (instead of smoking).\nWhat can you conclude?"
  },
  {
    "objectID": "lab/ex_ttest.html",
    "href": "lab/ex_ttest.html",
    "title": "Exercise - t-test",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/ex_ttest.html#exercise-1-heart",
    "href": "lab/ex_ttest.html#exercise-1-heart",
    "title": "Exercise - t-test",
    "section": "Exercise 1 (heart)",
    "text": "Exercise 1 (heart)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn <- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 <- qt(p = 0.025, df = n-1) # -2.09\nt975 <- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n# plug in the formula\nci_lower <- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper <- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals)."
  },
  {
    "objectID": "lab/ex_ttest.html#exercise-2-lung-function",
    "href": "lab/ex_ttest.html#exercise-2-lung-function",
    "title": "Exercise - t-test",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference <- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?"
  },
  {
    "objectID": "lab/lab_ttest.html#equal-variance-assumption",
    "href": "lab/lab_ttest.html#equal-variance-assumption",
    "title": "t-test",
    "section": "Equal variance assumption",
    "text": "Equal variance assumption\nFor two sample t-test, there is an assumption of equal variance in two groups. In R, t.test() automatically checks whether this is fulfilled. If not, it returns result from Welch’s t-test.\nIf you want to force t.test() to use equal variance, specify var.equal = T.\nThe ways to interpret results are the same."
  },
  {
    "objectID": "lab/list_of_commands.html",
    "href": "lab/list_of_commands.html",
    "title": "List of commands",
    "section": "",
    "text": "Data\n\nData import\n\nread.csv('data/your_data.csv', sep = ',')\n\n\n\nCreate variables\nCreate a numeric variable\n\na &lt;- 3\na  # return the value in console\nreturn(a)\n\nNumeric, character, logical variables\n\nclass(a)\n\nb &lt;- 'hadley'\nclass(b)\n\nc &lt;- TRUE\nclass(c)\n\n\n\nData structure (vector, matrix)\nCreate vectors and matrices\n\nnum_vector &lt;- c(1, 2, 3, 4, 5)\nchar_vector &lt;- c('student_a', 'student_b', 'student_c')\nlogical_vector &lt;- c(T, F, T, F)\n\n# matrix\nmatrix_1 &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = T)\n\n# matrix by combining vectors\nvec1 &lt;- c(1, 2)\nvec2 &lt;- c(3, 4)\n\nmatrix_c &lt;- cbind(vec1, vec2) # bind by columnn\nmatrix_r &lt;- rbind(vec2, vec2) # bind by row\n\n\n\n\nData exploration of a data.frame\nCreate a data.frame\n\nmini_data &lt;- data.frame(\n  age = c(20, 50, 32), \n  sex = c('male', 'female', 'male'), \n  has_covid = c(T, T, F)\n)\n\nGet the column (feature) names, dimension, number of rows (observation) and columns\n\ncolnames(mini_data)\ndim(mini_data)\nnrow(mini_data)\nncol(mini_data)\n\nSelect a variable (age) from the data\n\nmini_data$age\nmini_data['age']\nmini_data[, 1] # first column, which is 'age'\n\nFilter a variable based on another (for example, age for females (sex == 'female'))\n\nmini_data$age[mini_data$sex == 'female']\n\n# you can also break down the process:\nage &lt;- mini_data$age\nsex &lt;- mini_data$sex\nage[sex == 'female']\n\n\n\nDescriptive statistics\nContinuous variables\n\n# continuous variable x\nsummary(x)\nmin(x)\nmax(x)\nmean(x)\nmedian(x)\nquantile(x, 0.95)\nIQR(x) # interquartile range\n\nCategorical variables: count and percentage\n\n# continuous variable z\n# subjects per category in x\ntable(x)\n# percentage\ntable(x)/length(x) \n\n\n\nVisualisation\nWe let x, y be two continuous variables, and z be categorical. To create histogram, boxplot, scatterplot, you can use the following commands,\n\nhist(x) # histogram\nboxplot(x) # boxplot \nboxplot(x ~ z, data = data) # boxplot for two variables, where z is categorical\nplot(x,y)  # scatter plot of x, y\n\n\n\nHypothesis tests\n\nt-test\n\n# one sample (default tests against 0, conf.level 0.95)\nt.test(x)\n\n# one sample\nt.test(x, mu = your_value, conf.level = 0.95)\n\n# paired samples\nt.test(x1, x2, paired = T, conf.level = 0.95)\nt.test(x1-x2, conf.level = 0.95) # equivalent to one sample\n\n# two independent samples\nt.test(x, y, conf.level = 0.95)\n\n# check normal assumption\nqqnorm(x)\nqqline(x)\n\n\n\nz-test, chi-square tests and table analysis\n\n# test proportion: whether 123 success in 1000 equals prob = 0.15\nprop.test(x = 123, n = 1000, p = 0.15)\nbinom.test(x = 123, n = 1000, p = 0.15)\n\n# create binary variable\n# compare your continuous values against threshold\n# assign \"yes\" to those higher; otherwise, assign \"no\"\nhigh_value &lt;- ifelse(your_values &gt; threshold, \"yes\", \"no\")\n\n# count each category\ntable(x) # x is categorical! \n\n# cross tabulation (2 variables)\ntable(x,y) # x, y are categorical\n\n# chi.squared test\n# tb is a 2 by 2 table (matrix) with counts\nchisq.test(tb)\n\n\n\nnon-parametric methods\n\n# median ci (with descTools package)\nDescTools::MedianCI(x, conf.level = 0.95)\n\n# one sample (paired samples) wilcoxon test (signed rank)\nwilcox.test(x1, x2, paired = T)\n\n# two sample (independent) wilcoxon test (rank sum)\nwilcox.test(x, y, paired = F)\n\n\n\n\nRegression analysis\n\nLinear regression\n\n# univariate: y is dependent var, x,z are independent var\nlinear_model &lt;- lm(y ~ x, data = your_data)\n\n# multivarite\nlinear_model &lt;- lm(y ~ x+z, data = your_data)\n\n# model summary\nsummary(linear_model)\n\n# model diagnostic plots\nplot(linear_model)\n\n\n\nLogistic regression\n\n# univariate: y is dependent var, x,z are independent var\n# y needs to be either 0/1, or factors\nlogit_model &lt;- glm(y ~ x, data = your_data, family = 'binomial')\n\n# multivariate, summary and diagnostic are the same as linear model\n\n\n\nSurvival analysis\n\n# need package, survival\n# install.packages('survival')\nlibrary(survival)\n\n# fit kaplan-meier plot\nkm_fit &lt;- survfit(Surv(lifetime, death) ~ 1)\nplot(km_fit)\n\n# survival probabilities at specific times\ntme &lt;- c(1, 2, 5) # time points\nsummary(km_fit, times = tme)\n\n# log rank test (compare two genders)\nkm_fit_gender &lt;- survfit(Surv(lifetime, death) ~ gender)\nplot(km_fit_gender, col = c('blue', 'red'))\nsurvdiff(Surv(lifetime, death) ~ gender)"
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-2-birth-data",
    "href": "lab/lab_categorical.html#exercise-2-birth-data",
    "title": "Categorical data analysis",
    "section": "Exercise 2 (birth data)",
    "text": "Exercise 2 (birth data)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n2a)\nDoes smoking influence the birth weight of a baby?\nMake a histogram and a box plot, and find the median and mean for birth weight in grams (bwt) for the two groups of mothers: one group that smokes, and one that does not smoke durinng the pregnancy. (Hint: use variable smk for this task)\n\n# if you are using .rda: click on the data icon\n\n# load data\nbirth <- read.csv('data/birth.csv', sep = ',')\nhead(birth)\n\n  id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1  4 bwt <= 2500  28 120 other    smoker   1  no yes   0   0  709\n2 10 bwt <= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3 11 bwt <= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4 13 bwt <= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5 15 bwt <= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6 16 bwt <= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n\n# take the variables\nbirthweight <- birth$bwt\nsmoker <- birth$smk\n\n# get to know how many smoker and non-smoker\ntable(smoker)\n\nsmoker\nnonsmoker    smoker \n      115        74 \n\n# histogram for all (both categories together)\nhist(birthweight)\n\n\n\n# separate birthweight based on smoking\nbw_smoker <- birthweight[smoker == 'smoker']\nbw_nonsmoker <- birthweight[smoker == 'nonsmoker']\n\n# can produce summary statistics\nsummary(bw_smoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2370    2776    2773    3246    4238 \n\nsummary(bw_nonsmoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1021    2509    3100    3055    3622    4990 \n\n\nVisualise with histogram and boxplot\n\npar(mfrow = c(1, 2))\n# histogram for smokers\n# (you can try to add sample means for each group)\n# set the range for axis limits so they look comparable\nhist(bw_smoker, main = 'birthweight: smoker mothers', xlim = c(600, 5000))\nhist(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', xlim = c(600, 5000))\n\n\n\n# boxplot \nboxplot(bw_smoker, main = 'birthweight: smoker mothers', ylim = c(600, 5000))\nboxplot(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', ylim = c(600, 5000))\n\n\n\n\nFrom the histograms and boxplots, you can spot that there seems to be slightly smaller values of baby birth weight for smoker mothers; although we can not draw a concrete conclusion by looking at the graphs. Therefore, you should carry out a hypothesis test to be more certain.\n\n\n2b)\nAre the distributions for the two groups normal? Can we conclude that smoking has an effect on the weight of a newborn? Use a t-test.\n\nqqnorm(bw_smoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_smoker)\n\n\n\nqqnorm(bw_nonsmoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_nonsmoker)\n\n\n\n# two samples t-test \nt.test(bw_smoker, bw_nonsmoker, paired = F) # welch\n\n\n    Welch Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.7095, df = 170, p-value = 0.00743\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -486.95979  -76.46677\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\nt.test(bw_smoker, bw_nonsmoker, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -492.73382  -70.69274\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\n\nFrom the Q-Q plots, you can consider that the normality assumption is fulfilled.\nThe t-test (either Welch’s two sample t-test; or two sample t-test for equal variances) indicates that there is significant difference between the two groups. Birth weight of babies from smoker mothers are significantly lower than those from non-smoker mothers (p=0.009 if using the second test).\n\n\n2c)\nDo mothers of hypertension have a tendency to have babies with a lower birth weight? (The variable for hypertension is HT)\nMake a histogram of birth weight for the two groups of mothers with and without hypertension. What do you observe?\n\n# take out the variables\n# head(birth)\nhypertension <- birth$ht\n\n\n# separate birthweight based on hypertension\nbw_hypertension <- birthweight[hypertension == 'yes']\nbw_nohypertension <- birthweight[hypertension == 'no']\n\n# can produce summary statistics\nsummary(bw_hypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1135    1775    2495    2537    3232    3790 \n\nsummary(bw_nohypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2424    2992    2972    3475    4990 \n\npar(mfrow = c(1, 2))\n# can set an x axis limit so they are comparable\nhist(bw_hypertension, main = 'birthweight: hypertension', xlim = c(600, 5000))\nhist(bw_nohypertension, main = 'birthweight: no hypertension', xlim = c(600, 5000))\n\n\n\n# boxplot \nboxplot(bw_hypertension, main = 'birthweight: hypertension', ylim = c(600, 5000))\nboxplot(bw_nohypertension, main = 'birthweight: no hypertension', ylim = c(600, 5000))\n\n\n\n\nYou can read (from the y-axis of histograms) that baby birth weight for hypertension mother group have very few observations compared to those with no hypertension mothers. This can not be spotted in the boxplot.\nThis suggests that when you carry out exploratory analysis, it’s always useful to do it in different ways.\n\n# check normality\nqqnorm(bw_hypertension, main = 'birthweight: hypertension')\nqqline(bw_hypertension)\n\n\n\nqqnorm(bw_nohypertension, main = 'birthweight: no hypertension')\nqqline(bw_nohypertension)\n\n\n\n# can check the standard deviation to see if you need welch or equal variance t-test\nc(sd(bw_hypertension), sd(bw_nohypertension))\n\n[1] 917.3405 709.2265\n\n# two samples t-test \n# t.test(bw_hypertension, bw_nohypertension, paired = F) # welch\nt.test(bw_hypertension, bw_nohypertension, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_hypertension and bw_nohypertension\nt = -2.0192, df = 187, p-value = 0.04489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -861.09734  -10.02413\nsample estimates:\nmean of x mean of y \n 2536.750  2972.311 \n\n\nThe data in hypertension group (even though scarce) fall roughly on the straight line in the QQ plot. The data on the no hyper tension group (much more data) also fall on the straight line; this suggests that the normal assumption can be assumed to be fulfilled.\nWe have checked whether the variances between the two groups are very different (rule of thumb: twice standard deviation). We can see that they do not differ by that much; so we can use the t-test with equal variance.\n\n\n2d)\nAnalyse the effect of smoking on birth weight in a table (categorical analysis): compute the relative risk (risk ratio) and carry out a test to assess the strength of association.\nAlso do it for hypertension (instead of smoking).\nWhat can you conclude?\n\n# smoking on birth weight\n# cross tabulation\nlow_bw <- birth$low\ntable(low_bw)\n\nlow_bw\nbwt <= 2500  bwt > 2500 \n         59         130 \n\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_smk <- table(smoker, low_bw)\ntb_bw_smk\n\n           low_bw\nsmoker      bwt <= 2500 bwt > 2500\n  nonsmoker          29         86\n  smoker             30         44\n\n# compute your risk ratio!\n\nchisq.test(tb_bw_smk)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_smk\nX-squared = 4.2359, df = 1, p-value = 0.03958\n\n\nExposure variable is smoker, and outcome is low birth weight of baby.\nRisk in exposed group (smoker): 30/(30+44) = 0.405\nRisk in unexposed group (nonsmoker): 29/(29+86) = 0.252\nRelative risk (risk ratio): 0.405/0.252 = 1.61\nWith the chi-square test which has a p value of 0.03, you can say that there is evidence that smoking during preganancy affects the risk of low birth weight. (We have not computed the confidence interval here, but you should report them. Use an R package)\n\n(Optional)\nThe situation for hypertension is slightly tricker.\n\n# hypertension\ntable(hypertension)\n\nhypertension\n no yes \n177  12 \n\n# cross tabulation\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_ht <- table(hypertension, low_bw)\ntb_bw_ht\n\n            low_bw\nhypertension bwt <= 2500 bwt > 2500\n         no           52        125\n         yes           7          5\n\n# compute your risk ratio!\n\nExposure variable is hypertension, and outcome is low birth weight of baby.\nRisk in exposed group (hypertension): 7/(7+5) = 0.583\nRisk in unexposed group (no hypertension): 52/(52+125) = 0.293\nRelative risk (risk ratio): 0.405/0.252 = 1.99\n\n# carry out a chi-square test\nchisq.test(tb_bw_ht)\n\nWarning in chisq.test(tb_bw_ht): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_ht\nX-squared = 3.1431, df = 1, p-value = 0.07625\n\n# can specify the computation: \nchisq.test(tb_bw_ht, correct = F) # remove the continuity correction\n\nWarning in chisq.test(tb_bw_ht, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = 1, p-value = 0.03619\n\nchisq.test(tb_bw_ht, simulate.p.value = T) # simulate p values\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = NA, p-value = 0.05197\n\n\nYou can see that the conclusion for this chi-square test is problematic, as the data is very imbalanced. The p-value is around 0.05, so you are not able to reject the null hypothesis (of no association) with full confidence. You should report your confidence interval along with the point estimate, and also report your p-value even if it is not significantly small.\nYou should also double check with results computed in R packages that have implemented relative risk."
  },
  {
    "objectID": "lab/lab_regression.html",
    "href": "lab/lab_regression.html",
    "title": "Regression analysis",
    "section": "",
    "text": "Linear regression\nTBC\n\n\nLogistic regression\n(Dataset birth might be switched out)\n\n# load data\nbirth_data <- haven::read_dta('data/birth.dta')\n\n# print the first rows of the data set\nhead(birth_data)\n\n# A tibble: 6 × 12\n     id low            age   lwt eth       smk   ptl ht      ui        fvt   ttv\n  <dbl> <dbl+lbl>    <dbl> <dbl> <dbl+l> <dbl> <dbl> <dbl+l> <dbl+l> <dbl> <dbl>\n1     4 1 [bwt < 25…    28   120 3 [oth…     1     1 0 [no]  1 [yes]     0     0\n2    10 1 [bwt < 25…    29   130 1 [whi…     0     0 0 [no]  1 [yes]     2     0\n3    11 1 [bwt < 25…    34   187 2 [bla…     1     0 1 [yes] 0 [no]      0     0\n4    13 1 [bwt < 25…    25   105 3 [oth…     0     1 1 [yes] 0 [no]      0     0\n5    15 1 [bwt < 25…    25    85 3 [oth…     0     0 0 [no]  1 [yes]     0     4\n6    16 1 [bwt < 25…    27   150 3 [oth…     0     0 0 [no]  0 [no]      0     5\n# … with 1 more variable: bwt <dbl>\n\n\n\nFit a logistic regression model for low and age\n\n# low ~ age \n\nlr_low_age <- glm(low ~ age, data = birth_data, family = 'binomial')\nsummary(lr_low_age)\n\n\nCall:\nglm(formula = low ~ age, family = \"binomial\", data = birth_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0402  -0.9018  -0.7754   1.4119   1.7800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# the result regresssion coefficient is not odds ratio\nlr_low_age$coefficients\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# regression coefficients can also be extracted in this way:\ncoefficients(lr_low_age)\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# coef(lr_low_age)\n\n# confidence interval for age and intercept\nconfint(lr_low_age)\n\n                 2.5 %      97.5 %\n(Intercept) -1.0336270 1.847399861\nage         -0.1150799 0.008986436\n\n\n\n# to get odds ratio, exponentiate \nexp(coef(lr_low_age))\n\n(Intercept)         age \n  1.4690000   0.9501333 \n\nexp(confint(lr_low_age))\n\n                2.5 %   97.5 %\n(Intercept) 0.3557144 6.343305\nage         0.8912949 1.009027"
  },
  {
    "objectID": "lab/ex_logisticreg.html",
    "href": "lab/ex_logisticreg.html",
    "title": "Exercises - Logistic regression",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg.html",
    "href": "lab/ex_linearreg.html",
    "title": "Exercises - Linear regression",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_linearreg.html",
    "href": "lab/lab_linearreg.html",
    "title": "Regression analysis",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_survival.html",
    "href": "lab/ex_survival.html",
    "title": "Exercises - Survival analysis",
    "section": "",
    "text": "Datasets\n\nExercise 1: melanoma(rda link, csv link)\nExercise 2: liggetid (rda link, csv link)\n\nR script\n\n\nExercise 1: Melanoma\nThe data concern 205 patients with malignant melanoma who were operated at the university hospital in Odense, Denmark, in the period 1962-77. The patients were followed up to death or censored at the end of the study. We shall study the effect on survival of the patient’s gender and tumor thickness.\nThe variables included in the dataset are:\n\nstatus (1=death from disease, 2=censored, 4=death from other causes)\nlifetime (years) from operation\nulceration of tumor (1=yes, 2=no)\ntumor thickness in 1/100 mm\ngender (f=1, m=2)\nage at operation in years\ngrouped tumor thickness (1: 0-1 mm, 2: 2-5 mm, 3: 5+ mm)\nlogarithm of tumor thickness\n\n\n1a)\nThe “event of interest” is defined as “death from disease”. Pre-process the variables that R will use for basic survival analysis as shown in class.\n\n\n1b)\nMake a Kaplan-Meier plot of the survival curve. What are the estimated probabilities of surviving 1 year, 2 years, 5 years?\n\n\n1c)\nCompare survival according to gender. Test the difference with a logrank test.\n\n\n1d)\nStudy survival according to grouped tumor thickness by comparing survival curves in each group and test with a logrank test.\n\n\n\nExercise 2: length of hospital stay\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nAnalyze the relationship between the variables liggetid and kjoenn with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n\n2b)\nAnalyze the relationship between the variables liggetid and slag with a Kaplan-Meier plot. Test the difference with a log-rank test."
  },
  {
    "objectID": "lab/lab_logistic_reg.html",
    "href": "lab/lab_logistic_reg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-1-blood-pressure",
    "href": "lab/ex_linearreg.html#exercise-1-blood-pressure",
    "title": "Exercises - Linear regression",
    "section": "Exercise 1 (blood pressure)",
    "text": "Exercise 1 (blood pressure)\nThe dataset bp contains data on 20 healthy adults on two variables, Age and Blood pressure. We will explore the relationship between these two variables.\n\n1a)\nLoad the dataset. Find the correlation between age and blood pressure, and test if it is significant. Compute a 95% confidence interval for the regresion parameter.\nAlso find the squared correlation coefficient between age and blood pressure. What does it mean?\n\n\n1b)\nWhat is the blood pressure for a person at age 40? For a person at age 75? Comment."
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-2-lung-function",
    "href": "lab/ex_linearreg.html#exercise-2-lung-function",
    "title": "Exercises - Linear regression",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nMake a scatter plot of pefsit2 versus pefsit1; and make a separate scatter plot of pefsit1 versus weight. Insert a regression line on top of the scatterplots.\n\n\n2b)\nCompute the correlation between pefsit1 and pefsit2; and between pefsit1 and weight.\nWhy is the correlation between the first pair closer to 1 than the second pair?\n(You can get the pair-wise correlation between many other pairs of variables using cor(your_data)).\n\n\n2c)\nCarry out two regression analysis:\n\npefsit2 as dependent variable, pefsit1 as independent variable;\npefsit1 as dependent variable, weight as independent variable.\n\nInterpret the results in relation to the scatter plots.\n\n\n2d)\nMake residual analysis for the analyses you did before. Interpret the results.\n\n\n2e)\nMake a regression analysis with pefsit1 as dependent variable, and sex and weight as independent variables. Assess the model fit. Interpret the results.\n\n\n2f)\nMake a regression analysis with pefmean as dependent variable, and try out combinations of sex, height, weight as independent variables.\nWhich variables would you include in your final analysis? How much variation is explained? Assess the model fit, and interpret the results."
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-3-length-of-hospital-stay",
    "href": "lab/ex_linearreg.html#exercise-3-length-of-hospital-stay",
    "title": "Exercises - Linear regression",
    "section": "Exercise 3 (length of hospital stay)",
    "text": "Exercise 3 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n3a)\nCreate a box plot for length of hospital stay for men and women.\n\n\n3b)\nWe want to explain the variation in lengths of hospital stay. We will look at the independent variables kjoenn (gender) and slag (stroke).\nRun a regression analysis using the dependent variable liggetid. Also perform a residual analysis. What do you think about this analysis?\n\n\n3c)\nDo the same analysis, but on the log-transformed data. The transformed variable already exists in the dataset, lnliggti. Comment on the results."
  },
  {
    "objectID": "lab/ex_logisticreg.html#exercise-1-birth",
    "href": "lab/ex_logisticreg.html#exercise-1-birth",
    "title": "Exercises - Logistic regression",
    "section": "Exercise 1 (birth)",
    "text": "Exercise 1 (birth)\nIn a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, bwt, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (id)\n\nLow birth weight (low), i.e. bwt below or above 2500g\n\nAge of the mother in years (age)\n\nWeight (in pounds) at last menstrual period (lwt)\n\nEthnicity: white, black, or other (eth)\n\nSmoking status, smoker means current smoker, nonsmoker means not smoking during pregnancy (smk)\n\nHistory of premature labour, values 0, 1, 2… (ptl)\n\nHistory of hypertension: yes vs no (ht)\n\nUterine irritability, yes vs no (ui)\n\nFirst trimester visits, values 0, 1, 2… (ftv)\n\nThird trimester visits, values 0, 1, 2… (ttv)\n\nBirth weight in grams (bwt)\n\n\n1a)\nPerform a logistic regression analysis with low as dependent variable, and age as independent variable. Interpret the results.\n\n\n\n\n\n\nRecode low as is_low_bwt\n\n\n\nWith logistic regression (glm()), you need to specify the outcome variable as\n\neither 0 or 1 (numeric values)\nor a factor (ordered so that it matches 0 or 1 category)\n\nIn the solutions we should the first option (numeric 0 or 1)\nIt could be a good practice to name another variable, is_low_bwt, so that you keep the original low variable untouched; this is useful in case you want to double check if the coding is correct or need to start over.\n\n\n\n\n1b)\nPerform a logistic regression analysis with low as dependent variable and smk as independent variable.\n\n\n1c)\nPerform a logistic regression with low as dependent variable, and eth as independent variable. Be careful with the reference category.\n\n\n1d)\nPerform a logistic regression with low as dependent variable, and age,smk and eth as independent variables.\n\n\n1e)\nBased on the above analysis, set up a result table which reports:\n\nodds ratios OR (unadjusted, and adjusted)\n95% confidence intervals for OR\np-values for OR\n\nMake sure you know how to interpret the table."
  },
  {
    "objectID": "lab/ex_logisticreg.html#exercise-2-framingham",
    "href": "lab/ex_logisticreg.html#exercise-2-framingham",
    "title": "Exercises - Logistic regression",
    "section": "Exercise 2 (framingham)",
    "text": "Exercise 2 (framingham)\nWe use the data from the Framingham study, framingham. The dataset contains a selection of n = 500 men aged 31 to 65 years.\nThe response variable is FIRSTCHD, and this is equal to 1 if the individual has coronary heart disease and 0 otherwise.\nWe have four explanatory variables:\n\nMEANSBP, the average systolic blood pressure (mmHg) of two blood pressure measurements;\nSMOKE which is smoking (1 = yes, 0 = no);\nCHOLESTEROL which is serum cholesterol in mg/dl;\nAGE (age in years).\n\n\n2a)\nAnalyse the relationship between firstchd and smoke in a logistic regression model.\n\n\n2b)\nAnalyse the relationship between firstchd and meansbp in a logistic regression model.\n\n\n2c)\nInclude also the other two explanatory variables in a logisic regression model. Interpret the results."
  },
  {
    "objectID": "lab/lab_eda_part2.html#some-advanced-visualisation-ggplot2",
    "href": "lab/lab_eda_part2.html#some-advanced-visualisation-ggplot2",
    "title": "Exploratory data analysis (Part II)",
    "section": "some advanced visualisation (ggplot2)",
    "text": "some advanced visualisation (ggplot2)"
  },
  {
    "objectID": "lab/lab_eda_part2.html#missing-data",
    "href": "lab/lab_eda_part2.html#missing-data",
    "title": "Exploratory data analysis (Part II)",
    "section": "missing data",
    "text": "missing data"
  },
  {
    "objectID": "lab/lab_survival.html#exercise-1-melanoma",
    "href": "lab/lab_survival.html#exercise-1-melanoma",
    "title": "Survival analysis",
    "section": "Exercise 1: Melanoma",
    "text": "Exercise 1: Melanoma\nThe data concern 205 patients with malignant melanoma who were operated at the university hospital in Odense, Denmark, in the period 1962-77. The patients were followed up to death or censored at the end of the study. We shall study the effect on survival of the patient’s gender and tumor thickness.\nThe variables included in the dataset are:\n\nstatus (1=death from disease, 2=censored, 4=death from other causes)\nlifetime (years) from operation\nulceration of tumor (1=yes, 2=no)\ntumor thickness in 1/100 mm\ngender (f=1, m=2)\nage at operation in years\ngrouped tumor thickness (1: 0-1 mm, 2: 2-5 mm, 3: 5+ mm)\nlogarithm of tumor thickness\n\nFirst we load the dataset, and check what columns are there.\n\n# if you do not have survival package, install it by \n# install.packages('survival')\nlibrary(survival)\n\nmelanoma <- read.csv('data/melanoma.csv')\n\nhead(melanoma)\n\n  status   lifetime ulceration tumor_thickness gender age\n1      4 0.02739726        Yes            6.76      m  76\n2      4 0.08219178         No            0.65      m  56\n3      2 0.09589041         No            1.34      m  41\n4      4 0.27123288         No            2.90      f  71\n5      1 0.50684932        Yes           12.08      m  52\n6      1 0.55890411        Yes            4.84      m  28\n  grouped_tumor_thickness logarithm_of_tumor_thickness\n1                   5+ mm                    1.9110229\n2                  0-2 mm                   -0.4307829\n3                  0-2 mm                    0.2926696\n4                  2-5 mm                    1.0647107\n5                   5+ mm                    2.4915512\n6                  2-5 mm                    1.5769147\n\ncolnames(melanoma)\n\n[1] \"status\"                       \"lifetime\"                    \n[3] \"ulceration\"                   \"tumor_thickness\"             \n[5] \"gender\"                       \"age\"                         \n[7] \"grouped_tumor_thickness\"      \"logarithm_of_tumor_thickness\"\n\n\n\n1a)\nThe “event of interest” is defined as “death from disease”. Pre-process the variables that R will use for basic survival analysis as shown in class.\nWe only need “death from melanoma” or else, hence we need to recode the status variable which originally had three categories.\n\n# if status == 1, code 1; otherwise, code 0\n# this indicates whether deaths from melanoma\ndeath <- ifelse(melanoma$status == 1, 1, 0)\n\n# check if the labels are correct\ntable(death)\n\ndeath\n  0   1 \n148  57 \n\ntable(melanoma$status)\n\n\n  1   2   4 \n 57 134  14 \n\n\nYou can see that the category melanoma$status == 1 should match death == 1, and melanoma$status of 2 and 4 are merged together and coded as death == 0.\n\n\n1b)\nMake a Kaplan-Meier plot of the survival curve. What are the estimated probabilities of surviving 1 year, 2 years, 5 years?\n\n# take lifetime variable\nlifetime <- melanoma$lifetime\n\nkm_fit <- survfit(Surv(lifetime, death) ~ 1)\nplot(km_fit)\n\n# add title and text\ntitle(main = 'Kaplan-Meier survival estimate', \n      xlab = 'Time', \n      ylab = 'Survival probability')\n\n\n\n\nNow we check only time 1, 2, 5.\n\n# time 1, 2, 5\ntme <- c(1, 2, 5)\nsummary(km_fit, times = tme)\n\nCall: survfit(formula = Surv(lifetime, death) ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    193       6    0.970  0.0120        0.947        0.994\n    2    183       9    0.925  0.0187        0.889        0.962\n    5    122      30    0.769  0.0303        0.712        0.831\n\n\n\n\n1c)\nCompare survival according to gender. Test the difference with a logrank test.\n\n# gender == 1: female; gender == 2: male\ngender <- melanoma$gender\n\nkm_fit_gender <- survfit(Surv(lifetime, death) ~ gender)\nplot(km_fit_gender, col = c('blue', 'red'))\ntitle(main = 'Kaplan-Meier survival estimate: stratify by gender', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'bottomleft', \n       legend = c('Female', 'Male'), \n       lty = c('solid', 'solid'), \n       col = c('blue','red'))\n\n\n\n\nNow we test the difference with log-rank test\n\n# test difference with logrank test \nsurvdiff(Surv(lifetime, death) ~ gender)\n\nCall:\nsurvdiff(formula = Surv(lifetime, death) ~ gender)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\ngender=f 126       28     37.1      2.25      6.47\ngender=m  79       29     19.9      4.21      6.47\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\n\n\n\n1d)\nStudy survival according to grouped tumor thickness by comparing survival curves in each group and test with a logrank test.\nFirst we do some data processing\n\n# take out the variable\ngrouped_tumor_thickness <- melanoma$grouped_tumor_thickness\n\n# get an idea how many categories\ntable(grouped_tumor_thickness)\n\ngrouped_tumor_thickness\n0-2 mm 2-5 mm  5+ mm \n   109     64     32 \n\n\nNow we plot the KM plot, and carry out a log-rank test\n\nkm_fit_tumor <- survfit(Surv(lifetime, death) ~ grouped_tumor_thickness)\nplot(km_fit_tumor, col = c('blue', 'red', 'forestgreen'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by tumor thickness', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'bottomleft', \n        legend = c('0-2 mm', '2-5 mm', '5+ mm'), \n        lty = c('solid', 'solid', 'solid'), \n        col = c('blue','red', 'forestgreen'))\n\n\n\n# test difference with logrank test \nsurvdiff(Surv(lifetime, death) ~ grouped_tumor_thickness)\n\nCall:\nsurvdiff(formula = Surv(lifetime, death) ~ grouped_tumor_thickness)\n\n                                 N Observed Expected (O-E)^2/E (O-E)^2/V\ngrouped_tumor_thickness=0-2 mm 109       13    33.75     12.75     31.36\ngrouped_tumor_thickness=2-5 mm  64       30    16.39     11.30     15.88\ngrouped_tumor_thickness=5+ mm   32       14     6.86      7.42      8.45\n\n Chisq= 31.6  on 2 degrees of freedom, p= 1e-07"
  },
  {
    "objectID": "lab/lab_survival.html#exercise-2-length-of-hospital-stay",
    "href": "lab/lab_survival.html#exercise-2-length-of-hospital-stay",
    "title": "Survival analysis",
    "section": "Exercise 2: Length of hospital stay",
    "text": "Exercise 2: Length of hospital stay\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\nliggetid <- read.csv('data/liggetid.csv')\nhead(liggetid)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n4 1910     1   11   1987       3     11    87      3    25   mann       1    1\n5 1907     1    6   1987       3     13    87      3    30 kvinne       1    0\n6 1901    12   19   1987       3     13    87      4     2 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1\n4    77       14 2.639057        0        0        0        0        0      1\n5    80       17 2.833213        0        0        0        0        0      1\n6    86       20 2.995732        0        0        0        0        0      1\n\ncolnames(liggetid)\n\n [1] \"faar\"     \"fmaan\"    \"fdag\"     \"innaar\"   \"innmaan\"  \"inndag\"  \n [7] \"utaar\"    \"utmaan\"   \"utdag\"    \"kjoenn\"   \"kom_fra\"  \"slag\"    \n[13] \"alder\"    \"liggetid\" \"lnliggti\" \"kom_fra2\" \"kom_fra3\" \"kom_fra4\"\n[19] \"kom_fra5\" \"kom_fra6\" \"status\"  \n\n# all status are 1 \nstatus <- liggetid$status\n\n\n2a)\nAnalyze the relationship between the variables liggetid and kjoenn with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n# length of stay (los) vs gender\nlos <- liggetid$liggetid\nhead(los)\n\n[1] 13 17  6 14 17 20\n\n# take out gender variable\ngender <- liggetid$kjoenn\nhead(gender)\n\n[1] \"kvinne\" \"kvinne\" \"kvinne\" \"mann\"   \"kvinne\" \"kvinne\"\n\n# fit km \nkm_liggetid_gender <- survfit(Surv(los, status) ~ gender)\n\nplot(km_liggetid_gender, col = c('blue', 'red'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by gender', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'topright', \n        legend = c('Female', 'Male'), \n        lty = c('solid', 'solid'), \n        col = c('blue','red'))\n\n\n\n\nNow we do log-rank test\n\n# log rank test \nsurvdiff(Surv(los, status) ~ gender)\n\nCall:\nsurvdiff(formula = Surv(los, status) ~ gender)\n\nn=1138, 1 observation deleted due to missingness.\n\n                N Observed Expected (O-E)^2/E (O-E)^2/V\ngender=kvinne 714      714      799      9.06      31.9\ngender=mann   424      424      339     21.36      31.9\n\n Chisq= 31.9  on 1 degrees of freedom, p= 2e-08 \n\n\n\n\n2b)\nAnalyze the relationship between the variables liggetid and slag with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n# length of stay vs stroke\n# slag == 1: yes, slag == 2: no\nstroke <- liggetid$slag\n\n# fit km \nkm_liggetid_stroke <- survfit(Surv(los, status) ~ stroke)\n\nplot(km_liggetid_stroke, col = c('blue', 'red'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by stroke', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'topright', \n        legend = c('Stroke: yes', 'Stroke: no'), \n        lty = c('solid', 'solid'), \n        col = c('blue','red'))\n\n\n\n\n\n# log rank test\nsurvdiff(Surv(los, status) ~ stroke)\n\nCall:\nsurvdiff(formula = Surv(los, status) ~ stroke)\n\nn=1054, 85 observations deleted due to missingness.\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nstroke=0 891      891      864     0.814      4.61\nstroke=1 163      163      190     3.714      4.61\n\n Chisq= 4.6  on 1 degrees of freedom, p= 0.03"
  },
  {
    "objectID": "lab/lab_linearreg.html#exercise-1-blood-pressure",
    "href": "lab/lab_linearreg.html#exercise-1-blood-pressure",
    "title": "Regression analysis",
    "section": "Exercise 1 (blood pressure)",
    "text": "Exercise 1 (blood pressure)\nThe dataset bp contains data on 20 healthy adults on two variables, Age and Blood pressure. We will explore the relationship between these two variables.\n\n1a)\nLoad the dataset. Find the correlation between age and blood pressure, and test if it is significant. Compute a 95% confidence interval for the regresion parameter.\nAlso find the squared correlation coefficient between age and blood pressure. What does it mean?\n\n# load data\nbp <- read.csv('data/bp.csv')\nhead(bp)\n\n  Age bloodpressure\n1  20           120\n2  43           128\n3  63           141\n4  26           126\n5  53           134\n6  31           128\n\n\n\n# correlation age vs bp\ncor(bp$Age, bp$bloodpressure)\n\n[1] 0.966699\n\n# or,\ncor(bp)\n\n                   Age bloodpressure\nAge           1.000000      0.966699\nbloodpressure 0.966699      1.000000\n\n# 95% CI, p-value\ncor.test(bp$Age, bp$bloodpressure) \n\n\n    Pearson's product-moment correlation\n\ndata:  bp$Age and bp$bloodpressure\nt = 16.026, df = 18, p-value = 4.239e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9160501 0.9869976\nsample estimates:\n     cor \n0.966699 \n\n\n\n\n1b)\nWhat is the blood pressure for a person at age 40? For a person at age 75? Comment.\n\n# fit a linear regression model\nmodel_age_bp <- lm(bloodpressure ~ Age, data = bp)\nsummary(model_age_bp)\n\n\nCall:\nlm(formula = bloodpressure ~ Age, data = bp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7908 -1.2777  0.1688  1.8725  2.7816 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 112.31666    1.28744   87.24  < 2e-16 ***\nAge           0.44509    0.02777   16.03 4.24e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.12 on 18 degrees of freedom\nMultiple R-squared:  0.9345,    Adjusted R-squared:  0.9309 \nF-statistic: 256.8 on 1 and 18 DF,  p-value: 4.239e-12\n\n# to predict (insert x), you need to put data in a data frame\npredict(model_age_bp, \n        newdata = data.frame(Age = c(40, 75)), \n        interval = 'prediction')\n\n       fit      lwr      upr\n1 130.1202 125.5531 134.6873\n2 145.6983 140.7698 150.6268\n\n\nCan also visualize the relationship.\n\nplot(x = bp$Age, y = bp$bloodpressure, \n     main = 'Age versus Blood Pressure', \n     xlab = 'Age', ylab = 'Blood pressure', \n     pch = 20, \n     xlim = c(15, 80))\n# add the regression line on top\nabline(lm(bloodpressure ~ Age, data = bp), \n       col = 'blue', lwd = 3)"
  },
  {
    "objectID": "lab/lab_linearreg.html#exercise-2-lung-function",
    "href": "lab/lab_linearreg.html#exercise-2-lung-function",
    "title": "Regression analysis",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nMake a scatter plot of pefsit2 versus pefsit1; and make a separate scatter plot of pefsit1 versus weight. Insert a regression line on top of the scatterplots.\n\nlung_data <- read.csv('data/PEFH98-english.csv')\n# head(lung_data)\n# assign variables (not strictly necessary)\npefsit1 <- lung_data$pefsit1\npefsit2 <- lung_data$pefsit2\nweight <- lung_data$weight\n\npar(mfrow = c(1, 2)) # make plots in 1 row 2 col\n\n# scatter plot: pefsit2 vs pefsit1\nplot(x = pefsit1, y = pefsit2, \n     main = 'PEF sit1 vs PEF sit2', \n     xlab = 'pefsit1', ylab = 'pefsit2', \n     pch = 20)\nabline(lm(pefsit2 ~ pefsit1, data = lung_data), \n       col = 'blue', lwd = 3)\n\n# scatter plot: pefsit1 vs weight\nplot(x = weight, y = pefsit1, \n     main = 'Weight vs pefsit1', \n     xlab = 'Weight', ylab = 'pefsit1', \n     pch = 20)\nabline(lm(pefsit1 ~ weight, data = lung_data), \n       col = 'blue', lwd = 3)\n\n\n\n\n\n\n2b)\nCompute the correlation between pefsit1 and pefsit2; and between pefsit1 and weight.\nWhy is the correlation between the first pair closer to 1 than the second pair?\n(You can get the pair-wise correlation between many other pairs of variables using cor(your_data)).\n\ncor(pefsit2, pefsit1) # need to remove NA here\n\n[1] NA\n\nwhich(is.na(pefsit1)) # no missing\n\ninteger(0)\n\nwhich(is.na(pefsit2)) # 66th missing\n\n[1] 66\n\n# option 1: cor() removes NA for you \n# specify use complete observations\ncor(pefsit2, pefsit1, use = 'complete.obs')\n\n[1] 0.9693111\n\n# option 2: you process (remove) the row of missing \n# from both variables (remove element 66)\npefsit2_narm <- pefsit2[!is.na(pefsit2)]\npefsit1_narm <- pefsit1[!is.na(pefsit2)]\n\n# use pefsit2_narm instead of pefsit2 to compute cor\n# should be the same\ncor(pefsit2_narm, pefsit1_narm)\n\n[1] 0.9693111\n\n\nFor pefsit1 and weight it is straightforward, as there is not missing data.\n\n# pefsit1, weight\ncor(pefsit1, weight)\n\n[1] 0.7055166\n\n\nTo compute the correlation between multiple pairs, you need to select a few variables first.\n\n# pairwise for multiple pairs \n# use age, height, weight, pefsit1, pefsit2, pefsit3, pefmean\n# select a smaller dataset \nlungdata2 <- lung_data[, c('age', 'height', 'weight', 'pefsit1', \n                           'pefsit2', 'pefsit3', 'pefmean')]\n\nhead(lungdata2, 3)\n\n  age height weight pefsit1 pefsit2 pefsit3  pefmean\n1  20    165     50     400     400     410 405.0000\n2  20    185     75     480     460     510 491.6667\n3  21    178     70     490     540     560 505.0000\n\n# produce correlation matrix for all the variables here\n# round(1.2345, digits = 2) gives 1.23\nround(cor(lungdata2, use = 'complete.obs'), digits = 2)\n\n          age height weight pefsit1 pefsit2 pefsit3 pefmean\nage      1.00  -0.15  -0.18   -0.02   -0.02   -0.01   -0.03\nheight  -0.15   1.00   0.83    0.68    0.68    0.67    0.69\nweight  -0.18   0.83   1.00    0.71    0.70    0.67    0.70\npefsit1 -0.02   0.68   0.71    1.00    0.97    0.96    0.98\npefsit2 -0.02   0.68   0.70    0.97    1.00    0.98    0.99\npefsit3 -0.01   0.67   0.67    0.96    0.98    1.00    0.98\npefmean -0.03   0.69   0.70    0.98    0.99    0.98    1.00\n\n\n\n\n2c)\nCarry out two regression analysis:\n\npefsit2 as dependent variable, pefsit1 as independent variable;\npefsit1 as dependent variable, weight as independent variable.\n\nInterpret the results in relation to the scatter plots.\n\n# pef2 vs pef 1\nlm_pef2_pef1 <- lm(pefsit2 ~ pefsit1, data = lung_data)\nsummary(lm_pef2_pef1)\n\n\nCall:\nlm(formula = pefsit2 ~ pefsit1, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-101.136  -13.458   -2.588    9.009   75.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.73107   12.75039    0.92     0.36    \npefsit1      0.98549    0.02463   40.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.66 on 103 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9396,    Adjusted R-squared:  0.939 \nF-statistic:  1601 on 1 and 103 DF,  p-value: < 2.2e-16\n\n\n\n# pef1 vs weight \nlm_pef1_weight <- lm(pefsit1 ~ weight, data = lung_data)\nsummary(lm_pef1_weight)\n\n\nCall:\nlm(formula = pefsit1 ~ weight, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-145.025  -53.081   -6.085   41.587  259.269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -30.0819    53.2502  -0.565    0.573    \nweight        7.8587     0.7741  10.152   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.96 on 104 degrees of freedom\nMultiple R-squared:  0.4978,    Adjusted R-squared:  0.4929 \nF-statistic: 103.1 on 1 and 104 DF,  p-value: < 2.2e-16\n\n\n\n\n2d)\nMake residual analysis for the analyses you did before. Interpret the results.\n\npar(mfrow = c(2, 2)) # plot 2 by 2\nplot(lm_pef2_pef1)\n\n\n\nplot(lm_pef1_weight)\n\n\n\n\n\n\n2e)\nMake a regression analysis with pefsit1 as dependent variable, and sex and weight as independent variables. Assess the model fit. Interpret the results.\n\n# pefsit1 vs (weight, gender)\n# note that we converted gender into categorical\nlm_pef1_weight_gender <- lm(pefsit1 ~ weight + gender, \n                            data = lung_data)\n# lm_pef1_weight_gender\nsummary(lm_pef1_weight_gender)\n\n\nCall:\nlm(formula = pefsit1 ~ weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-132.141  -46.988   -0.877   45.971  180.290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  224.143     60.453   3.708 0.000339 ***\nweight         3.204      0.985   3.253 0.001544 ** \ngendermale   127.280     20.017   6.359 5.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68.94 on 103 degrees of freedom\nMultiple R-squared:  0.6393,    Adjusted R-squared:  0.6323 \nF-statistic: 91.29 on 2 and 103 DF,  p-value: < 2.2e-16\n\n\n\n\n2f)\nMake a regression analysis with pefmean as dependent variable, and try out combinations of sex, height, weight as independent variables.\nWhich variables would you include in your final analysis? How much variation is explained? Assess the model fit, and interpret the results.\n\n# we can try two sets\n# 1. height weight gender\nlm_pefm_height_weight_gen <- lm(pefmean ~ height + weight + gender, \n                                data = lung_data)\n\nsummary(lm_pefm_height_weight_gen)\n\n\nCall:\nlm(formula = pefmean ~ height + weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-178.443  -42.347   -4.134   50.155  172.662 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -68.931    225.291  -0.306    0.760    \nheight         2.213      1.571   1.409    0.162    \nweight         1.956      1.301   1.504    0.136    \ngendermale   122.597     21.555   5.688 1.26e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 70.37 on 101 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6423,    Adjusted R-squared:  0.6317 \nF-statistic: 60.46 on 3 and 101 DF,  p-value: < 2.2e-16\n\n\n\n# 2. weight gender\nlm_pefm_weight_gen <- lm(pefmean ~ weight + gender, \n                                data = lung_data)\n\nsummary(lm_pefm_weight_gen)\n\n\nCall:\nlm(formula = pefmean ~ weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-172.872  -41.467    0.706   46.601  168.533 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  236.214     62.242   3.795 0.000251 ***\nweight         3.114      1.013   3.076 0.002697 ** \ngendermale   132.260     20.533   6.441 3.95e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 70.71 on 102 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6353,    Adjusted R-squared:  0.6281 \nF-statistic: 88.84 on 2 and 102 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "lab/lab_linearreg.html#exercise-3-length-of-hospital-stay",
    "href": "lab/lab_linearreg.html#exercise-3-length-of-hospital-stay",
    "title": "Regression analysis",
    "section": "Exercise 3 (length of hospital stay)",
    "text": "Exercise 3 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n3a)\nCreate a box plot for length of hospital stay for men and women.\n\nliggetid <- read.csv('data/liggetid.csv')\nhead(liggetid, 3)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1\n\n# boxplot\nboxplot(liggetid ~ kjoenn, data = liggetid)\n\n\n\n\n\n\n3b)\nWe want to explain the variation in lengths of hospital stay. We will look at the independent variables kjoenn (gender) and slag (stroke).\nRun a regression analysis using the dependent variable liggetid. Also perform a residual analysis. What do you think about this analysis?\n\n# response (dep): liggetid\n# predictor (indep): kjoenn, slag\n\nlm_ligge <- lm(liggetid ~ slag + kjoenn, \n               data = liggetid)\nsummary(lm_ligge)\n\n\nCall:\nlm(formula = liggetid ~ slag + kjoenn, data = liggetid)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-175.3 -111.7  -59.7    1.3 1072.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  142.702      8.174  17.457  < 2e-16 ***\nslag          34.557     17.194   2.010   0.0447 *  \nkjoennmann   -72.002     12.865  -5.597 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201.4 on 1050 degrees of freedom\n  (86 observations deleted due to missingness)\nMultiple R-squared:  0.03137,   Adjusted R-squared:  0.02953 \nF-statistic:    17 on 2 and 1050 DF,  p-value: 5.398e-08\n\n\n\n# visualize the residual\npar(mfrow = c(2, 2))\nplot(lm_ligge)\n\n\n\n\n\n\n3c)\nDo the same analysis, but on the log-transformed data. The transformed variable already exists in the dataset, lnliggti. Comment on the results.\n\n# response (dep): log transformed (lnliggti)\n# predictor (indep): kjoenn, slag\n\nlm_logligge <- lm(lnliggti ~ slag + kjoenn, \n                  data = liggetid)\nsummary(lm_logligge)\n\n\nCall:\nlm(formula = lnliggti ~ slag + kjoenn, data = liggetid)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9560 -0.8650 -0.1474  0.8281  3.2469 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.95601    0.05470  72.327  < 2e-16 ***\nslag         0.40896    0.11496   3.557 0.000391 ***\nkjoennmann  -0.41282    0.08604  -4.798 1.83e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.346 on 1049 degrees of freedom\n  (87 observations deleted due to missingness)\nMultiple R-squared:  0.03099,   Adjusted R-squared:  0.02915 \nF-statistic: 16.78 on 2 and 1049 DF,  p-value: 6.733e-08\n\n\n\npar(mfrow = c(2, 2))\nplot(lm_logligge)"
  },
  {
    "objectID": "lab/lab_logistic_reg.html#exercise-1-birth",
    "href": "lab/lab_logistic_reg.html#exercise-1-birth",
    "title": "Logistic regression",
    "section": "Exercise 1 (birth)",
    "text": "Exercise 1 (birth)\nIn a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, bwt, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (id)\n\nLow birth weight (low), i.e. bwt below or above 2500g\n\nAge of the mother in years (age)\n\nWeight (in pounds) at last menstrual period (lwt)\n\nEthnicity: white, black, or other (eth)\n\nSmoking status, smoker means current smoker, nonsmoker means not smoking during pregnancy (smk)\n\nHistory of premature labour, values 0, 1, 2… (ptl)\n\nHistory of hypertension: yes vs no (ht)\n\nUterine irritability, yes vs no (ui)\n\nFirst trimester visits, values 0, 1, 2… (ftv)\n\nThird trimester visits, values 0, 1, 2… (ttv)\n\nBirth weight in grams (bwt)\n\n\n# load data\nbirth_data &lt;- read.csv('data/birth.csv')\n\n# print the first rows of the data set\nhead(birth_data)\n\n  id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1  4 bwt &lt;= 2500  28 120 other    smoker   1  no yes   0   0  709\n2 10 bwt &lt;= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3 11 bwt &lt;= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4 13 bwt &lt;= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5 15 bwt &lt;= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6 16 bwt &lt;= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n\n\n\n1a)\nPerform a logistic regression analysis with low as dependent variable, and age as independent variable. Interpret the results.\n\n\n\n\n\n\nRecode low as is_low_bwt\n\n\n\nWith logistic regression (glm()), you need to specify the outcome variable as\n\neither 0 or 1 (numeric values)\nor a factor (ordered so that it matches 0 or 1 category)\n\nIn the example we should the first option (numeric 0 or 1)\nIt could be a good practice to create another variable, is_low_bwt, so that you keep the original low variable untouched; this is useful in case you want to double check if the coding is correct or need to start over.\n\n\n\n# be careful with the levels: bwt&lt;= 2500 is the cases (code as 1)\ntable(birth_data$low)\n\n\nbwt &lt;= 2500  bwt &gt; 2500 \n         59         130 \n\n# recode low variable (numeric 1 and 0)\nbirth_data$is_low_bwt &lt;- ifelse(birth_data$low == 'bwt &lt;= 2500',1,0)\n\n# check each category\ntable(birth_data$is_low_bwt)\n\n\n  0   1 \n130  59 \n\n\n\n# fit lr (using is_low_bwt)\nlr_low_age &lt;- glm(is_low_bwt ~ age, \n                  data = birth_data, \n                  family = 'binomial')\nsummary(lr_low_age)\n\n\nCall:\nglm(formula = is_low_bwt ~ age, family = \"binomial\", data = birth_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n# double check whether 0/1 categories are correct\ntable(lr_low_age$y)\n\n\n  0   1 \n130  59 \n\n\n\n# the result regresssion coefficient is not odds ratio\nlr_low_age$coefficients\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# equivalently,\n# coefficients(lr_low_age)\n# coef(lr_low_age)\n# confidence interval for beta:\n# confint(lr_low_age)\n\n\n# to get odds ratio, exponentiate \nexp(coef(lr_low_age)) # OR\n\n(Intercept)         age \n  1.4690000   0.9501333 \n\nexp(confint(lr_low_age)) # CI for OR\n\n                2.5 %   97.5 %\n(Intercept) 0.3557144 6.343305\nage         0.8912949 1.009027\n\n\n\n\n1b)\nPerform a logistic regression analysis with low as dependent variable and smk as independent variable.\n\n# low ~ smk \nlr_low_smk &lt;- glm(is_low_bwt ~ smk, \n                  data = birth_data, \n                  family = 'binomial')\nsummary(lr_low_smk)\n\n\nCall:\nglm(formula = is_low_bwt ~ smk, family = \"binomial\", data = birth_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.0871     0.2147  -5.062 4.14e-07 ***\nsmksmoker     0.7041     0.3196   2.203   0.0276 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 229.80  on 187  degrees of freedom\nAIC: 233.8\n\nNumber of Fisher Scoring iterations: 4\n\n# odds ratio \nexp(coef(lr_low_smk))\n\n(Intercept)   smksmoker \n  0.3372093   2.0219436 \n\nexp(confint(lr_low_smk))\n\n                2.5 %    97.5 %\n(Intercept) 0.2177709 0.5070199\nsmksmoker   1.0818724 3.8005817\n\n\n\n\n1c)\nPerform a logistic regression with low as dependent variable, and eth as independent variable. Be careful with the reference category.\n\n# low ~ eth \n# ethnicity needs to be factorised\n# baseline: white\nbirth_data$eth &lt;- eth &lt;- factor(birth_data$eth, \n              levels = c('white', 'black', 'other'), \n              labels = c('white', 'black', 'other'))\nhead(birth_data$eth)\n\n[1] other white black other other other\nLevels: white black other\n\n# fit lr \nlr_low_eth &lt;- glm(is_low_bwt ~ eth, \n                  data = birth_data, \n                  family = 'binomial')\nsummary(lr_low_eth)\n\n\nCall:\nglm(formula = is_low_bwt ~ eth, family = \"binomial\", data = birth_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.1550     0.2391  -4.830 1.36e-06 ***\nethblack      0.8448     0.4634   1.823   0.0683 .  \nethother      0.6362     0.3478   1.829   0.0674 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 229.66  on 186  degrees of freedom\nAIC: 235.66\n\nNumber of Fisher Scoring iterations: 4\n\n# odds ratio \nexp(coef(lr_low_eth))\n\n(Intercept)    ethblack    ethother \n  0.3150685   2.3275362   1.8892340 \n\nexp(confint(lr_low_eth))\n\n                2.5 %    97.5 %\n(Intercept) 0.1929856 0.4950281\nethblack    0.9255511 5.7746995\nethother    0.9565420 3.7578709\n\n\n\n\n1d)\nPerform a logistic regression with low as dependent variable, and age,smk and eth as independent variables.\n\nlr_low_all &lt;- glm(is_low_bwt ~ age + smk + eth, \n                  data = birth_data, \n                  family = 'binomial')\nsummary(lr_low_all)\n\n\nCall:\nglm(formula = is_low_bwt ~ age + smk + eth, family = \"binomial\", \n    data = birth_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -1.00755    0.86166  -1.169  0.24228   \nage         -0.03488    0.03340  -1.044  0.29634   \nsmksmoker    1.10055    0.37195   2.959  0.00309 **\nethblack     1.01141    0.49342   2.050  0.04039 * \nethother     1.05673    0.40596   2.603  0.00924 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 218.86  on 184  degrees of freedom\nAIC: 228.86\n\nNumber of Fisher Scoring iterations: 4\n\n# odds ratio \nexp(coef(lr_low_all))\n\n(Intercept)         age   smksmoker    ethblack    ethother \n  0.3651110   0.9657186   3.0058203   2.7494834   2.8769483 \n\nexp(confint(lr_low_all))\n\n                 2.5 %   97.5 %\n(Intercept) 0.06601379 1.967972\nage         0.90303360 1.029955\nsmksmoker   1.47208358 6.378576\nethblack    1.03958814 7.308152\nethother    1.31818618 6.531492\n\n\n\n\n1e)\nBased on the above analysis, set up a result table which reports:\n\nodds ratios OR (unadjusted, and adjusted)\n95% confidence intervals for OR\np-values for OR\n\nMake sure you know how to interpret the table."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MF9130E - Introductory course in Statistics",
    "section": "",
    "text": "Welcome!\nYou are on the course website for MF9130E - Introductory Course in Statistics.\n\nThe course is intended for students and researchers who are interested in statistics and R programming, with applications in medical and healthcare data. No previous programming experience is required to participate in this course.\nThis website is developed by the instructors of the course, hosted for free and public access on Github. The course github repository can be accessed here.\nYou can check the course page by UiO for information related to applications, evaluations and other administrative matters.\nAbout home exam: information regarding home exam will be talked about in the class on the first and last days. Assignment tasks for home exam will be put in Canvas, and you should submit your assignment in Inspera. More information about the exam\n\n\nPreparation\nYou should have a working solution of R (either installed on your own laptop, or using Posit cloud) before the course.\nIt would also be helpful if you familiarize yourself with the course website.\n\nGet Started provides some information about software installation, where to download data and code, and some resources.\nCourse material provides an overview of the material and corresponding links for each session.\nR Lab and Code hosts the lab session notes.\n\n\n\n\nSchedule\nYou can find the official course schedule provided by University of Oslo here. If there is an error in the time and place on this page, please refer to the official schedule.\n\nWeek 1\n\n\n\n\n\n\n\n\n\nDate\nTime\nTopic\nPlace\n\n\n\n\nApr 24\n12:45-16:00\nLecture: course introduction, data and descriptive statistics\nDME, Lille auditorium\n\n\nApr 25\n08:30-11:45\nGuided lab session: introduction to R, descriptive statistics with R\nDME, Auditorium 13\n\n\nApr 25\n12:45-16:00\nLecture: probability, Bayes law, diagnostic tests, distributions\nDME, Auditorium 13\n\n\nApr 26\n08:30-11:45\nGuided lab session\nDME, Store auditorium\n\n\nApr 26\n12:45-16:00\nLecture: statistical inference, hypothesis testing, confidence intervals, t-tests\nDME, Store auditorium\n\n\nApr 27\n08:30-11:45\nGuided lab session\nDME, Runde auditorium R105\n\n\nApr 27\n12:45-16:00\nLecture: categorical data analysis\nDME, Runde auditorium R105\n\n\nApr 28\n08:30-11:45\nGuided lab session\nDME, Store Auditorium\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\n\n\nDate\nTime\nTopic\nPlace\n\n\n\n\nMay 8\n08:30-11:45\nLecture and lab: exploratory data analysis, transformation, non-parametric methods\nDME, Lille auditorium\n\n\nMay 8\n12:45-16:00\nLecture and lab: sample size, statistical power\nDME, Lille auditorium\n\n\nMay 9\n08:30-11:45\nLecture: study designs, principle of clinical trials\nHelga Engs hus Auditorium 3\n\n\nMay 9\n12:45-16:00\nLecture and lab: regression I: simple regression, correlation\nHelga Engs hus Auditorium 3\n\n\nMay 10\n08:30-11:45\nLecture and lab: regression II: dummy variables, confounding, multiple regression\nDME, Auditorium 13\n\n\nMay 10\n12:45-16:00\nLecture and lab: regression III: multiple regression (continued), interactions\nDME, Auditorium 13\n\n\nMay 11\n08:30-11:45\nLecture and lab: logistic regression\nDME, Auditorium 13\n\n\nMay 11\n12:45-16:00\nLecture and lab: survival analysis, course summary\nDME, Auditorium 13"
  },
  {
    "objectID": "lab/lab_logistic_reg.html#exercise-2-framingham",
    "href": "lab/lab_logistic_reg.html#exercise-2-framingham",
    "title": "Logistic regression",
    "section": "Exercise 2 (framingham)",
    "text": "Exercise 2 (framingham)\nWe use the data from the Framingham study, framingham. The dataset contains a selection of n = 500 men aged 31 to 65 years.\nThe response variable is FIRSTCHD, and this is equal to 1 if the individual has coronary heart disease and 0 otherwise.\nWe have four explanatory variables:\n\nMEANSBP, the average systolic blood pressure (mmHg) of two blood pressure measurements;\nSMOKE which is smoking (1 = yes, 0 = no);\nCHOLESTEROL which is serum cholesterol in mg/dl;\nAGE (age in years).\n\n\n2a)\nAnalyse the relationship between firstchd and smoke in a logistic regression model.\n\nframingham &lt;- read.csv('data/framingham.csv')\nhead(framingham)\n\n  obsno age       smoke cholesterol    firstchd meansbp\n1     2  38     smoking         211 no-evidence   122.0\n2     6  43     smoking         192 no-evidence   129.0\n3    15  42     smoking         205 no-evidence   129.5\n4    22  57     smoking         196 no-evidence    77.5\n5    24  38     smoking         167 no-evidence   133.5\n6    26  48 non-smoking         347 no-evidence   133.0\n\n# check the outcome variable\n# evidence is 1, no-evidence is 0\ntable(framingham$firstchd)\n\n\n   evidence no-evidence \n         37         463 \n\n# code into 1 and 0 (numeric)\nframingham$firstchd_coded &lt;- ifelse(framingham$firstchd == 'evidence', 1, 0)\n\n# check if correct\ntable(framingham$firstchd_coded)\n\n\n  0   1 \n463  37 \n\n\n\n# firstchd, smoke \nlr_chd_smk &lt;- glm(firstchd_coded ~ smoke, \n                  data = framingham, \n                  family = 'binomial')\n# double check if category is correct\ntable(lr_chd_smk$y)  \n\n\n  0   1 \n463  37 \n\nsummary(lr_chd_smk)\n\n\nCall:\nglm(formula = firstchd_coded ~ smoke, family = \"binomial\", data = framingham)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.6655     0.3656  -7.290  3.1e-13 ***\nsmokesmoking   0.1806     0.4136   0.437    0.662    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 263.86  on 499  degrees of freedom\nResidual deviance: 263.67  on 498  degrees of freedom\nAIC: 267.67\n\nNumber of Fisher Scoring iterations: 5\n\n# odds ratio \nexp(coef(lr_chd_smk))\n\n (Intercept) smokesmoking \n  0.06956522   1.19791667 \n\nexp(confint(lr_chd_smk))\n\n                  2.5 %   97.5 %\n(Intercept)  0.03120555 0.133367\nsmokesmoking 0.55661970 2.876730\n\n\n\n\n2b)\nAnalyse the relationship between firstchd and meansbp in a logistic regression model.\n\n# firstchd, meansbp\nlr_chd_bp &lt;- glm(firstchd_coded ~ meansbp, \n                 data = framingham, \n                 family = 'binomial')\n\nsummary(lr_chd_bp)\n\n\nCall:\nglm(formula = firstchd_coded ~ meansbp, family = \"binomial\", \n    data = framingham)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.846488   1.020563  -5.729 1.01e-08 ***\nmeansbp      0.024479   0.007178   3.410 0.000649 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 263.86  on 499  degrees of freedom\nResidual deviance: 253.36  on 498  degrees of freedom\nAIC: 257.36\n\nNumber of Fisher Scoring iterations: 5\n\n# odds ratio \nexp(coef(lr_chd_bp))\n\n(Intercept)     meansbp \n 0.00289003  1.02478099 \n\nexp(confint(lr_chd_bp))\n\n                   2.5 %     97.5 %\n(Intercept) 0.0003828574 0.02159271\nmeansbp     1.0100963603 1.03921069\n\n\n\n\n2c)\nInclude also the other two explanatory variables in a logisic regression model. Interpret the results.\n\nlr_chd_all &lt;- glm(firstchd_coded ~ meansbp + smoke + cholesterol + age, \n                  data = framingham, \n                  family = 'binomial')\nsummary(lr_chd_all)\n\n\nCall:\nglm(formula = firstchd_coded ~ meansbp + smoke + cholesterol + \n    age, family = \"binomial\", data = framingham)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -10.097386   1.719736  -5.871 4.32e-09 ***\nmeansbp        0.016815   0.007698   2.184  0.02893 *  \nsmokesmoking   0.408227   0.432310   0.944  0.34502    \ncholesterol    0.007685   0.003764   2.042  0.04115 *  \nage            0.066512   0.022283   2.985  0.00284 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 263.86  on 499  degrees of freedom\nResidual deviance: 240.25  on 495  degrees of freedom\nAIC: 250.25\n\nNumber of Fisher Scoring iterations: 6\n\n# odds ratio \nround(exp(coef(lr_chd_all)), digits = 3)\n\n (Intercept)      meansbp smokesmoking  cholesterol          age \n       0.000        1.017        1.504        1.008        1.069 \n\nround(exp(confint(lr_chd_all)), digits = 3)\n\n             2.5 % 97.5 %\n(Intercept)  0.000  0.001\nmeansbp      1.001  1.032\nsmokesmoking 0.674  3.743\ncholesterol  1.000  1.015\nage          1.024  1.118"
  },
  {
    "objectID": "lab/ex_nonpara.html",
    "href": "lab/ex_nonpara.html",
    "title": "Exercise - Non-parametric tests",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/ex_nonpara.html#exercise-1-lung-function",
    "href": "lab/ex_nonpara.html#exercise-1-lung-function",
    "title": "Exercise - Non-parametric tests",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, 2 male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe shall consider the difference betweenn PEF measured in a sitting position and in a standing position, i.e. we look at diff_pef = pefstam - pefsitm. First compute this variable.\n\n\n1b)\nMake a histogram and Q-Q plot for the differences.\n\n\n1c)\nDo a non-parametric test to decide whether pefsitm and pefstam are significantly different."
  },
  {
    "objectID": "lab/ex_nonpara.html#exercise-2-length-of-hospital-stay",
    "href": "lab/ex_nonpara.html#exercise-2-length-of-hospital-stay",
    "title": "Exercise - Non-parametric tests",
    "section": "Exercise 2 (length of hospital stay)",
    "text": "Exercise 2 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nWe want to compare the length of hospital stay for patients with and without stroke. First get a better understanding of the data. Start with computing some summary (descriptive) statistics for these two variables:\n\nmean, median, min, max, 75-th percentile for length of hospital stay (liggetid);\ncount of how many patient have stroke, and do not have stroke (slag == 1 or 0)\n\nVisualize the variable liggetid using histogram and Q-Q plot, is the requirement for t-test met?\n\n\n2b)\nSince we want to compare the length of hospital stay for two groups, we need to separate the length of stay data into two groups, based on whether patients have stroke or not. Create two variables, ligt_s1 and ligt_s0.\nDo an appropriate non-parametric test. What is the difference between this test and the one you did in Exercise 1?\n\n\n\n\n\n\nSeparate one variable via index and which()\n\n\n\n\n\nPreviously, we showed how to separate one variable based on another,\n\nExercise 2c) in EDA I\nExercise 2a) in Categorical data analysis\n\nYou can try to use this method here as well; but you might notice that there are some NA in the results. This is because when you do stroke == 0 on NA, instead of giving you T or F, it gives NA; so the result of filtering liggetid keeps both T and NA.\nThese NAs can be removed post-hoc; but a clearer solution is to set 2 conditions inside the square bracket, and combine with &.\n\n!is.na(stroke) means stroke variable not NA\nstroke == 1 means stroke variable equals to 1\nwhich() identifies the indices for stroke where the conditions were met, e.g. the 4th, 22th elements.\n\n\n\n\n\n\n2c)\nDo the same analysis with a t-test on the logarithm-transformed data. The transformed variable already exists in the data set as lnliggti."
  },
  {
    "objectID": "lab/ex_nonpara.html#exercise-3-antibody",
    "href": "lab/ex_nonpara.html#exercise-3-antibody",
    "title": "Exercise - Non-parametric tests",
    "section": "Exercise 3 (antibody)",
    "text": "Exercise 3 (antibody)\n(Baker et al., 1980) We have data on concentration of antibody of Type III Group B Streptococcus (GBS) in 20 volunteers before and after immunization. The dataset is antibody.\n\n3a)\nThe comparison of the antibody levels was summarized in the report of this study as ‘\\(t = 1.8; P &gt; 0.05\\)’. Comment on the result.\n\n\n3b)\nAnalyse the data with an appropriate test. Justify your choice."
  },
  {
    "objectID": "lab/ex_nonpara.html#exercise-4-nausea",
    "href": "lab/ex_nonpara.html#exercise-4-nausea",
    "title": "Exercise - Non-parametric tests",
    "section": "Exercise 4 (nausea)",
    "text": "Exercise 4 (nausea)\n(Williams et al., 1989) Patients receiving chemotherapy as outpatients were randomized to receive either an active antiemetic treatment or placebo. The following table shows measurements (in mm) on a 100mm linear analogue self-assessment scale for nausea. The dataset is nausea.\n\ngroup: 1 is active (treatment), and 2 is placebo.\n\n\n4a)\nChoose an appropriate test for comparing the treatment and placebo groups. Justify your choice."
  },
  {
    "objectID": "lab/lab_nonpara.html#exercise-1-lung-function",
    "href": "lab/lab_nonpara.html#exercise-1-lung-function",
    "title": "Non-parametric tests",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, 2 male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe shall consider the difference betweenn PEF measured in a sitting position and in a standing position, i.e. we look at diff_pef = pefstam - pefsitm. First compute this variable.\n\nlung_data <- read.csv('data/PEFH98-english.csv')\nhead(lung_data, 3)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n\n# compute diff_pef\ndiff_pef <- lung_data$pefstam - lung_data$pefsitm\n\n\n\n1b)\nMake a histogram and Q-Q plot for the differences.\n\npar(mfrow = c(1, 2))\nhist(diff_pef)\n\n# qqplot \nqqnorm(diff_pef)\nqqline(diff_pef)\n\n\n\n\n\n\n1c)\nDo a non-parametric test to decide whether pefsitm and pefstam are significantly different.\n\n# one sample (signed rank test)\nwilcox.test(diff_pef)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  diff_pef\nV = 3547.5, p-value = 7.032e-05\nalternative hypothesis: true location is not equal to 0\n\n\n\n# alternatively\npefstam <- lung_data$pefstam\npefsitm <- lung_data$pefsitm\n\nwilcox.test(pefstam, pefsitm, paired = T) # paired \n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  pefstam and pefsitm\nV = 3547.5, p-value = 7.032e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n# the data is not severly skewed, can also use t-test \nt.test(pefstam, pefsitm, paired = T)\n\n\n    Paired t-test\n\ndata:  pefstam and pefsitm\nt = 3.6974, df = 104, p-value = 0.0003498\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  4.423204 14.656161\nsample estimates:\nmean difference \n       9.539683"
  },
  {
    "objectID": "lab/lab_nonpara.html#exercise-2-length-of-hospital-stay",
    "href": "lab/lab_nonpara.html#exercise-2-length-of-hospital-stay",
    "title": "Non-parametric tests",
    "section": "Exercise 2 (length of hospital stay)",
    "text": "Exercise 2 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nWe want to compare the length of hospital stay for patients with and without stroke. First get a better understanding of the data. Start with computing some summary (descriptive) statistics for these two variables:\n\nmean, median, min, max, 75-th percentile for length of hospital stay (liggetid);\ncount of how many patient have stroke, and do not have stroke (slag == 1 or 0)\n\nVisualize the variable liggetid using histogram and Q-Q plot, is the requirement for t-test met?\n\nliggetid <- read.csv('data/liggetid.csv')\nhead(liggetid, 3)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1\n\n\n\n# liggetid (length of stay: los)\nlos <- liggetid$liggetid\nstroke <- liggetid$slag\n\nsummary(los)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    21.0    42.0   117.1   102.0  1215.0 \n\ntable(stroke)\n\nstroke\n  0   1 \n891 163 \n\nsummary(stroke) # this shows 85 missing\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.0000  0.0000  0.1547  0.0000  1.0000      85 \n\nsum(is.na(stroke)) # 85\n\n[1] 85\n\n\n\npar(mfrow = c(1, 2))\n# visualize length of hospital stay (los)\nhist(los, breaks = 25)\n# check normality\nqqnorm(los, pch = 20, main = 'Q-Q plot for length of stay')\nqqline(los, col = 'red', lwd = 2)\n\n\n\n\n\n\n2b)\nSince we want to compare the length of hospital stay for two groups, we need to separate the length of stay data into two groups, based on whether patients have stroke or not. Create two variables, ligt_s1 and ligt_s0.\nDo an appropriate non-parametric test. What is the difference between this test and the one you did in Exercise 1?\n\n\n\n\n\n\nSeparate one variable via index and which()\n\n\n\n\n\nPreviously, we showed how to separate one variable based on another,\n\nExercise 2c) in EDA I\nExercise 2a) in Categorical data analysis\n\nYou can try to use this method here as well; but you might notice that there are some NA in the results. This is because when you do stroke == 0 on NA, instead of giving you T or F, it gives NA; so the result of filtering liggetid keeps both T and NA.\nThese NAs can be removed post-hoc; but a clearer solution is to set 2 conditions inside the square bracket, and combine with &.\n\n!is.na(stroke) means stroke variable not NA\nstroke == 1 means stroke variable equals to 1\nwhich() identifies the indices for stroke where the conditions were met, e.g. the 4th, 22th elements.\n\n\n\n\n\n# separate los \nligt_s1 <- los[which(!is.na(stroke) & stroke == 1)]\nligt_s0 <- los[which(!is.na(stroke) & stroke == 0)]\n\n# mean, median \nc(mean(ligt_s1), median(ligt_s1))\n\n[1] 145.0123  58.0000\n\nc(mean(ligt_s0), median(ligt_s0))\n\n[1] 116.7722  40.0000\n\n\n\n# visualize\npar(mfrow = c(2,2))\n\n# s1\nhist(ligt_s1, main = 'Length of hospital stay, stroke yes', \n     xlab = 'mean: 145; median: 58', \n     xlim = c(0, 1300))\nabline(v = mean(ligt_s1), col = 'blue', lwd = 2)\nabline(v = median(ligt_s1), col = 'blue', lwd = 2, lty = 'dashed')\n# qqplot\nqqnorm(ligt_s1, pch = 20, main = 'Q-Q plot for length of stay, stroke yes')\nqqline(ligt_s1, col = 'red', lwd = 2)\n\n# s0\nhist(ligt_s0, main = 'Length of hospital stay, stroke no', \n     xlab = 'mean: 116; median: 40', \n     xlim = c(0, 1300))\nabline(v = mean(ligt_s0), col = 'blue', lwd = 2)\nabline(v = median(ligt_s0), col = 'blue', lwd = 2, lty = 'dashed')\n# qqplot\nqqnorm(ligt_s0, pch = 20, main = 'Q-Q plot for length of stay, stroke no')\nqqline(ligt_s0, col = 'red', lwd = 2)\n\n\n\n\n\n# non parametric test\n# not matched; independent samples\n# wilcoxon rank sum \nwilcox.test(ligt_s1, ligt_s0)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  ligt_s1 and ligt_s0\nW = 84060, p-value = 0.001361\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nCompare with t-test\n\n\n\nIf you do a t-test (inappropriately) instead of Mann-Whitney U test, you’ll have different results!\n\n\n\n# t-test on skewed data\nt.test(ligt_s1, ligt_s0)\n\n\n    Welch Two Sample t-test\n\ndata:  ligt_s1 and ligt_s0\nt = 1.5847, df = 220.8, p-value = 0.1145\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.879646 63.359853\nsample estimates:\nmean of x mean of y \n 145.0123  116.7722 \n\n\n\n\n2c)\nDo the same analysis with a t-test on the logarithm-transformed data. The transformed variable already exists in the data set as lnliggti.\n\n# use log transformed \nloglos <- liggetid$lnliggti\nhist(loglos, main = 'Log transformed length of stay')\n\n\n\nqqnorm(loglos, pch = 20, main = 'Q-Q plot for log(length of stay)')\nqqline(loglos, col = 'red', lwd = 2)\n\n\n\n# separate data\nlogligt_s1 <- loglos[which(!is.na(stroke) & stroke == 1)]\nlogligt_s0 <- loglos[which(!is.na(stroke) & stroke == 0)]\n\nt.test(logligt_s1, logligt_s0, paired = F)\n\n\n    Welch Two Sample t-test\n\ndata:  logligt_s1 and logligt_s0\nt = 3.4218, df = 237.88, p-value = 0.0007321\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.1587474 0.5895572\nsample estimates:\nmean of x mean of y \n 4.180085  3.805933"
  },
  {
    "objectID": "lab/lab_nonpara.html#exercise-3-antibody",
    "href": "lab/lab_nonpara.html#exercise-3-antibody",
    "title": "Non-parametric tests",
    "section": "Exercise 3 (antibody)",
    "text": "Exercise 3 (antibody)\n(Baker et al., 1980) We have data on concentration of antibody of Type III Group B Streptococcus (GBS) in 20 volunteers before and after immunization. The dataset is antibody.\n\n3a)\nThe comparison of the antibody levels was summarized in the report of this study as ‘\\(t = 1.8; P > 0.05\\)’. Comment on the result.\n\nantibody <- read.csv('data/antibody.csv')\nhead(antibody)\n\n  before after\n1    0.4   0.4\n2    0.4   0.5\n3    0.4   0.5\n4    0.4   0.9\n5    0.5   0.5\n6    0.5   0.5\n\nbefore <- antibody$before\nafter <- antibody$after\n\nCompute some summary statistics for the two variables, and then reproduce the result by the t-test.\n\n# some summary stat of the data\nsummary(before)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.400   0.500   0.600   0.745   0.900   2.000 \n\nsummary(after)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.400   0.500   0.850   1.925   1.375  12.200 \n\n# reproduce the result from t-test\nt.test(antibody$before, antibody$after, paired = T)\n\n\n    Paired t-test\n\ndata:  antibody$before and antibody$after\nt = -1.8498, df = 19, p-value = 0.07996\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.5151563  0.1551563\nsample estimates:\nmean difference \n          -1.18 \n\n\n\n\n(Optional)\nWe can visually get an understanding of whether this result makes sense.\n\n# long format data (requires tidyr package)\nantibody_longdata <- tidyr::pivot_longer(antibody, \n                                         cols = tidyr::everything(), \n                                         names_to = 'group', \n                                         values_to = 'measurements')\nhead(antibody_longdata)\n\n# A tibble: 6 × 2\n  group  measurements\n  <chr>         <dbl>\n1 before          0.4\n2 after           0.4\n3 before          0.4\n4 after           0.5\n5 before          0.4\n6 after           0.5\n\n\n\npar(mfrow = c(1, 2))\nboxplot(measurements ~ group, \n        data = antibody_longdata, \n        horizontal = T, \n        main = 'Before and after measurements')\n\n# limit the y axis (horizontal x axis)\nboxplot(measurements ~ group, \n        data = antibody_longdata, \n        horizontal = T, \n        ylim = c(0, 4),\n        main = 'Limit the measurement range')\n\n\n\n\n\npar(mfrow = c(2,2))\n# histogram \nhist(before, main = 'Measurements: before')\nqqnorm(before, pch = 20, main = 'Q-Q plot for measurements: before')\nqqline(before, col = 'red', lwd = 2)\n\nhist(after, main = 'Measurements: after')\nqqnorm(after, pch = 20, main = 'Q-Q plot for measurements: after')\nqqline(after, col = 'red', lwd = 2)\n\n\n\n\n\n# histogram for the difference s\nhist(after - before, main = 'Difference (after - before)')\n\n\n\n\n\n\n3b)\nAnalyse the data with an appropriate test. Justify your choice.\n\n# matched data (paired)\n# one-sample test (signed rank)\n\nwilcox.test(before, after, paired = T)\n\nWarning in wilcox.test.default(before, after, paired = T): cannot compute exact\np-value with ties\n\n\nWarning in wilcox.test.default(before, after, paired = T): cannot compute exact\np-value with zeroes\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  before and after\nV = 2, p-value = 0.00412\nalternative hypothesis: true location shift is not equal to 0\n\n# the error means there are ties (no change)\n\n\n\n\n\n\n\nPaired vs independent data\n\n\n\nYou can try to do a test treating the data as independent. The conclusion could be different! Be careful with what you are testing.\n\n\n\n# what happens if you use independent two-samples test?\nwilcox.test(before, after, paired = F) # p = 0.14\n\nWarning in wilcox.test.default(before, after, paired = F): cannot compute exact\np-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  before and after\nW = 146.5, p-value = 0.1454\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "lab/lab_nonpara.html#exercise-4-nausea",
    "href": "lab/lab_nonpara.html#exercise-4-nausea",
    "title": "Non-parametric tests",
    "section": "Exercise 4 (nausea)",
    "text": "Exercise 4 (nausea)\n(Williams et al., 1989) Patients receiving chemotherapy as outpatients were randomized to receive either an active antiemetic treatment or placebo. The following table shows measurements (in mm) on a 100mm linear analogue self-assessment scale for nausea. The dataset is nausea.\n\ngroup: 1 is active (treatment), and 2 is placebo.\n\n\n4a)\nChoose an appropriate test for comparing the treatment and placebo groups. Justify your choice.\n\nnausea <- read.csv('data/nausea.csv')\nhead(nausea)\n\n  value group\n1     0     1\n2     0     1\n3     0     1\n4     0     1\n5     0     1\n6     2     1\n\ntreatment <- nausea$value[nausea$group == 1]\nplacebo <- nausea$value[nausea$group == 2]\n\nsummary(treatment)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.50   14.00   16.95   21.25   76.00 \n\nsummary(placebo)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   26.25   47.50   47.10   68.75   95.00 \n\n\n\n# these samples are not matched: two sample independent test\n\n# boxplot\nboxplot(value ~ group, data = nausea, \n        main = 'Nausea: treatment vs placebo')\n\n\n\n\n\nwilcox.test(treatment, placebo, paired = F)\n\nWarning in wilcox.test.default(treatment, placebo, paired = F): cannot compute\nexact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  treatment and placebo\nW = 78.5, p-value = 0.001039\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "lab/lab_eda_part2.html",
    "href": "lab/lab_eda_part2.html",
    "title": "Exploratory data analysis (Part II)",
    "section": "",
    "text": "Dataset: liggetid (rda link, csv link)\nR script"
  },
  {
    "objectID": "lab/lab_eda_part2.html#subsetting-and-rename-variables",
    "href": "lab/lab_eda_part2.html#subsetting-and-rename-variables",
    "title": "Exploratory data analysis (Part II)",
    "section": "Subsetting and rename variables",
    "text": "Subsetting and rename variables\nIn the previous section, we have used the following commands when we first explore a dataset.\n\n# basic descriptive \ndim(liggetid)\ncolnames(liggetid)\nncol(liggetid)\nsummary(liggetid) # on the entire dataset\nsummary(liggetid$liggetid) # on one variable\n\nYou can also use str() on the dataset to get some information of the data.\n\n# str (structure) tells you data type \nstr(liggetid) \n\nTo take a smaller subset of the dataset, you can write in the column name in the square bracket. You can not extract more than 1 variable using the $.\n\n# year of admission, age, sex, admission from, stroke, length of stay\ndata_los &lt;- liggetid[, c('innaar', 'alder', 'kjoenn', \n                         'kom_fra', 'slag', 'liggetid')]\nhead(data_los, 3)\n\n  innaar alder kjoenn kom_fra slag liggetid\n1   1987    81 kvinne       1    0       13\n2   1987    96 kvinne       1    0       17\n3   1987    79 kvinne       1    0        6\n\n\nNow we can rename the variables in English; or any name you prefer. Simply assign the names(dataset) with the new names.\n\n# rename in english\nnames(data_los) # same as colnames(data_los)\n\n[1] \"innaar\"   \"alder\"    \"kjoenn\"   \"kom_fra\"  \"slag\"     \"liggetid\"\n\nnames(data_los) &lt;- c('adm_year', 'age', 'sex', 'adm_from', 'stroke', 'los')\nhead(data_los, 3)\n\n  adm_year age    sex adm_from stroke los\n1     1987  81 kvinne        1      0  13\n2     1987  96 kvinne        1      0  17\n3     1987  79 kvinne        1      0   6"
  },
  {
    "objectID": "lab/lab_eda_part2.html#filter-data",
    "href": "lab/lab_eda_part2.html#filter-data",
    "title": "Exploratory data analysis (Part II)",
    "section": "Filter data",
    "text": "Filter data\nFiltering a dataset is a very common task in data analysis. It is often the case where you need to filter based on gender, a certain range of values or combined.\nNote that with base R filtering, you need to call the variable in this way: data$variable (e.g. data_los$sex). If you want to avoid this, you can try the tidyverse solution (next section).\n\n# base R filtering needs you to use data$variable inside the square bracket\n# filter based on sex == kvinne\ndsex_kvinne &lt;- data_los[data_los$sex == 'kvinne', ]\n# dsex_kvinne # print out yourself to see the outcome!\n\n# based on length of stay over 1000\ndata_los[data_los$los &gt; 1000, ]\n\n     adm_year age    sex adm_from stroke  los\n1046     1982  82 kvinne        2      1 1020\n1048     1982  86 kvinne        3      0 1022\n1062     1981  87 kvinne        2      0 1074\n1074     1981  71 kvinne        3      0 1013\n1104     1981  76 kvinne        2      0 1022\n1120     1981  85 kvinne        1      0 1215\n1128     1982  78 kvinne        3      0 1037\n1131     1982  86 kvinne        4      0 1010\n\n# stroke has some NA, let us examine those\ndstroke_na &lt;- data_los[is.na(data_los$stroke), ]\n# dstroke_na # print out yourself to see the outcome!\n\n# combine multiple conditions with &\ndata_los[data_los$adm_year == 1986 & \n           data_los$age &gt; 80 & \n           data_los$sex == 'mann' &\n           data_los$los &gt; 100, ]\n\n    adm_year age  sex adm_from stroke los\n366     1986  87 mann        2     NA 214\n399     1986  82 mann        2     NA 123\n408     1986  93 mann        2     NA 170\n552     1986  82 mann        3      0 138\n583     1986  86 mann        3      0 135"
  },
  {
    "objectID": "lab/lab_eda_part2.html#subsetting-and-rename-variables-1",
    "href": "lab/lab_eda_part2.html#subsetting-and-rename-variables-1",
    "title": "Exploratory data analysis (Part II)",
    "section": "Subsetting and rename variables",
    "text": "Subsetting and rename variables\nFirst load the packages.\n\n# install.packages('tidyverse')\nlibrary(data.table)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(RColorBrewer) # color palette\n\nUse select() (from dplyr package) to select a subset of the data\n\ndata_los2 &lt;- select(liggetid, c('innaar', 'alder',  'kjoenn', \n                                'kom_fra', 'slag', 'liggetid'))\nhead(data_los2, 3)\n\n  innaar alder kjoenn kom_fra slag liggetid\n1   1987    81 kvinne       1    0       13\n2   1987    96 kvinne       1    0       17\n3   1987    79 kvinne       1    0        6\n\n\nUse setnames(data, oldname, newname) (from data.table package) to change variable names one by one.\n\n# rename\n# note: you can also use the base R way here\nsetnames(data_los2, old = 'innaar', new = 'adm_year')\nsetnames(data_los2, old = 'alder', new = 'age')\nsetnames(data_los2, old = 'kjoenn', new = 'sex')\nsetnames(data_los2, old = 'kom_fra', new = 'adm_from')\nsetnames(data_los2, old = 'slag', new = 'stroke')\nsetnames(data_los2, old = 'liggetid', new = 'los')\n\nhead(data_los2, 3)\n\n  adm_year age    sex adm_from stroke los\n1     1987  81 kvinne        1      0  13\n2     1987  96 kvinne        1      0  17\n3     1987  79 kvinne        1      0   6"
  },
  {
    "objectID": "lab/lab_eda_part2.html#filter-data-with-dplyr",
    "href": "lab/lab_eda_part2.html#filter-data-with-dplyr",
    "title": "Exploratory data analysis (Part II)",
    "section": "Filter data with dplyr",
    "text": "Filter data with dplyr\nThe benefit of using the filter function from dplyr package is that you do not need to use the $ anymore:\n\n# filter based on sex == kvinne\ndsex_kvinne2 &lt;- filter(data_los2, sex == 'kvinne')\n# dsex_kvinne2 # print out yourself to see what happened here\n\n# based on length of stay over 1000\nfilter(data_los2, los &gt; 1000)\n\n  adm_year age    sex adm_from stroke  los\n1     1982  82 kvinne        2      1 1020\n2     1982  86 kvinne        3      0 1022\n3     1981  87 kvinne        2      0 1074\n4     1981  71 kvinne        3      0 1013\n5     1981  76 kvinne        2      0 1022\n6     1981  85 kvinne        1      0 1215\n7     1982  78 kvinne        3      0 1037\n8     1982  86 kvinne        4      0 1010\n\n# combine multiple conditions with &\n# admission year 1986, age greater than 80, sex mann\nfilter(data_los2, adm_year == 1986 & age &gt; 80 & sex == 'mann' & los &gt; 100)\n\n  adm_year age  sex adm_from stroke los\n1     1986  87 mann        2     NA 214\n2     1986  82 mann        2     NA 123\n3     1986  93 mann        2     NA 170\n4     1986  82 mann        3      0 138\n5     1986  86 mann        3      0 135"
  },
  {
    "objectID": "lab/lab_eda_part2.html#scatter-plot",
    "href": "lab/lab_eda_part2.html#scatter-plot",
    "title": "Exploratory data analysis (Part II)",
    "section": "Scatter plot",
    "text": "Scatter plot\nWe first explore age and length of hospital stay (los) with scatter plots.\n\n# age vs length of stay\n# baseR:\nplot(data_los$age, data_los$los)\n\n\n\n\n\n\n\n# ggplot \nplt_scat &lt;- ggplot(data = data_los, \n                   mapping = aes(x = age, y = los))\nplt_scat &lt;- plt_scat + geom_point() # add point\nplt_scat\n\n\n\n\n\n\n\n# can make some customizations: change titles, bigger fonts\nplt_scat &lt;- plt_scat + labs(\n  x = 'Age', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay versus age'\n)\nplt_scat &lt;- plt_scat + theme_bw() # make white background\nplt_scat &lt;- plt_scat + theme(\n  axis.text = element_text(size = 15),\n  axis.title = element_text(size = 20), \n  plot.title = element_text(size = 20)\n)\nplt_scat\n\n\n\n\n\n\n\n\nYou can add more information in the plot, with colors and shapes.\n\n# can add on more information: color\nplt_scat2 &lt;- ggplot(data = data_los, \n                    mapping = aes(x = age, y = los, shape = sex, color = sex))\nplt_scat2 &lt;- plt_scat2 + geom_point(size = 2, alpha = 0.7)\n# customize\nplt_scat2 &lt;- plt_scat2 + labs(\n  x = 'Age', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay versus age'\n)\nplt_scat2 &lt;- plt_scat2 + theme_bw() # make white background\n# change text size\nplt_scat2 &lt;- plt_scat2 + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15)\n)\n# change color\nplt_scat2 &lt;- plt_scat2 + scale_color_brewer(palette = 'Set1')\nplt_scat2\n\n\n\n\n\n\n\n\nYou can also use R packages beyond the tidyverse, to add histograms on top of the scatter plot.\n\n# can add histogram on top\nlibrary(ggExtra)\n\nggMarginal(plt_scat, type = 'histogram')"
  },
  {
    "objectID": "lab/lab_eda_part2.html#histogram",
    "href": "lab/lab_eda_part2.html#histogram",
    "title": "Exploratory data analysis (Part II)",
    "section": "Histogram",
    "text": "Histogram\n\nplt_hist &lt;- ggplot(data = data_los, mapping = aes(x = los, fill = sex))\nplt_hist &lt;- plt_hist + geom_histogram() \nplt_hist &lt;- plt_hist + facet_wrap( ~ sex, ncol = 1)\n\n# some customization\nplt_hist &lt;- plt_hist + theme_minimal() # make minimal background\n# change axis\nplt_hist &lt;- plt_hist + labs(\n  x = 'Length of hospital stay (days)', \n  y = 'Count', \n  title = 'Histograms for length of hospital stay'\n)\n# change text size\nplt_hist &lt;- plt_hist + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15)\n)\n# change color\nplt_hist &lt;- plt_hist + scale_fill_brewer(palette = 'Set1')\nplt_hist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou can plot a few histogram (or density) in the same plot.\n\nlibrary(ggridges)\nplt_ridge &lt;- ggplot(data = data_los, \n                    mapping = aes(x = los, y = adm_year, fill = sex))\nplt_ridge &lt;- plt_ridge + geom_density_ridges(alpha = 0.6) \nplt_ridge &lt;- plt_ridge + theme_ridges()\nplt_ridge &lt;- plt_ridge + labs(\n  x = 'Length of hosptial stay (days)', \n  y = 'Admission year', \n  title = 'Length of stay in each year, for each gender'\n)\n# change color\nplt_ridge &lt;- plt_ridge + scale_fill_brewer(palette = 'Set1')\nplt_ridge\n\nPicking joint bandwidth of 32.7"
  },
  {
    "objectID": "lab/lab_eda_part2.html#box-plot",
    "href": "lab/lab_eda_part2.html#box-plot",
    "title": "Exploratory data analysis (Part II)",
    "section": "Box plot",
    "text": "Box plot\n\n# we try to plot length of stay versus year, and potentially sex and adm type\n# base R is limited in this regard\npar(mfrow = c(1, 3))\nboxplot(los ~ adm_year, data = data_los, main = 'Length of stay vs year')\nboxplot(los ~ sex, data = data_los,  horizontal = T, main = 'Length of stay vs sex')\nboxplot(los ~ adm_from, data = data_los, horizontal = T,\n        main = 'Length of stay vs admission type')\n\n\n\n\n\n\n\n\n\n# ggplot can make more flexible plots\n# add color \nplt_box &lt;- ggplot(data = data_los, \n                  mapping = aes(x = adm_year, y = los, fill = sex))\nplt_box &lt;- plt_box + geom_boxplot(outlier.size = 1)\n# plt_box &lt;- plt_box + facet_wrap( ~ sex)\nplt_box &lt;- plt_box + coord_flip()\n\n# customize\nplt_box &lt;- plt_box + theme_bw() # make white background\nplt_box &lt;- plt_box + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, both men and women'\n)\nplt_box &lt;- plt_box + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12)\n)\n\nplt_box &lt;- plt_box + scale_fill_brewer(palette = 'Set1')\nplt_box \n\n\n\n\n\n\n\n\n\n# add even more information \nplt_box2 &lt;- ggplot(data = data_los, \n                  mapping = aes(x = adm_year, y = los, fill = sex))\nplt_box2 &lt;- plt_box2 + geom_boxplot(outlier.size = 0.8)\nplt_box2 &lt;- plt_box2 + facet_wrap( ~ adm_from)\n\n\n# customize\nplt_box2 &lt;- plt_box2 + theme_bw() # make white background\nplt_box2 &lt;- plt_box2 + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, each type of admission'\n)\nplt_box2 &lt;- plt_box2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12), \n  axis.text.x = element_text(angle = 45) # more readable\n)\n\nplt_box2 &lt;- plt_box2 + scale_fill_brewer(palette = 'Set1')\nplt_box2"
  },
  {
    "objectID": "lab/lab_eda_part2.html#overview",
    "href": "lab/lab_eda_part2.html#overview",
    "title": "Exploratory data analysis (Part II)",
    "section": "Overview",
    "text": "Overview\nIn the previous section (EDA Part I), we have introduced some basic manipulation, where we focused mostly on 1 or 2 variables.\nIn this part, we will introduce how to\n\nselect multiple columns from a data frame (base R and tidyverse);\nassign new column names for variables (base R and tidyverse);\nexplore a dataset with slightly more advanced visualization using ggplot.\n\nTidyverse is a collection of R packages for data manipulation and visualization. It is widely used in the R community, both in academia and industry.\nThis course is introductory statistics where statistical methodology is the focus. However, we think that exploratory data analysis (EDA) and an intuitive way of understanding of your data is crucial for selecting the correct method; that is why we have this section.\n\n\n\n\n\n\nDo I need to learn tidyverse and ggplot?\n\n\n\nTidyverse data manipulation and ggplot are for slightly more experienced R users; no, you do not need them for your home exam.\nIf you are new to R, focus on base R functions and plots (scatter, boxplot, histogram, qqplot).\n\n\n\nExplore dataset: length of hospital stay\nThe data liggetid was collected at the Geriatric Department at Ullevål Sykehus. For a complete description of the variables, please refer to Exercise 2 in Non-parametric tests.\nWe will focus on the following variables:\n\nYear of hospital admission (innaar)\nGender, men and women (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\n\nWe will first select a subset of the variables above from the original dataset, rename the variables, filter (to extract rows or subjects from) the dataset based on one or several conditions; and then introduce some advanced visualization techniques as part of the exploration (not required in home exam).\n\nliggetid &lt;- read.csv('data/liggetid.csv')\nhead(liggetid, 3)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1"
  },
  {
    "objectID": "lab/lab_nonpara.html",
    "href": "lab/lab_nonpara.html",
    "title": "Non-parametric tests",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_linearreg-I.html",
    "href": "lab/lab_linearreg-I.html",
    "title": "Solutions - Linear regression I",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_linearreg-I.html#exercise-1-lung-function",
    "href": "lab/lab_linearreg-I.html#exercise-1-lung-function",
    "title": "Solutions - Linear regression I",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sitting position, and three times in a standing position.\nThe variables are\n\nAge (years)\n\nGender (female, male)\n\nHeight (cm)\n\nWeight (kg)\n\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\n\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\n\nMean of the three measurements made in a sitting position (pefsitm)\n\nMean of the three measurements made in a standing position (pefstam)\n\nMean of all six PEF values (pefmean)\n\n\n1a)\nMake a scatter plot of pefsit2 versus pefsit1; and make a separate scatter plot of pefsit1 versus weight. Insert a regression line on top of the scatter plots.\nNote: You can add a regression line on top of an existing scatter plot by using the abline(reg = lm(y ~ x)), where x is the variable in the x-axis and y is the variable on the y-axis of the scatterplot.\n\nlung_data <- read.csv('data/PEFH98-english.csv')\n\n# head(lung_data)\n# assign variables (not strictly necessary)\npefsit1 <- lung_data$pefsit1\npefsit2 <- lung_data$pefsit2\nweight <- lung_data$weight\n\n# scatter plot: pefsit2 vs pefsit1\n\n#Simple version (using default plot options):\nplot(x = pefsit1, y = pefsit2)\nabline(lm(pefsit2 ~ pefsit1, data = lung_data))\n\n\n\n#Alternative using the formula notation:\n# plot(pefsit2 ~ pefsit1, data=lung_data)\n\npar(mfrow = c(1, 2)) # make plots in 1 row 2 col\n\n#More elaborate version:\nplot(x = pefsit1, y = pefsit2, \n     main = 'PEF sit1 vs PEF sit2',         #add a plot heading\n     xlab = 'pefsit1', ylab = 'pefsit2',    #change the axis labels\n     pch = 20)                              #change the point type\nabline(lm(pefsit2 ~ pefsit1, data = lung_data), \n       col = 'blue', lwd = 3)     #change the line colour and width\n\n# scatter plot: pefsit1 vs weight\nplot(x = weight, y = pefsit1, \n     main = 'Weight vs pefsit1', \n     xlab = 'Weight', ylab = 'pefsit1', \n     pch = 20)\nabline(lm(pefsit1 ~ weight, data = lung_data), \n       col = 'blue', lwd = 3)\n\n\n\n\nBoth graphs show a quite nice linear association.\n\n\n1b)\nCompute the correlation between pefsit1 and pefsit2; and between pefsit1 and weight.\nWhy is the correlation between the first pair closer to 1 than the second pair?\nNote: You can get the pair-wise correlation between many other pairs of variables using cor(your_data). Be mindful of missing values in your data; if there are any missing values, you can still calculate the correlation by using `cor(your_data, use=‘pairwise.complete.obs’).\n\ncor(pefsit2, pefsit1) # need to remove NA here\n\n[1] NA\n\nwhich(is.na(pefsit1)) # no missing\n\ninteger(0)\n\nwhich(is.na(pefsit2)) # 66th missing\n\n[1] 66\n\n# option 1: cor() removes NA for you \n# specify use complete observations\ncor(pefsit2, pefsit1, use = 'pairwise.complete.obs')\n\n[1] 0.9693111\n\n# option 2 (advanced): you process (remove) the row of missing \n# from both variables (remove element 66)\npefsit2_narm <- pefsit2[!is.na(pefsit2)]\npefsit1_narm <- pefsit1[!is.na(pefsit2)]\n\n# use pefsit2_narm instead of pefsit2 to compute cor\n# should be the same\ncor(pefsit2_narm, pefsit1_narm)\n\n[1] 0.9693111\n\n# option 3: cor.test automatically removes missing values:\ncor.test(pefsit2, pefsit1)\n\n\n    Pearson's product-moment correlation\n\ndata:  pefsit2 and pefsit1\nt = 40.016, df = 103, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9550846 0.9790797\nsample estimates:\n      cor \n0.9693111 \n\n\nThe output indicates a very significant correlation between pefsit2 and pefsit1 (r=0.97, p < 0.001). This could be expected from observing the scatterplot.\nFor pefsit1 and weight it is straightforward, as there is not missing data.\n\n# pefsit1, weight\ncor.test(pefsit1, weight)\n\n\n    Pearson's product-moment correlation\n\ndata:  pefsit1 and weight\nt = 10.152, df = 104, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5948116 0.7899585\nsample estimates:\n      cor \n0.7055166 \n\n\nThis again shows a very small p-value (r = 0.71, p < 0.001), which indicates that the correlation between pefsit1 and weight is significant. However, the value of the correlation is slightly lower than for pefsit2 and pefsit1, and this corresponds to the slightly worse fit of the line in the scatter plot.\nTo compute the correlation between multiple pairs, you need to select a few variables first.\n\n# pairwise for multiple pairs \n# use age, height, weight, pefsit1, pefsit2, pefsit3, pefmean\n# select a smaller dataset \nlungdata2 <- lung_data[, c('age', 'height', 'weight', 'pefsit1', \n                           'pefsit2', 'pefsit3', 'pefmean')]\n\nhead(lungdata2, 3)\n\n  age height weight pefsit1 pefsit2 pefsit3  pefmean\n1  20    165     50     400     400     410 405.0000\n2  20    185     75     480     460     510 491.6667\n3  21    178     70     490     540     560 505.0000\n\n# produce correlation matrix for all the variables here\n# round(1.2345, digits = 2) gives 1.23\nround(cor(lungdata2, use = 'pairwise.complete.obs'), digits = 2)\n\n          age height weight pefsit1 pefsit2 pefsit3 pefmean\nage      1.00  -0.14  -0.17   -0.01   -0.02   -0.01   -0.03\nheight  -0.14   1.00   0.83    0.68    0.68    0.67    0.69\nweight  -0.17   0.83   1.00    0.71    0.70    0.67    0.70\npefsit1 -0.01   0.68   0.71    1.00    0.97    0.96    0.98\npefsit2 -0.02   0.68   0.70    0.97    1.00    0.98    0.99\npefsit3 -0.01   0.67   0.67    0.96    0.98    1.00    0.98\npefmean -0.03   0.69   0.70    0.98    0.99    0.98    1.00\n\n\nThe correlation table shows that age has a very low correlation to the other variables in the dataset, that height and weight are quite well correlated to the pefsit measurements, while (as expected) all pefsit measurements are very correlated with one another.\n\n\n1c)\nCarry out two regression analysis:\n\npefsit2 as dependent variable (y), pefsit1 as independent variable (x);\npefsitm as dependent variable (y), weight as independent variable (x).\n\nInterpret the results in relation to the scatter plots.\n\n# pef2 vs pef 1\nlm_pef2_pef1 <- lm(pefsit2 ~ pefsit1, data = lung_data)\nsummary(lm_pef2_pef1)\n\n\nCall:\nlm(formula = pefsit2 ~ pefsit1, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-101.136  -13.458   -2.588    9.009   75.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.73107   12.75039    0.92     0.36    \npefsit1      0.98549    0.02463   40.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.66 on 103 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9396,    Adjusted R-squared:  0.939 \nF-statistic:  1601 on 1 and 103 DF,  p-value: < 2.2e-16\n\nconfint(lm_pef2_pef1)\n\n                  2.5 %    97.5 %\n(Intercept) -13.5563243 37.018463\npefsit1       0.9366442  1.034329\n\n\nThe regression coefficient for pefsit1 is highly significant (p < 0.001), the estimated regression line is pefsit2 = 11.7 + 0.98 * pefsit1, with the 95% confidence interval for the slope coefficient being (0.94, 1.03), and R^2 = 93.96%. The model is highly statistically significant (p < 0.001).\n\n# pef1 vs weight \nlm_pef1_weight <- lm(pefsit1 ~ weight, data = lung_data)\nsummary(lm_pef1_weight)\n\n\nCall:\nlm(formula = pefsit1 ~ weight, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-145.025  -53.081   -6.085   41.587  259.269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -30.0819    53.2502  -0.565    0.573    \nweight        7.8587     0.7741  10.152   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.96 on 104 degrees of freedom\nMultiple R-squared:  0.4978,    Adjusted R-squared:  0.4929 \nF-statistic: 103.1 on 1 and 104 DF,  p-value: < 2.2e-16\n\nconfint(lm_pef1_weight)\n\n                  2.5 %    97.5 %\n(Intercept) -135.678966 75.515264\nweight         6.323643  9.393683\n\n\nThe regression coeficient for weight is highly significant (p < 0.001), the estimated regression line is pefsit1 = -30.08+7.86 * weight, with the 95% confidence interval for the slope coefficient being (6.32, 9.39). This second model seems again highly statistically significant, but the R^2 is much lower (49.78%).\nSome comments on the two analyses:\n\nWe get a much better fit for pefsit1 vs pefsit2 than we get for pefsit1 vs weight. This is reasonable since the two PEF-values measure the same underlying quantity.\n\nThe confidence intervals tell us what the uncertainty in the estimate is. We have 95% confidence that the confidence interval covers the population value.\n\n\n\n1d)\nPerform a residual analysis for the analyses you did before. Interpret the results.\n\npar(mfrow = c(2, 2)) # plot 2 by 2\nplot(lm_pef2_pef1)\n\n\n\nplot(lm_pef1_weight)\n\n\n\n\nThe plots show a quite good adherence of the residuals to the normal distribution."
  },
  {
    "objectID": "lab/lab_linearreg-I.html#exercise-2-blood-pressure",
    "href": "lab/lab_linearreg-I.html#exercise-2-blood-pressure",
    "title": "Solutions - Linear regression I",
    "section": "Exercise 2 (blood pressure)",
    "text": "Exercise 2 (blood pressure)\nThe dataset bp contains data on 20 healthy adults on two variables, Age and Blood pressure. We will explore the relationship between these two variables.\n\n2a)\nLoad the dataset. Find the correlation between age and blood pressure, and test if it is significant.\nFit a simple linear regression model, where blood pressure is the dependent variable and age is the independent variable. What is the 95% confidence interval for the regression slope parameter?\nAlso find the squared correlation coefficient between age and blood pressure. What does it mean?\nBonus task (homework): Plot the regression line, and check the residuals.\n\n# load data\nbp <- read.csv('data/bp.csv')\nhead(bp)\n\n  Age bloodpressure\n1  20           120\n2  43           128\n3  63           141\n4  26           126\n5  53           134\n6  31           128\n\n\n\n# correlation age vs bp\ncor(bp$Age, bp$bloodpressure)\n\n[1] 0.966699\n\n# or,\ncor(bp)\n\n                   Age bloodpressure\nAge           1.000000      0.966699\nbloodpressure 0.966699      1.000000\n\n# 95% CI, p-value\ncor.test(bp$Age, bp$bloodpressure) \n\n\n    Pearson's product-moment correlation\n\ndata:  bp$Age and bp$bloodpressure\nt = 16.026, df = 18, p-value = 4.239e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9160501 0.9869976\nsample estimates:\n     cor \n0.966699 \n\n\nComment: the correlation between age and blood pressure is very close to 1 (0.9667), and it is highly statistically significant (p < 0.001).\n\n\n2b)\nWhat is the blood pressure for a person at age 40? For a person at age 75? Comment.\n\n# fit a linear regression model\nmodel_age_bp <- lm(bloodpressure ~ Age, data = bp)\nsummary(model_age_bp)\n\n\nCall:\nlm(formula = bloodpressure ~ Age, data = bp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7908 -1.2777  0.1688  1.8725  2.7816 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 112.31666    1.28744   87.24  < 2e-16 ***\nAge           0.44509    0.02777   16.03 4.24e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.12 on 18 degrees of freedom\nMultiple R-squared:  0.9345,    Adjusted R-squared:  0.9309 \nF-statistic: 256.8 on 1 and 18 DF,  p-value: 4.239e-12\n\nconfint(model_age_bp)\n\n                  2.5 %      97.5 %\n(Intercept) 109.6118594 115.0214613\nAge           0.3867409   0.5034373\n\n# to predict (insert x), you need to put data in a data frame\npredict(model_age_bp, \n        newdata = data.frame(Age = c(40, 75)), \n        interval = 'prediction')\n\n       fit      lwr      upr\n1 130.1202 125.5531 134.6873\n2 145.6983 140.7698 150.6268\n\n\nThe estimated regression parameter is 0.445, and the 95% confidence interval is (0.387, 0.503). The squared correlation coefficient (R^2) between age and blood pressure is 0.935: this value is very close to 1 (as expected, having observed also a large correlation), and it means that there is a very good fit. In order to conclude that the model is adequate we should conduct a residual analysis as well.\nThe regression parameter is positive, meaning that blood pressure increases with age: the predicted blood pressure for a person at age 40 is 112.317 + 0.445*40 = 130.12, at age 75 it becomes 112.317 + 0.445*75 = 145.69.\nBonus task (homework): Plot the regression line, and check the residuals.\n\nplot(x = bp$Age, y = bp$bloodpressure, xlim = c(15, 80))\n# add the regression line on top\nabline(lm(bloodpressure ~ Age, data = bp), col = 'blue')\n\n\n\n# residual plots\npar(mfrow = c(2, 2)) # plot 2 by 2\nplot(lm(bloodpressure ~ Age, data = bp))"
  },
  {
    "objectID": "lab/lab_linearreg-II.html",
    "href": "lab/lab_linearreg-II.html",
    "title": "Solutions - Linear regression II",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_linearreg-II.html#exercise-1-lung-function",
    "href": "lab/lab_linearreg-II.html#exercise-1-lung-function",
    "title": "Solutions - Linear regression II",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nWe continue with the analysis of the lung function data (PEFH98-english) by including more than one independent variable in the analysis.\n\n1a)\nMake a regression analysis with pefsit1 as dependent variable, and sex and weight as independent variables. Assess the model fit. Interpret the results.\n\nlung_data &lt;- read.csv('data/PEFH98-english.csv')\n\n# pefsit1 vs (weight, gender)\n# note that we converted gender into categorical\nlm_pef1_weight_gender &lt;- lm(pefsit1 ~ weight + gender, \n                            data = lung_data)\n# lm_pef1_weight_gender\nsummary(lm_pef1_weight_gender)\n\n\nCall:\nlm(formula = pefsit1 ~ weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-132.141  -46.988   -0.877   45.971  180.290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  224.143     60.453   3.708 0.000339 ***\nweight         3.204      0.985   3.253 0.001544 ** \ngendermale   127.280     20.017   6.359 5.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68.94 on 103 degrees of freedom\nMultiple R-squared:  0.6393,    Adjusted R-squared:  0.6323 \nF-statistic: 91.29 on 2 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nComments:\n\nThe regression coefficient for gender is 127.3. This is the estimated difference in lung function between men and women given the same weight.\n\nThe regression coefficient for weight is 3.20.\n\nBoth predictors are highly significant (p &lt; 0.001).\n\nThe explained variation from the regression model is 0.639 (R^2).\n\nLet us check the residuals of the model to conclude on the model adequacy in terms of assumptions. The residuals fit well to a normal distribution.\n\n#residual plots\npar(mfrow=c(2,2))\nplot(lm_pef1_weight_gender)\n\n\n\n\n\n\n\n\n\n\n1b)\nPerform a regression analysis with pefmean as dependent variable, and try out combinations of gender, height, and weight as independent variables.\nWhich variables would you include in your final analysis? How much variation is explained? Assess the model fit, and interpret the results.\nFirst, include all three variables:\n\n# 1. height weight gender\nlm_pefm_height_weight_gen &lt;- lm(pefmean ~ height + weight + gender, data = lung_data)\n\nsummary(lm_pefm_height_weight_gen)\n\n\nCall:\nlm(formula = pefmean ~ height + weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-178.443  -42.347   -4.134   50.155  172.662 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -68.931    225.291  -0.306    0.760    \nheight         2.213      1.571   1.409    0.162    \nweight         1.956      1.301   1.504    0.136    \ngendermale   122.597     21.555   5.688 1.26e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 70.37 on 101 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6423,    Adjusted R-squared:  0.6317 \nF-statistic: 60.46 on 3 and 101 DF,  p-value: &lt; 2.2e-16\n\n\nComments:\n\nThe regression coefficient for gender is 122.6. This is the estimated difference in lung function between men and women given the same height and weight.\n\nThe regression coefficient for height is 2.213 and for weight 1.956. These two predictors are not significant. The reason is that height and weight are highly correlated, and thus one of them is sufficient: we drop height and keep weight, because the latter has the smallest p-value.\n\nThe variation explained by the model is 0.642.\n\nIn the second step, we remove height:\n\n# 2. weight gender\nlm_pefm_weight_gen &lt;- lm(pefmean ~ weight + gender, data = lung_data)\n\nsummary(lm_pefm_weight_gen)\n\n\nCall:\nlm(formula = pefmean ~ weight + gender, data = lung_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-172.872  -41.467    0.706   46.601  168.533 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  236.214     62.242   3.795 0.000251 ***\nweight         3.114      1.013   3.076 0.002697 ** \ngendermale   132.260     20.533   6.441 3.95e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 70.71 on 102 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6353,    Adjusted R-squared:  0.6281 \nF-statistic: 88.84 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nComments:\n\nWe now explain almost as much of the variation as in the previous analysis (63.5% vs 64.2%).\n\nWeight is significant as well as gender.\n\nSo this second model looks better than the previous one. Is this model also good in terms of residuals? (remember to always check this!). From the histogram and the normal probability plot, the residuals appear to be normally distributed.\n\n#residual plots\npar(mfrow=c(2,2))\nplot(lm_pefm_weight_gen)\n\n\n\n\n\n\n\n\nConclusion: We choose the best fitting variables and avoid including variables that are highly correlated. Always good to try and reduce the model in a sensible way."
  },
  {
    "objectID": "lab/lab_linearreg-II.html#exercise-2-length-of-hospital-stay",
    "href": "lab/lab_linearreg-II.html#exercise-2-length-of-hospital-stay",
    "title": "Solutions - Linear regression II",
    "section": "Exercise 2 (length of hospital stay)",
    "text": "Exercise 2 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nCreate a boxplot for length of hospital stay for men and women.\n\nliggetid &lt;- read.csv('data/liggetid.csv')\nhead(liggetid, 3)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1\n\n# boxplot\nboxplot(liggetid ~ kjoenn, data = liggetid)\n\n\n\n\n\n\n\n\nThe plots show a quite skewed distribution of the variable liggetid.\n\n\n2b)\nWe want to explain the variation in lengths of hospital stay. We will look at the independent variables kjoenn (gender) and slag (stroke).\nRun a regression analysis using the dependent variable liggetid. Also perform a residual analysis. What do you think about this analysis?\n\n# response (dep): liggetid\n# predictor (indep): kjoenn, slag\n\nlm_ligge &lt;- lm(liggetid ~ slag + kjoenn, data = liggetid)\nsummary(lm_ligge)\n\n\nCall:\nlm(formula = liggetid ~ slag + kjoenn, data = liggetid)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-175.3 -111.7  -59.7    1.3 1072.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  142.702      8.174  17.457  &lt; 2e-16 ***\nslag          34.557     17.194   2.010   0.0447 *  \nkjoennmann   -72.002     12.865  -5.597 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201.4 on 1050 degrees of freedom\n  (86 observations deleted due to missingness)\nMultiple R-squared:  0.03137,   Adjusted R-squared:  0.02953 \nF-statistic:    17 on 2 and 1050 DF,  p-value: 5.398e-08\n\n\nComments:\n\nExplained variation is just 3.1%. The reason is that both independent variables are dichotomous (i.e. have just two values). In such cases one sometimes sees a very small explained variation even though the variables may have an important effect.\n\nThe two predictors seem significant. Can we conclude that we are satisfied with the model? We first have to check residuals, see below.\n\nImportantly, when doing so it turns out that the residuals have a very skewed distribution and do not fit well to a normal distribution. Moreover, the normal probability plot shows a huge deviation from the straight line. This fits with the fact that the data are very skewed, also showing many extreme values, as was already evident from the boxplot.\n\n# visualize the residuals\npar(mfrow = c(2, 2))\nplot(lm_ligge)\n\n\n\n\n\n\n\n\n\n\n2c)\nDo the same analysis, but on the log-transformed data. The transformed variable already exists in the dataset, lnliggti. Comment on the results.\n\n# response (dep): log transformed (lnliggti)\n# predictor (indep): kjoenn, slag\n\nlm_logligge &lt;- lm(lnliggti ~ slag + kjoenn, \n                  data = liggetid)\nsummary(lm_logligge)\n\n\nCall:\nlm(formula = lnliggti ~ slag + kjoenn, data = liggetid)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9560 -0.8650 -0.1474  0.8281  3.2469 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.95601    0.05470  72.327  &lt; 2e-16 ***\nslag         0.40896    0.11496   3.557 0.000391 ***\nkjoennmann  -0.41282    0.08604  -4.798 1.83e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.346 on 1049 degrees of freedom\n  (87 observations deleted due to missingness)\nMultiple R-squared:  0.03099,   Adjusted R-squared:  0.02915 \nF-statistic: 16.78 on 2 and 1049 DF,  p-value: 6.733e-08\n\n\nThere is a clear significant effect of both predictors when using the response on a log scale. The explained variation is unchanged. Are the residuals better? Yes! Now the residuals appear normally distributed.\n\npar(mfrow = c(2, 2))\nplot(lm_logligge)\n\n\n\n\n\n\n\n\n\n\n2d) Bonus task (do this only if you still have time)\n(A somewhat more mathematical exercise.) Set up the regression equation for lnliggti as estimated in the previous task. Use the exponential function exp() on both sides of the equation. What kind of equation do you get now for liggetid? How much does the length of hospital stay increase for those who have a stroke? Comment.\nTo write the regression model on the original scale we start from the model fit on log scale: log(liggetid) = 3.956 - 0.413 * kjoenn + 0.409 * slag and then invert the relationship to get back on the original scale liggetid = exp(3.956 - 0.413 * kjoenn + 0.409 * slag) which means liggetid = 52.25 * exp(-0.413 * kjoenn) * exp(0.409 * slag)\nSo, the interpretation of the model is that women without stroke (slag) have an average length of hospital stay (liggetid) of 52.25 days, while those with stroke have an average hospital stay of 52.5 * exp(0.409) = 78.65 days. On the other hand, men without stroke have an average hospital stay of 52.25 * exp(-0.413) = 34.57 days, while those with stroke have an average stay of 52.25 * exp(-0.413) * exp(0.409) = 52.04 days. Thus, the hospital stay is on average shorter for men, as already evidenced by the boxplots in task a)."
  },
  {
    "objectID": "lab/ex_linearreg-II.html",
    "href": "lab/ex_linearreg-II.html",
    "title": "Exercises - Linear regression II",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg-II.html#exercise-1-lung-function",
    "href": "lab/ex_linearreg-II.html#exercise-1-lung-function",
    "title": "Exercises - Linear regression II",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nWe continue with the analysis of the lung function data (PEFH98-english) by including more than one independent variable in the analysis.\n\n1a)\nMake a regression analysis with pefsit1 as dependent variable, and sex and weight as independent variables. Assess the model fit. Interpret the results.\n\n\n1b)\nPerform a regression analysis with pefmean as dependent variable, and try out combinations of sex, height, and weight as independent variables.\nWhich variables would you include in your final analysis? How much variation is explained? Assess the model fit, and interpret the results."
  },
  {
    "objectID": "lab/ex_linearreg-II.html#exercise-2-length-of-hospital-stay",
    "href": "lab/ex_linearreg-II.html#exercise-2-length-of-hospital-stay",
    "title": "Exercises - Linear regression II",
    "section": "Exercise 2 (length of hospital stay)",
    "text": "Exercise 2 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nCreate a boxplot for length of hospital stay for men and women.\n\n\n2b)\nWe want to explain the variation in lengths of hospital stay. We will look at the independent variables kjoenn (gender) and slag (stroke).\nRun a regression analysis using the dependent variable liggetid. Also perform a residual analysis. What do you think about this analysis?\n\n\n2c)\nDo the same analysis, but on the log-transformed data. The transformed variable already exists in the dataset, lnliggti. Comment on the results.\n\n\n2d) Bonus task (do this only if you still have time)\n(A somewhat more mathematical exercise.) Set up the regression equation for lnliggti as estimated in the previous task. Use the exponential function exp() on both sides of the equation. What kind of equation do you get now for liggetid? How much does the length of hospital stay increase for those who have a stroke? Comment."
  },
  {
    "objectID": "lab/lab_linearreg-III.html",
    "href": "lab/lab_linearreg-III.html",
    "title": "Solutions - Linear regression III",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_linearreg-III.html#exercise-1-crime-rate-data.",
    "href": "lab/lab_linearreg-III.html#exercise-1-crime-rate-data.",
    "title": "Solutions - Linear regression III",
    "section": "Exercise 1 (crime rate data).",
    "text": "Exercise 1 (crime rate data).\nThis data set contains crime rate data for the individual US states, including District of Columbia (Washington D.C.), coded in the variable state. The variables are violent crime rate (crime), murder rate (murder), percentage metropolitan (pctmetro), percentage high school graduates (pcths), percentage poverty (poverty), percentage single parents (single).\n\n1a)\nMake scatter plots with violent crime rate on the y-axis and percentage metropolitan or percentage single parents on the x-axis. Mark each point by the state initials.\nInsert regression lines in each of the diagrams. Comment on the diagram. You see that Washington D.C. (dc) has a particularly high crime rate.\nNote: One way how you can add labels (e.g. the state initials) to the data points in a scatter plot is to use the command text(), like so: text(y ~ x, labels = lab), where x and y are the variables on the x- and y-axis in the scatterplot and lab is the variable that we want to use for the labels.\n\ncrime_data <- read.csv('data/crime_mod.csv')\n# head(crime_data)\n\npar(mfrow = c(1, 2)) # make plots in 1 row 2 col\n\n# scatter plot: crime vs pctmetro\nplot(crime ~ pctmetro, pch=\".\", data = crime_data)\ntext(crime ~ pctmetro, labels = state, data = crime_data)\n\nabline(lm(crime ~ pctmetro, data = crime_data), col = 'blue')\n\n# scatter plot: crime vs single\nplot(crime ~ single, pch='.', data = crime_data)\ntext(crime ~ single, labels = state, data = crime_data)\n\nabline(lm(crime ~ single, data = crime_data), col = 'blue')\n\n\n\n\nFrom the plots we see that Washington D.C. is an outlier. One must be careful with regression analysis.\n\n\n1b)\nDo a regression analysis with crime rate as the dependent variable and percentage metropolitan as the independent variable. Perform also residual analysis.\n\n# regression: crime vs pctmetro\nsummary(lm(crime ~ pctmetro, data = crime_data))\n\n\nCall:\nlm(formula = crime ~ pctmetro, data = crime_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-497.30 -226.15  -24.61  133.02 1952.76 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -123.683    170.511  -0.725    0.472    \npctmetro      10.929      2.408   4.539 3.69e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 373.9 on 49 degrees of freedom\nMultiple R-squared:  0.296, Adjusted R-squared:  0.2816 \nF-statistic:  20.6 on 1 and 49 DF,  p-value: 3.685e-05\n\n# residual analysis:\npar(mfrow=c(2,2))\nplot(lm(crime ~ pctmetro, data = crime_data))\n\n\n\n\nThe explained variation is not so large, even though the predictor is highly significant. When checking residuals, we realize that they deviate quite consistently from the normal distribution (there is clearly an outlier). The analysis cannot be trusted.\n\n\n1c)\nNext, do a regression analysis with percentage single parents as the independent variable. Perform the residual analysis.\n\n# regression: crime vs single\nsummary(lm(crime ~ single, data = crime_data))\n\n\nCall:\nlm(formula = crime ~ single, data = crime_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-767.42 -116.82  -20.58  125.28  719.70 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1362.53     186.23  -7.316 2.15e-09 ***\nsingle        174.42      16.17  10.788 1.53e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 242.5 on 49 degrees of freedom\nMultiple R-squared:  0.7037,    Adjusted R-squared:  0.6977 \nF-statistic: 116.4 on 1 and 49 DF,  p-value: 1.529e-14\n\n# residual analysis:\npar(mfrow=c(2,2))\nplot(lm(crime ~ single, data = crime_data))\n\n\n\n\nNow the fit looks much better: the explained variation is a lot larger, and the predictor is highly significant. Also the residuals look okay. What has changed from the analysis at the previous task?\n\n\n1d)\nFinally, have a look at the fourth plot of the regression diagnostics plots from tasks b) and c), which shows Residuals vs Leverage. Interpret the results in terms of an analysis of potential influence points.\nFrom both plots it is evident that Washington D.C. (row number 51) has way too strong leverage on the models, and hence should be removed.\nAfter removing this outlier (e.g. via index and which()), we refit the simple linear regression for crime vs percentage metropolitan, and get the following output:\n\n# regression: crime vs pctmetro\nsummary(lm(crime ~ pctmetro, \n           data = crime_data[which(crime_data$state != 'dc'),]))\n\n\nCall:\nlm(formula = crime ~ pctmetro, data = crime_data[which(crime_data$state != \n    \"dc\"), ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-382.45 -182.81    1.09  163.98  450.40 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   25.558    111.117   0.230    0.819    \npctmetro       8.108      1.585   5.115 5.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 240.5 on 48 degrees of freedom\nMultiple R-squared:  0.3528,    Adjusted R-squared:  0.3393 \nF-statistic: 26.16 on 1 and 48 DF,  p-value: 5.44e-06\n\nids <- crime_data$state != 'dc' & crime_data$state != 'ms'\nwhich.ids <- which(ids)\ncrime_subset <- crime_data[which.ids, ]\nfit <- lm(crime ~ pctmetro, data = crime_subset)\nsummary(fit)\n\n\nCall:\nlm(formula = crime ~ pctmetro, data = crime_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-386.26 -181.21   -5.05  167.71  456.76 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.058    116.042   0.035    0.972    \npctmetro       8.378      1.642   5.103 5.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 241.8 on 47 degrees of freedom\nMultiple R-squared:  0.3565,    Adjusted R-squared:  0.3428 \nF-statistic: 26.04 on 1 and 47 DF,  p-value: 5.933e-06\n\nplot(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n# residual analysis:\npar(mfrow=c(2,2))\nplot(lm(crime ~ pctmetro, \n        data = crime_data[which(crime_data$state != 'dc'),]))\n\n\n\n\nNow the predictor is still significant, but the explained variation increased from 29.6% to 35.3%. Also normal probability plot of the residuals and leverage plot look much better now.\nLet us check the model for crime vs percentage of single parents:\n\n# regression: crime vs single\nsummary(lm(crime ~ single, \n           data = crime_data[which(crime_data$state != 'dc'),]))\n\n\nCall:\nlm(formula = crime ~ single, data = crime_data[which(crime_data$state != \n    \"dc\"), ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-599.75 -127.40  -12.87  111.62  705.70 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -878.86     246.90  -3.560 0.000849 ***\nsingle        130.11      22.03   5.905  3.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 227.5 on 48 degrees of freedom\nMultiple R-squared:  0.4208,    Adjusted R-squared:  0.4087 \nF-statistic: 34.87 on 1 and 48 DF,  p-value: 3.499e-07\n\n# residual analysis:\npar(mfrow=c(2,2))\nplot(lm(crime ~ single, \n        data = crime_data[which(crime_data$state != 'dc'),]))\n\n\n\n\nIn this case, instead, even if the predictor is still highly significant, the explained variation is much smaller! (and the predictor coefficient changed from 174 to 130, huge effect of a single data point). This is because, differently from the previous analysis, the outlier was improving the regression fit by driving the entire analysis.\nNormal probability plot of the residuals and leverage analysis help us verify that we now solved the issue in this analysis as well. In order to be extremely careful, we should do a scatter plot of crime vs single for the data without Washington D.C., and check whether MS (row number 25) is another leverage point/ outlier. Try!"
  },
  {
    "objectID": "lab/lab_linearreg-III.html#exercise-2-birth.",
    "href": "lab/lab_linearreg-III.html#exercise-2-birth.",
    "title": "Solutions - Linear regression III",
    "section": "Exercise 2 (birth).",
    "text": "Exercise 2 (birth).\nIn a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, bwt, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (id)\n\nLow birth weight (low), i.e. bwt below or above 2500g\n\nAge of the mother in years (age)\n\nWeight (in pounds) at last menstrual period (lwt)\n\nEthnicity: white, black, or other (eth)\n\nSmoking status, smoker means current smoker, nonsmoker means not smoking during pregnancy (smk)\n\nHistory of premature labour, values 0, 1, 2… (ptl)\n\nHistory of hypertension: yes vs no (ht)\n\nUterine irritability, yes vs no (ui)\n\nFirst trimester visits, values 0, 1, 2… (ftv)\n\nThird trimester visits, values 0, 1, 2… (ttv)\n\nBirth weight in grams (bwt)\n\n\n2a)\nMake scatter plots of birth weight (bwt) versus age of the mother (age), and versus weight of mother (lwt). Edit the scatter plots to insert a regression line.\n\nbirth <- read.csv('data/birth.csv')\n# head(birth)\n\npar(mfrow = c(1, 2)) # make plots in 1 row 2 col\n\n# scatter plots:\nplot(bwt ~ age, data = birth)\nabline(lm(bwt ~ age, data = birth), col = 'blue')\n\nplot(bwt ~ lwt, data = birth)\nabline(lm(bwt ~ lwt, data = birth), col = 'blue')\n\n\n\n\nMake also separate regression lines for smokers and non-smokers. Interpret the results.\n\npar(mfrow = c(2, 2)) # make plots in 2 row 2 col\n\n# scatter plots separate for smokers and non-smokers:\nplot(bwt ~ age, data = birth[which(birth$smk == 'smoker'),], \n     main=\"Smokers\")\nabline(lm(bwt ~ age, data = birth[which(birth$smk == 'smoker'),]), \n          col = 'blue')\n\nplot(bwt ~ age, data = birth[which(birth$smk == 'nonsmoker'),], \n     main=\"Non-smokers\")\nabline(lm(bwt ~ age, data = birth[which(birth$smk == 'nonsmoker'),]), \n       col = 'blue')\n\nplot(bwt ~ lwt, data = birth[which(birth$smk == 'smoker'),], \n     main=\"Smokers\")\nabline(lm(bwt ~ lwt, data = birth[which(birth$smk == 'smoker'),]), \n          col = 'blue')\n\nplot(bwt ~ lwt, data = birth[which(birth$smk == 'nonsmoker'),], \n     main=\"Non-smokers\")\nabline(lm(bwt ~ lwt, data = birth[which(birth$smk == 'nonsmoker'),]), \n       col = 'blue')\n\n\n\n\nWe can comment the four plots above as follows:\n\nSmoking mothers tend to have newborns with lower birthweight.\n\nBirthweight increases with the mother’s age only for non-smokers, while in the smoking mothers group the relationship is reverted.\n\nBirthweight increases with the weight of the mother, but less in the smoking group.\n\n\n\n2b)\nCompute the correlation between birth weight and weight of mother.\n\ncor.test(~ bwt + lwt, data = birth)\n\n\n    Pearson's product-moment correlation\n\ndata:  bwt and lwt\nt = 2.5856, df = 187, p-value = 0.01048\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.04423134 0.32003247\nsample estimates:\n      cor \n0.1857887 \n\n\nThe correlation is not so large (18.6%), but significantly different from 0 at the 5% level (p = 0.01).\n\n\n2c)\nMake boxplots of birth weight for smokers and non-smokers separately.\n\nboxplot(bwt ~ smk, data = birth)\n\n\n\n\nThe plot indicates some negative effect of smoking on birthweight.\n\n\n2d)\nLet us perform regression analyses with birth weight as the dependent variable. First, use only smoking as an independent variable.\n\n# regression\nsummary(lm(bwt ~ smk, data = birth))\n\n\nCall:\nlm(formula = bwt ~ smk, data = birth)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2064.24  -477.24    35.04   545.04  1935.04 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3054.96      66.93  45.642  < 2e-16 ***\nsmksmoker    -281.71     106.97  -2.634  0.00916 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 717.8 on 187 degrees of freedom\nMultiple R-squared:  0.03576,   Adjusted R-squared:  0.03061 \nF-statistic: 6.936 on 1 and 187 DF,  p-value: 0.009156\n\n\nNote the low explained variation, 3.6%, in spite of smoking being important. This is very often the case when the independent variable is dichotomous (i.e. has only two values). Let us then fit a multiple linear regression.\nIn the second analysis, use also weight of mother as an independent variable. Interpret the results in relation to the earlier results of this exercise.\n\n# scatter plots:\nsummary(lm(bwt ~ smk + lwt, data = birth))\n\n\nCall:\nlm(formula = bwt ~ smk + lwt, data = birth)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2030.16  -447.00    27.74   514.20  1968.51 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2500.174    230.833  10.831   <2e-16 ***\nsmksmoker   -270.013    105.590  -2.557   0.0114 *  \nlwt            4.238      1.690   2.508   0.0130 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 707.8 on 186 degrees of freedom\nMultiple R-squared:  0.06731,   Adjusted R-squared:  0.05728 \nF-statistic: 6.711 on 2 and 186 DF,  p-value: 0.001533\n\n\nSome comments:\n\nThe explained variation is slightly larger than in the previous model (6.7%).\n\nSmoking is still highly significant.\n\nMother’s weight is significant, but the coefficient is not large.\n\nResiduals are fine.\n\nAnd finally, in a third analysis, add smoking, weight of the mother and ethnicity.\n\n# scatter plots:\nsummary(lm(bwt ~ smk + lwt + eth, data = birth))\n\n\nCall:\nlm(formula = bwt ~ smk + lwt + eth, data = birth)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2290.41  -459.78    24.54   474.69  1706.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2295.338    290.787   7.894 2.56e-13 ***\nsmksmoker   -398.290    108.481  -3.672 0.000316 ***\nlwt            3.934      1.704   2.308 0.022107 *  \nethother     107.873    165.869   0.650 0.516277    \nethwhite     504.648    153.078   3.297 0.001174 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 680.1 on 184 degrees of freedom\nMultiple R-squared:  0.1482,    Adjusted R-squared:  0.1297 \nF-statistic: 8.002 on 4 and 184 DF,  p-value: 5.718e-06\n\n# residual analysis:\npar(mfrow=c(2,2))\nplot(lm(bwt ~ smk + lwt + eth, data = birth))\n\n\n\n\nThis model looks a lot better than the ones before! The explained variation is larger (14.8%), and all predictors are significant.\nRemember to check the residuals before concluding. The residuals look in line with the regression assumption of normality, so we can conclude that this final model is the best."
  },
  {
    "objectID": "lab/ex_linearreg-I.html",
    "href": "lab/ex_linearreg-I.html",
    "title": "Exercises - Linear regression I",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg-I.html#exercise-1-lung-function",
    "href": "lab/ex_linearreg-I.html#exercise-1-lung-function",
    "title": "Exercises - Linear regression I",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sitting position, and three times in a standing position.\nThe variables are\n\nAge (years)\n\nGender (female, male)\n\nHeight (cm)\n\nWeight (kg)\n\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\n\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\n\nMean of the three measurements made in a sitting position (pefsitm)\n\nMean of the three measurements made in a standing position (pefstam)\n\nMean of all six PEF values (pefmean)\n\n\n1a)\nMake a scatter plot of pefsit2 versus pefsit1; and make a separate scatter plot of pefsit1 versus weight. Insert a regression line on top of the scatter plots.\nNote: You can add a regression line on top of an existing scatter plot by using the abline(reg = lm(y ~ x)), where x is the variable in the x-axis and y is the variable on the y-axis of the scatterplot.\n\n\n1b)\nCompute the correlation between pefsit1 and pefsit2; and between pefsit1 and weight.\nWhy is the correlation between the first pair closer to 1 than the second pair?\nNote: You can get the pair-wise correlation between many other pairs of variables using cor(your_data). Be mindful of missing values in your data; if there are any missing values, you can still calculate the correlation by using `cor(your_data, use=‘pairwise.complete.obs’).\n\n\n1c)\nCarry out two regression analysis:\n\npefsit2 as dependent variable (y), pefsit1 as independent variable (x);\npefsitm as dependent variable (y), weight as independent variable (x).\n\nInterpret the results in relation to the scatter plots.\n\n\n1d)\nPerform a residual analysis for the analyses you did before. Interpret the results."
  },
  {
    "objectID": "lab/ex_linearreg-I.html#exercise-2-blood-pressure",
    "href": "lab/ex_linearreg-I.html#exercise-2-blood-pressure",
    "title": "Exercises - Linear regression I",
    "section": "Exercise 2 (blood pressure)",
    "text": "Exercise 2 (blood pressure)\nThe dataset bp contains data on 20 healthy adults on two variables, Age and Blood pressure. We will explore the relationship between these two variables.\n\n2a)\nLoad the dataset. Find the correlation between age and blood pressure, and test if it is significant.\nFit a simple linear regression model, where blood pressure is the dependent variable and age is the independent variable. What is the 95% confidence interval for the regression slope parameter?\nAlso find the squared correlation coefficient between age and blood pressure. What does it mean?\nBonus task (homework): Plot the regression line, and check the residuals.\n\n\n2b)\nWhat is the blood pressure for a person at age 40? For a person at age 75? Comment.\nBonus task (homework): Plot the regression line, and check the residuals."
  },
  {
    "objectID": "lab/ex_linearreg-III.html",
    "href": "lab/ex_linearreg-III.html",
    "title": "Exercises - Linear regression III",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg-III.html#exercise-1-crime-rate-data.",
    "href": "lab/ex_linearreg-III.html#exercise-1-crime-rate-data.",
    "title": "Exercises - Linear regression III",
    "section": "Exercise 1 (crime rate data).",
    "text": "Exercise 1 (crime rate data).\nThis data set contains crime rate data for the individual US states, including District of Columbia (Washington D.C.), coded in the variable state. The variables are violent crime rate (crime), murder rate (murder), percentage metropolitan (pctmetro), percentage high school graduates (pcths), percentage poverty (poverty), percentage single parents (single).\n\n1a)\nMake scatter plots with violent crime rate on the y-axis and percentage metropolitan or percentage single parents on the x-axis. Mark each point by the state name.\nInsert regression lines in each of the diagrams. Comment on the diagram. You see that Washington D.C. (dc) has a particularly high crime rate.\nNote: One way how you can add labels (e.g. the state name) to the data points in a scatter plot is to use the command text(), like so: text(y ~ x, labels = lab), where x and y are the variables on the x- and y-axis in the scatterplot and lab is the variable that we want to use for the labels.\n\n\n1b)\nDo a regression analysis with crime rate as the dependent variable and percentage metropolitan as the independent variable. Perform also residual analysis.\n\n\n1c)\nNext, do a regression analysis with percentage single parents as the independent variable. Perform the residual analysis.\n\n\n1d)\nFinally, have a look at the fourth plot of the regression diagnostics plots from tasks b) and c), which shows Residuals vs Leverage. Interpret the results in terms of an analysis of potential influence points."
  },
  {
    "objectID": "lab/ex_linearreg-III.html#exercise-2-birth.",
    "href": "lab/ex_linearreg-III.html#exercise-2-birth.",
    "title": "Exercises - Linear regression III",
    "section": "Exercise 2 (birth).",
    "text": "Exercise 2 (birth).\nIn a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, bwt, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (id)\n\nLow birth weight (low), i.e. bwt below or above 2500g\n\nAge of the mother in years (age)\n\nWeight (in pounds) at last menstrual period (lwt)\n\nEthnicity: white, black, or other (eth)\n\nSmoking status, smoker means current smoker, nonsmoker means not smoking during pregnancy (smk)\n\nHistory of premature labour, values 0, 1, 2… (ptl)\n\nHistory of hypertension: yes vs no (ht)\n\nUterine irritability, yes vs no (ui)\n\nFirst trimester visits, values 0, 1, 2… (ftv)\n\nThird trimester visits, values 0, 1, 2… (ttv)\n\nBirth weight in grams (bwt)\n\n\n2a)\nMake scatter plots of birth weight (bwt) versus age of the mother (age), and versus weight of mother (lwt). Edit the scatter plots to insert a regression line.\nMake also separate regression lines for smokers and non-smokers. Interpret the results.\n\n\n2b)\nCompute the correlation between birth weight and weight of mother.\n\n\n2c)\nMake boxplots of birth weight for smokers and non-smokers separately.\n\n\n2d)\nLet us perform regression analyses with birth weight as the dependent variable. First, use only smoking as an independent variable.\nIn the second analysis, use also weight of mother as an independent variable. Interpret the results in relation to the earlier results of this exercise.\nAnd finally, in a third analysis, add smoking, weight of the mother and ethnicity."
  },
  {
    "objectID": "lab/demo_interactive.html",
    "href": "lab/demo_interactive.html",
    "title": "Demo: run code interactively",
    "section": "",
    "text": "Create variables\nCreate a numeric variable\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "get_started/get_started.html",
    "href": "get_started/get_started.html",
    "title": "Get started",
    "section": "",
    "text": "Here you will find useful information on how to get started with the software.\nIn this course we will be using R. You can either\n\n(recommended) have R and Rstudio installed on your laptop\nor, use Posit cloud (formerly Rstudio Cloud).\n\nPlease let us know if you have issue accessing data files on GitHub or content on the webpage!\n\nOption 1 (recommend): Set up your RStudio on your laptop\nYou will need both R and Rstudio, they are two separate things.\n\n\n\n\n\n\nTip\n\n\n\nWatch these YouTube videos if you have trouble when trying at home.\n\n\nYou can download Rstudio here. In this page it will first ask you to install R.\n\nYou can download R for Linux, macOS or Windows here or here\n\nOnce you have finished installing both R and Rstudio, open Rstudio, and you should be seeing something like this:\n\n\n\nOption 2: Use Posit Cloud\n\n\n\n\n\n\nNote\n\n\n\nIt is recommended to have R and Rstudio installed on your laptop, this is because you have a better control of where you prefer to download data and course material. This is also useful when you want to analyse your own datasets. For example, you might have to upload datafiles to the server for Posit Cloud to work.\nHowever, if there is a problem with the installation, you can use Posit Cloud as an alternative.\nOn Tuesday morning we will see if most people can successfully make R run on their laptop and make necessary adjustments.\n\n\nPosit Cloud is free of charge for personal users. You need to sign up for a new user account and have internet connection.\n\nOnce you signed up, you can open a new workspace, and it will look like something like this. If it is the first time you use Posit Cloud, it will be empty (i.e. no script or data).\n\n\n\nData\nThe datasets used in this course are stored in this folder.\nIf you have trouble accessing or downloading datasets from GitHub, please let us know - we will upload a copy on Canvas.\n\nDownload data from GitHub\n\nGo to the repository, select the data you want to download by left-click the file name\nIf it is a dataset in .csv, .txt format, you will see something like this\n\n\n\nfind Raw button, right-click, you will see a list of options\nchoose Download Linked File, this will download the data into your default download folder\nalternatively, choose Download linked file as… so that you can change where you put it and file name.\n\n\nIf it is a dataset in .dta, .xlsx or other format, you might not see the data directly:\n\n\n\nfind Download button, left-click, and you will download it into your default download folder.\n\n\nOptional: download data via URL\nIf you feel like it, you can also download data inside R via URL. You can read more about it here.\n\n\n\n\nCode\nR scripts are stored in this folder.\nTo download an R script, it is similar to downloading a dataset\n\nAlternatively, you can create your own R script locally inside Rstudio, by copy and paste the script from Github.\n\nBooks and papers\nYou can find these books in the library.\n\nEssential medical statistics (Kirkwood and Sterne, 2nd edition, 2003) (link\nStatistiske metoder i medisin og helsefag (Aalen, Frigessi, Moger, Scheel, Skovlund og Veierød, 2 utg, 2018) (link)\n\n\n\nVideos\nInstall R and Rstudio YouTube videos"
  },
  {
    "objectID": "for_instructors/plan.html",
    "href": "for_instructors/plan.html",
    "title": "Week 1 (4.24-4.28)",
    "section": "",
    "text": "Self study sessions\nLab sessions"
  },
  {
    "objectID": "for_instructors/plan.html#monday-m",
    "href": "for_instructors/plan.html#monday-m",
    "title": "Week 1 (4.24-4.28)",
    "section": "Monday (M)",
    "text": "Monday (M)\n\nAfternoon: lecture: Intro, descriptive statistics (M)"
  },
  {
    "objectID": "for_instructors/plan.html#tuesday-ca",
    "href": "for_instructors/plan.html#tuesday-ca",
    "title": "Week 1 (4.24-4.28)",
    "section": "Tuesday (C,A)",
    "text": "Tuesday (C,A)\n\nMorning: R lab (C)\n9:00-10:00 Introduction to R\n10:15-11:00 Descriptive statistics\n11:00-12:00 Lab practice, plus discussion\n\n\nAfternoon: lecture: Probability, Diagnostic testing (A)\n13:00-13:15 Intro to probabilites and diagnostic testing\n13:15-14:30 Self study\n14:30-15:45 Paper and discussions in groups (COVID testing)\n15:45-16:00 Wrap up"
  },
  {
    "objectID": "for_instructors/plan.html#wednesday-av",
    "href": "for_instructors/plan.html#wednesday-av",
    "title": "Week 1 (4.24-4.28)",
    "section": "Wednesday (A,V)",
    "text": "Wednesday (A,V)\n\nMorning: R lab, lecture: distributions, simulation (A)\n9:00 - 9:15 Intro to probability distributions\n9:15 - 10:15 Self study\n10:15 - 11:45 R lab (Simulations)\n11:45 - 12:00 Wrap up\n\n\nAfternoon: lecture: confidence interval, t-test (V)\n13:00-13:30 lecture on confidence interval (intro)\n13:30-14:30 self study CI\n14:30-15:15 intro on testing - t test (one sample), simplest situation - t test (two sample)\n15:15-16:00 self study/home\n\n\nThursday (C,V)\n\nMorning: R lab (C)\n9:00-9:45 Lab\n\nrepeat theory\nR example\n\n9:45 - 11:00 Lab\n11:00-11:30 Lab (discussion)\n\n\nAfternoon: lecture: proportions, tables, chi-sq (V)\n12:30-13:30 lecture\n\nproportions (CI)\ntables\nrisks\n\n13:30-14:30 self study\n14:30-15:15 V intro to chi-sq test"
  },
  {
    "objectID": "for_instructors/plan.html#monday-cv",
    "href": "for_instructors/plan.html#monday-cv",
    "title": "Week 1 (4.24-4.28)",
    "section": "Monday (C,V)",
    "text": "Monday (C,V)\n\nMorning: lecture + lab: transformations, EDA, non-parametric tests (C)\n\n\nAfternoon: lecture + lab: sample size (V)"
  },
  {
    "objectID": "for_instructors/plan.html#tuesday-jo-m",
    "href": "for_instructors/plan.html#tuesday-jo-m",
    "title": "Week 1 (4.24-4.28)",
    "section": "Tuesday (Jo? M)",
    "text": "Tuesday (Jo? M)\nMorning: Study design (Jo?)\nAfternoon: regression (M)"
  },
  {
    "objectID": "for_instructors/plan.html#wednesday-mc",
    "href": "for_instructors/plan.html#wednesday-mc",
    "title": "Week 1 (4.24-4.28)",
    "section": "Wednesday (M,C?)",
    "text": "Wednesday (M,C?)\nMorning: regression\nAfternoon: regression"
  },
  {
    "objectID": "for_instructors/plan.html#thursday-m",
    "href": "for_instructors/plan.html#thursday-m",
    "title": "Week 1 (4.24-4.28)",
    "section": "Thursday (M)",
    "text": "Thursday (M)"
  },
  {
    "objectID": "lab/ex_eda_part1.html",
    "href": "lab/ex_eda_part1.html",
    "title": "Exercise - EDA (part I)",
    "section": "",
    "text": "Generate a variable named weight, with the following measurements:\n50 75 70 74 95 83 65 94 66 65 65 75 84 55 73 68 72 67 53 65\n\n\n\nMake a simple descriptive analysis of the variable, what are the mean, standard deviation, maximum, minimum and quantiles?\nHow to interpret the data?\n\n\n\nMake a histogram.\n\n\n\nMake a boxplot. What do the two dots on the top represent?"
  },
  {
    "objectID": "lab/ex_eda_part1.html#exercise-1-weight",
    "href": "lab/ex_eda_part1.html#exercise-1-weight",
    "title": "Exercise - EDA (part I)",
    "section": "",
    "text": "Generate a variable named weight, with the following measurements:\n50 75 70 74 95 83 65 94 66 65 65 75 84 55 73 68 72 67 53 65\n\n\n\nMake a simple descriptive analysis of the variable, what are the mean, standard deviation, maximum, minimum and quantiles?\nHow to interpret the data?\n\n\n\nMake a histogram.\n\n\n\nMake a boxplot. What do the two dots on the top represent?"
  },
  {
    "objectID": "lab/ex_eda_part1.html#exercise-2-lung-function",
    "href": "lab/ex_eda_part1.html#exercise-2-lung-function",
    "title": "Exercise - EDA (part I)",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nDownload and open PEFH98-english.dta into R.\nIf you have problem with .dta data format, you can also use PEFH98-english.csv.\nPay attention to how gender is coded. We might have to modify it.\n\n\n2b)\nHow many observations are there (number of subjects)? How do you get a list of variable names from your dataset?\nMake a histogram for each of the following variables. Compute means, and interpret the results.\nheight\nweight\nage\npefsitm\npefstam\n\n\n2c)\nMake histograms for the variables height and pefmean for men and women separately. Also try to make boxplots.\nWhat conclusion can you draw?\n\n\n2d)\nMake three scatterplots to compare\n\npefmean with height\npefmean with weight\npefmean with age\n\nWhat association do you see?"
  },
  {
    "objectID": "lab/lab_intro_r.html",
    "href": "lab/lab_intro_r.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a language and environment for statistical computing, data analysis, visualisation and graphics and many more. It is a free and open source software, under the terms of GNU General Public License.\nR runs on a wide variety of platforms, including Windows, Linux and MacOS."
  },
  {
    "objectID": "lab/lab_intro_r.html#create-a-numeric-variable",
    "href": "lab/lab_intro_r.html#create-a-numeric-variable",
    "title": "Introduction to R",
    "section": "Create a numeric variable",
    "text": "Create a numeric variable\nTo create a variable, you type variable_name &lt;- variable_value in the console.\n\n# create a numeric variable number_1\na &lt;- 3\na\n\n[1] 3\n\n\nYou can carry out **mathematical calculation8* on numeric variables, such as exponentiation, addition, division and many more.\n\n# assign values to variables a, b, c\na &lt;- 3\nb &lt;- 4\nc &lt;- 7\n\n# calculate the average of a,b,c\n# output directly\n(a+b+c)/3\n\n[1] 4.666667\n\n# or, save into a new variable d\nd &lt;- (a+b+c)/3\nd\n\n[1] 4.666667\n\n# e to the power of a (e = 2.7182)\nexp(a)\n\n[1] 20.08554"
  },
  {
    "objectID": "lab/lab_intro_r.html#data-types",
    "href": "lab/lab_intro_r.html#data-types",
    "title": "Introduction to R",
    "section": "Data types",
    "text": "Data types\nIn R, there are a few types of variables. The ones you will interact with are:\n\nnumeric (real numbers): 1.2, -5\ninteger: 1, 2, 2000\ncharacter (strings): “male”, “female”\nlogical (binary, 1/0): True or False\n\nNote that code that start with # are comments, and are not evaluated.\n\n# create a numeric variable number_1\nnumber_1 &lt;- 1.2\n\n# a character variable student\nstudent &lt;- 'hadley'\n\n# a logical variable true_or_false\ntrue_or_false &lt;- T\n\nTo evaluate (or return) the variable you have created, you can either type the name of the variable, or print() with the variable name inside the bracket.\n\nnumber_1\n\n[1] 1.2\n\nprint(number_1)\n\n[1] 1.2\n\n\nYou can check the variable type using class(variable_name):\n\nclass(number_1)\n\n[1] \"numeric\"\n\nclass(student)\n\n[1] \"character\"\n\nclass(true_or_false)\n\n[1] \"logical\"\n\n\n\n\n\n\n\n\nName your variable\n\n\n\nIt is good practice to give your variable a name that is both easy to understand, and also valid.\n\nNames are case sensitive, VariableA is not the same as variablea\nNumbers can not be a variable name by itself. Combining numbers and letters is allowed, but should start with a letter, such as variable3, but NOT 22variable\nYou can use underscores (“snake_case” naming style). In fact it encourages readability, so it is my personal favoriate.\n\nAvoid the following:\n\nOther special characters, such as dot and dollar sign: var.A, var$A have special meanings in R.\nAvoid using function names like function, list and so on. If you really can’t think of a better name, you can use names my_function, list_1 to avoid the ambiguity."
  },
  {
    "objectID": "lab/lab_intro_r.html#vectors",
    "href": "lab/lab_intro_r.html#vectors",
    "title": "Introduction to R",
    "section": "Vectors",
    "text": "Vectors\nA vector is a list of values; it can be numeric, and also characters and logical.\nTo create a vector, use function c().\n\n# numeric\nnum_vector &lt;- c(1, 2, 3, 4, 5)\nnum_vector\n\n[1] 1 2 3 4 5\n\n# character\nchar_vector &lt;- c('student_a', 'student_b', 'student_c')\nchar_vector\n\n[1] \"student_a\" \"student_b\" \"student_c\"\n\n# logical \nlogical_vector &lt;- c(T, F, T, F)\nlogical_vector\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nThere are some shortcuts to create a sequence of values; not required to learn, but very useful.\n\n# numeric\n# num_vector &lt;- c(1, 2, 3, 4, 5)\nnum_vector &lt;- 1:5 # from 1 to 5\nseq(from = 1, to = 11, by = 2) # from 1 to 11, with 2 between each\n\n[1]  1  3  5  7  9 11\n\nrep(1, 5) # repeat 1 for 5 times\n\n[1] 1 1 1 1 1\n\n# character\n# char_vector &lt;- c('student_a', 'student_b', 'student_c')\nchar_vector &lt;- paste0('student_', c('a', 'b', 'c'))\nchar_vector\n\n[1] \"student_a\" \"student_b\" \"student_c\"\n\n\n\n\n\n\n\n\nTypes of elements in a vector\n\n\n\nIn a vector, types of the elements must be the same. If you try to combine multiple types of variables in the same vector, such as a numeric number and a character, R will try to convert them into the same type.\nTry to combine the following values into a vector, and see what happens.\n\n1.52, “student_a”\n1.52, TRUE (logical)\nTRUE, “student_a”\n\n\n\n\nCombine multiple vectors\nYou can combine multiple vectors using c(). For example, vec1 has 3 elements, vec2 has 2 elements (assuming that they are of the same type), combining them gives 5 elements.\n\nvec1 &lt;- c(1, 3, 5)\nvec2 &lt;- c(100, 101)\nc(vec1, vec2)\n\n[1]   1   3   5 100 101\n\n# you can also save it into a new variable, \n# so that you can access it in the future\nvec_combined &lt;- c(vec1, vec2)\nvec_combined\n\n[1]   1   3   5 100 101"
  },
  {
    "objectID": "lab/lab_intro_r.html#matrix",
    "href": "lab/lab_intro_r.html#matrix",
    "title": "Introduction to R",
    "section": "Matrix",
    "text": "Matrix\nA matrix can be thought of as a stack of vectors. When you collect data from \\(n\\) patients (or subjects), you measure a few aspects on each patient such as age, sex, height and smoking. Let’s say you have measured \\(p\\) aspects. This forms a matrix of size \\(n \\times p\\).\nYou might not need to create a matrix from scratch in R (because the focus of this course is data analysis); but it is helpful to understand some basic data manipulation commands.\nYou can create a matrix using matrix(), with some parameters:\n\nmatrix_1 &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = T)\nmatrix_1\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nYou can also create a matrix by combining two vectors of the same size, using cbind() or rbind(), which stands for “column bind” and “rowbind”.\n\nvec1 &lt;- c(1, 2)\nvec2 &lt;- c(3, 4)\n\n# bind by columnn\nmatrix_c &lt;- cbind(vec1, vec2)\nmatrix_c\n\n     vec1 vec2\n[1,]    1    3\n[2,]    2    4\n\n# bind by row\nmatrix_r &lt;- rbind(vec1, vec2)\nmatrix_r\n\n     [,1] [,2]\nvec1    1    2\nvec2    3    4"
  },
  {
    "objectID": "lab/lab_intro_r.html#dataframe",
    "href": "lab/lab_intro_r.html#dataframe",
    "title": "Introduction to R",
    "section": "Dataframe",
    "text": "Dataframe\nDataframe, data.frame is a format of data commonly used in data analysis with R and python. It can be considered as a matrix, but allows a mixture of data types, such as numeric and categorical measurements (age and sex).\nIn this course, you will mostly be working with dataframes.\nWe create a small dataframe of 3 subjects:\n\nSubject 1 is a 20 years-old male who has covid\nSubject 2 is a 50 years-old female who has covid\nSubject 3 is a 32 years-old male who does not have covid\n\nThis is how you can present the dataframe, where each column has a different data type.\n\nmini_data &lt;- data.frame(\n  age = c(20, 50, 32), \n  sex = c('male', 'female', 'male'), \n  has_covid = c(T, T, F)\n)\nmini_data\n\n  age    sex has_covid\n1  20   male      TRUE\n2  50 female      TRUE\n3  32   male     FALSE"
  },
  {
    "objectID": "lab/lab_intro_r.html#dimension-of-your-data",
    "href": "lab/lab_intro_r.html#dimension-of-your-data",
    "title": "Introduction to R",
    "section": "Dimension of your data",
    "text": "Dimension of your data\nYou can find the size of a vector with length().\nFor a matrix or dataframe, you can use dim(). It will return nrow ncol, number of rows and number of columns.\n\nvec1 &lt;- c(1, 2)\nlength(vec1)\n\n[1] 2\n\n# matrix\nmat &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, byrow = 2)\ndim(mat)\n\n[1] 2 2\n\n# dataframe\ndim(mini_data)\n\n[1] 3 3\n\n\n\n\n\n\n\n\ndim() or length()\n\n\n\nIf you use dim() on a vector, it returns NULL. Given that a vector is just a matrix with 1 row (or column), this seems insensible.\nNonetheless, dim() works on matrix objects. if you convert the vector into a matrix with nrow =1 or ncol = 1, dim() will work.\nIf you use length() on a matrix, it will return the total number of elements, i.e. ncol times nrow.\n\n\nYou can also use nrow(), ncol() to get the number of rows and columns explicitly.\n\n# ncol, nrow\n# dim(mini_data)\nnrow(mini_data)\n\n[1] 3\n\nncol(mini_data)\n\n[1] 3"
  },
  {
    "objectID": "lab/lab_intro_r.html#accessing-elements-in-your-data",
    "href": "lab/lab_intro_r.html#accessing-elements-in-your-data",
    "title": "Introduction to R",
    "section": "Accessing elements in your data",
    "text": "Accessing elements in your data\nFor a vector, you can access\n\nan element at a given position\nmultiple elements at given positions\n\nelements beyond, or below a certain element\n\nSometimes you might need to combine previous knowledge to get what you want (e.g. to know how many elements in total there are).\n\nletters &lt;- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')\n\n# 3rd letter\nletters[3]\n\n[1] \"c\"\n\n# 3rd, and 5th\nletters[c(3, 5)]\n\n[1] \"c\" \"e\"\n\n# letters beyond 4\nletters[5:8] # or, letters[5:length(letters)]\n\n[1] \"e\" \"f\" \"g\" \"h\"\n\n\nFor a matrix,\n\nmatrix[r, c] to get the element on \\(r\\)-th row, \\(c\\)-th column.\nmatrix[r, ], matrix[, c] to get all the elements on \\(r\\)-th row or \\(c\\)-th column\n\n\nmat_3by3 &lt;- matrix(data = 1:9, nrow = 3, byrow = T)\nmat_3by3\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n# element (2,3)\nmat_3by3[2, 3]\n\n[1] 6\n\n# first row\nmat_3by3[1,]\n\n[1] 1 2 3\n\n\nFor a dataframe,\n\nyou can use indices (row, col) in the same way as matrices above;\nuse data$column_name, or data['column_name'] to access the entire column\n\nConventionally, each row is a subject, and each columnn is a variable (or aspect of measurement, feature, characteristic, risk factor etc).\n\nmini_data\n\n  age    sex has_covid\n1  20   male      TRUE\n2  50 female      TRUE\n3  32   male     FALSE\n\n# first row \nmini_data[1, ]\n\n  age  sex has_covid\n1  20 male      TRUE\n\n# second col\nmini_data[, 2]\n\n[1] \"male\"   \"female\" \"male\"  \n\n# via column name using $\nmini_data$age\n\n[1] 20 50 32\n\n# alternatively, \nmini_data['age']\n\n  age\n1  20\n2  50\n3  32\n\n\n\n\n\n\n\n\nFilter data based on criteria\n\n\n\nYou might have a task where you need to filter elements based on another variable: for example, select the age based on sex. This task is done in 2 steps:\n\ncreate a logical (binary, true or false) variable on sex, call it sex_indicator\nselect the elements in age vector, corresponding to sex_ind == TRUE. (The operator == evaluates whether the criteria is met)\n\nThe following example illustrates this process. You will use this a few times in the course, for example to select the height measured for men and women.\n\n\n\nage &lt;- c(55, 60, 65)\nsex &lt;- c('Male', 'Female', 'Male')\n\n# select age for only female\n# first create a variable indicating 'sex == Female'\n# i.e. if the element is Female, returns T; otherwise, F\n\nsex_indicator &lt;- sex == 'Female'\nsex_indicator\n\n[1] FALSE  TRUE FALSE\n\n# next combine age with sex_indicator, this only selects the 2nd element\nage[sex_indicator] \n\n[1] 60\n\n# you can skip the middle step:\nage[sex == 'Female']\n\n[1] 60"
  },
  {
    "objectID": "lab/lab_intro_r.html#modify-existing-data-optional",
    "href": "lab/lab_intro_r.html#modify-existing-data-optional",
    "title": "Introduction to R",
    "section": "Modify existing data (optional)",
    "text": "Modify existing data (optional)\n\n\n\n\n\n\nKeep your original data safe!\n\n\n\nModifying an existing data is easy, but you should be aware of the risks. In this class we only modify data we created in the class so there is little risk, but you might have your own datasets to analyse in the future.\nYou should keep your original data in a safe place, and work on copies of it.\nVersion control is a good skill to learn.\n\n\n\n# vector\n# make e into E \nletters[5] &lt;- 'E'\nletters\n\n[1] \"a\" \"b\" \"c\" \"d\" \"E\" \"f\" \"g\" \"h\"\n\n# matrix\n# make (1, 1) 20\nmat_3by3[1, 1] # originally was 1\n\n[1] 1\n\nmat_3by3[1, 1] &lt;- 20\nmat_3by3\n\n     [,1] [,2] [,3]\n[1,]   20    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n# dataframe\n# make so that subject 2 does not have covid\nmini_data\n\n  age    sex has_covid\n1  20   male      TRUE\n2  50 female      TRUE\n3  32   male     FALSE\n\nmini_data$has_covid[2] &lt;- F\nmini_data\n\n  age    sex has_covid\n1  20   male      TRUE\n2  50 female     FALSE\n3  32   male     FALSE"
  },
  {
    "objectID": "lab/lab_intro_r.html#working-directory-r-project",
    "href": "lab/lab_intro_r.html#working-directory-r-project",
    "title": "Introduction to R",
    "section": "Working directory, R project",
    "text": "Working directory, R project\nYou can think of the working directory as the folder where R looks for (and saves) your scripts by default.\nYou can check where your working directory by running the following command.\n\ngetwd()\n\n[1] \"/Users/chizhang/GitHub/teaching_mf9130e/lab\"\n\n\nYou can manually set this to a folder of your choosing by setwd(path).\nIt is recommanded to use R project. It sets a folder just for the current tasks you work on, so that you do not need to set the working directory every time you open RStudio. Read more about how to create an R project."
  },
  {
    "objectID": "lab/lab_intro_r.html#import-data-1",
    "href": "lab/lab_intro_r.html#import-data-1",
    "title": "Introduction to R",
    "section": "Import data",
    "text": "Import data\nData exist in different formats,\n\ncsv is one of the most commonly used data format for tabular data. If possible, it is a good idea to use this data format as it is readable by different languages and softwares\nxlsx is also good for storing tabular data; however it is slightly more complicated than csv.\nrda can be used to store R data (such as lists, higher dimensional arrays);\nSome formats are data created by foreign softwares (such as dta created by STATA), and they would require some specific R packages to load in.\n\nIt is difficult to summarise all the data formats here, so you should check the documentation on how to import and write (save) data of different types.\n\n# read a csv file\nbirth &lt;- read.csv('data/birth.csv', sep = ',')\n\nbirth\n\n     id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1     4 bwt &lt;= 2500  28 120 other    smoker   1  no yes   0   0  709\n2    10 bwt &lt;= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3    11 bwt &lt;= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4    13 bwt &lt;= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5    15 bwt &lt;= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6    16 bwt &lt;= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n7    17 bwt &lt;= 2500  23  97 other nonsmoker   0  no yes   1   5 1588\n8    18 bwt &lt;= 2500  24 128 black nonsmoker   1  no  no   1   2 1701\n9    19 bwt &lt;= 2500  24 132 other nonsmoker   0 yes  no   0   5 1729\n10   20 bwt &lt;= 2500  21 165 white    smoker   0 yes  no   1   4 1790\n11   22 bwt &lt;= 2500  32 105 white    smoker   0  no  no   0   0 1818\n12   23 bwt &lt;= 2500  19  91 white    smoker   2  no yes   0  12 1885\n13   24 bwt &lt;= 2500  25 115 other nonsmoker   0  no  no   0   3 1893\n14   25 bwt &lt;= 2500  16 130 other nonsmoker   0  no  no   1   4 1899\n15   26 bwt &lt;= 2500  25  92 white    smoker   0  no  no   0   4 1928\n16   27 bwt &lt;= 2500  20 150 white    smoker   0  no  no   2   5 1928\n17   28 bwt &lt;= 2500  21 200 black nonsmoker   0  no yes   2   4 1928\n18   29 bwt &lt;= 2500  24 155 white    smoker   1  no  no   0   6 1936\n19   30 bwt &lt;= 2500  21 103 other nonsmoker   0  no  no   0   5 1970\n20   31 bwt &lt;= 2500  20 125 other nonsmoker   0  no yes   0   2 2055\n21   32 bwt &lt;= 2500  25  89 other nonsmoker   2  no  no   1   4 2055\n22   33 bwt &lt;= 2500  19 102 white nonsmoker   0  no  no   2   3 2082\n23   34 bwt &lt;= 2500  19 112 white    smoker   0  no yes   0   4 2084\n24   35 bwt &lt;= 2500  26 117 white    smoker   1  no  no   0   7 2084\n25   36 bwt &lt;= 2500  24 138 white nonsmoker   0  no  no   0   1 2100\n26   37 bwt &lt;= 2500  17 130 other    smoker   1  no yes   0   9 2125\n27   40 bwt &lt;= 2500  20 120 black    smoker   0  no  no   3   6 2126\n28   42 bwt &lt;= 2500  22 130 white    smoker   1  no yes   1   4 2187\n29   43 bwt &lt;= 2500  27 130 black nonsmoker   0  no yes   0   6 2187\n30   44 bwt &lt;= 2500  20  80 other    smoker   0  no yes   0   6 2211\n31   45 bwt &lt;= 2500  17 110 white    smoker   0  no  no   0   5 2225\n32   46 bwt &lt;= 2500  25 105 other nonsmoker   1  no  no   1   5 2240\n33   47 bwt &lt;= 2500  20 109 other nonsmoker   0  no  no   0   5 2240\n34   49 bwt &lt;= 2500  18 148 other nonsmoker   0  no  no   0   3 2282\n35   50 bwt &lt;= 2500  18 110 black    smoker   1  no  no   0   4 2296\n36   51 bwt &lt;= 2500  20 121 white    smoker   1  no yes   0   4 2296\n37   52 bwt &lt;= 2500  21 100 other nonsmoker   1  no  no   4   0 2301\n38   54 bwt &lt;= 2500  26  96 other nonsmoker   0  no  no   0   6 2325\n39   56 bwt &lt;= 2500  31 102 white    smoker   1  no  no   1   5 2353\n40   57 bwt &lt;= 2500  15 110 white nonsmoker   0  no  no   0   3 2353\n41   59 bwt &lt;= 2500  23 187 black    smoker   0  no  no   1   5 2367\n42   60 bwt &lt;= 2500  20 122 black    smoker   0  no  no   0   4 2381\n43   61 bwt &lt;= 2500  24 105 black    smoker   0  no  no   0   3 2381\n44   62 bwt &lt;= 2500  15 115 other nonsmoker   0  no yes   0   4 2381\n45   63 bwt &lt;= 2500  23 120 other nonsmoker   0  no  no   0   2 2395\n46   65 bwt &lt;= 2500  30 142 white    smoker   1  no  no   0   4 2410\n47   67 bwt &lt;= 2500  22 130 white    smoker   0  no  no   1   2 2410\n48   68 bwt &lt;= 2500  17 120 white    smoker   0  no  no   3   6 2414\n49   69 bwt &lt;= 2500  23 110 white    smoker   1  no  no   0   9 2424\n50   71 bwt &lt;= 2500  17 120 black nonsmoker   0  no  no   2   6 2438\n51   75 bwt &lt;= 2500  26 154 other nonsmoker   1 yes  no   1  10 2442\n52   76 bwt &lt;= 2500  20 105 other nonsmoker   0  no  no   3   6 2450\n53   77 bwt &lt;= 2500  26 190 white    smoker   0  no  no   0   4 2466\n54   78 bwt &lt;= 2500  14 101 other    smoker   1  no  no   0   7 2466\n55   79 bwt &lt;= 2500  28  95 white    smoker   0  no  no   2   7 2466\n56   81 bwt &lt;= 2500  14 100 other nonsmoker   0  no  no   2   6 2495\n57   82 bwt &lt;= 2500  23  94 other    smoker   0  no  no   0   4 2495\n58   83 bwt &lt;= 2500  17 142 black nonsmoker   0 yes  no   0   2 2495\n59   84 bwt &lt;= 2500  21 130 white    smoker   0 yes  no   3   4 2495\n60   85  bwt &gt; 2500  19 182 black nonsmoker   0  no yes   0   4 2523\n61   86  bwt &gt; 2500  33 155 other nonsmoker   0  no  no   3   6 2551\n62   87  bwt &gt; 2500  20 105 white    smoker   0  no  no   1  10 2557\n63   88  bwt &gt; 2500  21 108 white    smoker   0  no yes   2  10 2594\n64   89  bwt &gt; 2500  18 107 white    smoker   0  no yes   0   2 2600\n65   91  bwt &gt; 2500  21 124 other nonsmoker   0  no  no   0   5 2622\n66   92  bwt &gt; 2500  22 118 white nonsmoker   0  no  no   1   1 2637\n67   93  bwt &gt; 2500  17 103 other nonsmoker   0  no  no   1   7 2637\n68   94  bwt &gt; 2500  29 123 white    smoker   0  no  no   1   4 2663\n69   95  bwt &gt; 2500  26 113 white    smoker   0  no  no   0   2 2665\n70   96  bwt &gt; 2500  19  95 other nonsmoker   0  no  no   0   4 2722\n71   97  bwt &gt; 2500  19 150 other nonsmoker   0  no  no   1   9 2733\n72   98  bwt &gt; 2500  22  95 other nonsmoker   0 yes  no   0  10 2750\n73   99  bwt &gt; 2500  30 107 other nonsmoker   1  no yes   2  17 2750\n74  100  bwt &gt; 2500  18 100 white    smoker   0  no  no   0   0 2769\n75  101  bwt &gt; 2500  18 100 white    smoker   0  no  no   0   0 2769\n76  102  bwt &gt; 2500  15  98 black nonsmoker   0  no  no   0   7 2778\n77  103  bwt &gt; 2500  25 118 white    smoker   0  no  no   3   7 2782\n78  104  bwt &gt; 2500  20 120 other nonsmoker   0  no yes   0   4 2807\n79  105  bwt &gt; 2500  28 120 white    smoker   0  no  no   1   6 2821\n80  106  bwt &gt; 2500  32 121 other nonsmoker   0  no  no   2  10 2835\n81  107  bwt &gt; 2500  31 100 white nonsmoker   0  no yes   3   4 2835\n82  108  bwt &gt; 2500  36 202 white nonsmoker   0  no  no   1   7 2836\n83  109  bwt &gt; 2500  28 120 other nonsmoker   0  no  no   0   8 2863\n84  111  bwt &gt; 2500  25 120 other nonsmoker   0  no yes   2  10 2877\n85  112  bwt &gt; 2500  28 167 white nonsmoker   0  no  no   0  12 2877\n86  113  bwt &gt; 2500  17 122 white    smoker   0  no  no   0   9 2906\n87  114  bwt &gt; 2500  29 150 white nonsmoker   0  no  no   2   4 2920\n88  115  bwt &gt; 2500  26 168 black    smoker   0  no  no   0   6 2920\n89  116  bwt &gt; 2500  17 113 black nonsmoker   0  no  no   1  12 2920\n90  117  bwt &gt; 2500  17 113 black nonsmoker   0  no  no   1  12 2920\n91  118  bwt &gt; 2500  24  90 white    smoker   1  no  no   1   1 2948\n92  119  bwt &gt; 2500  35 121 black    smoker   1  no  no   1  11 2948\n93  120  bwt &gt; 2500  25 155 white nonsmoker   0  no  no   1   5 2977\n94  121  bwt &gt; 2500  25 125 black nonsmoker   0  no  no   0   4 2977\n95  123  bwt &gt; 2500  29 140 white    smoker   0  no  no   2   7 2977\n96  124  bwt &gt; 2500  19 138 white    smoker   0  no  no   2   2 2977\n97  125  bwt &gt; 2500  27 124 white    smoker   0  no  no   0   3 2992\n98  126  bwt &gt; 2500  31 215 white    smoker   0  no  no   2  11 3005\n99  127  bwt &gt; 2500  33 109 white    smoker   0  no  no   1   6 3033\n100 128  bwt &gt; 2500  21 185 black    smoker   0  no  no   2   8 3042\n101 129  bwt &gt; 2500  19 189 white nonsmoker   0  no  no   2   4 3062\n102 130  bwt &gt; 2500  23 130 black nonsmoker   0  no  no   1   4 3062\n103 131  bwt &gt; 2500  21 160 white nonsmoker   0  no  no   0  11 3062\n104 132  bwt &gt; 2500  18  90 white    smoker   0  no yes   0   6 3076\n105 133  bwt &gt; 2500  18  90 white    smoker   0  no yes   0   6 3076\n106 134  bwt &gt; 2500  32 132 white nonsmoker   0  no  no   4   7 3080\n107 135  bwt &gt; 2500  19 132 other nonsmoker   0  no  no   0   3 3090\n108 136  bwt &gt; 2500  24 115 white nonsmoker   0  no  no   2   5 3090\n109 137  bwt &gt; 2500  22  85 other    smoker   0  no  no   0   5 3090\n110 138  bwt &gt; 2500  22 120 white nonsmoker   0 yes  no   1   3 3100\n111 139  bwt &gt; 2500  23 128 other nonsmoker   0  no  no   0   8 3104\n112 140  bwt &gt; 2500  22 130 white    smoker   0  no  no   0   4 3132\n113 141  bwt &gt; 2500  30  95 white    smoker   0  no  no   2   4 3147\n114 142  bwt &gt; 2500  19 115 other nonsmoker   0  no  no   0   7 3175\n115 143  bwt &gt; 2500  16 110 other nonsmoker   0  no  no   0   3 3175\n116 144  bwt &gt; 2500  21 110 other    smoker   0  no yes   0   7 3203\n117 145  bwt &gt; 2500  30 153 other nonsmoker   0  no  no   0   6 3203\n118 146  bwt &gt; 2500  20 103 other nonsmoker   0  no  no   0   5 3203\n119 147  bwt &gt; 2500  17 119 other nonsmoker   0  no  no   0   9 3225\n120 148  bwt &gt; 2500  17 119 other nonsmoker   0  no  no   0   9 3225\n121 149  bwt &gt; 2500  23 119 other nonsmoker   0  no  no   2   5 3232\n122 150  bwt &gt; 2500  24 110 other nonsmoker   0  no  no   0   6 3232\n123 151  bwt &gt; 2500  28 140 white nonsmoker   0  no  no   0   4 3234\n124 154  bwt &gt; 2500  26 133 other    smoker   2  no  no   0   3 3260\n125 155  bwt &gt; 2500  20 169 other nonsmoker   1  no yes   1   8 3274\n126 156  bwt &gt; 2500  24 115 other nonsmoker   0  no  no   2  11 3274\n127 159  bwt &gt; 2500  28 250 other    smoker   0  no  no   6  13 3303\n128 160  bwt &gt; 2500  20 141 white nonsmoker   2  no yes   1   7 3317\n129 161  bwt &gt; 2500  22 158 black nonsmoker   1  no  no   2   5 3317\n130 162  bwt &gt; 2500  22 112 white    smoker   2  no  no   0   7 3317\n131 163  bwt &gt; 2500  31 150 other    smoker   0  no  no   2   7 3321\n132 164  bwt &gt; 2500  23 115 other    smoker   0  no  no   1  10 3331\n133 166  bwt &gt; 2500  16 112 black nonsmoker   0  no  no   0  11 3374\n134 167  bwt &gt; 2500  16 135 white    smoker   0  no  no   0   3 3374\n135 168  bwt &gt; 2500  18 229 black nonsmoker   0  no  no   0   6 3402\n136 169  bwt &gt; 2500  25 140 white nonsmoker   0  no  no   1   8 3416\n137 170  bwt &gt; 2500  32 134 white    smoker   1  no  no   4   7 3430\n138 172  bwt &gt; 2500  20 121 black    smoker   0  no  no   0   6 3444\n139 173  bwt &gt; 2500  23 190 white nonsmoker   0  no  no   0   3 3459\n140 174  bwt &gt; 2500  22 131 white nonsmoker   0  no  no   1   7 3460\n141 175  bwt &gt; 2500  32 170 white nonsmoker   0  no  no   0   4 3473\n142 176  bwt &gt; 2500  30 110 other nonsmoker   0  no  no   0   8 3475\n143 177  bwt &gt; 2500  20 127 other nonsmoker   0  no  no   0   3 3487\n144 179  bwt &gt; 2500  23 123 other nonsmoker   0  no  no   0  10 3544\n145 180  bwt &gt; 2500  17 120 other    smoker   0  no  no   0   7 3572\n146 181  bwt &gt; 2500  19 105 other nonsmoker   0  no  no   0   8 3572\n147 182  bwt &gt; 2500  23 130 white nonsmoker   0  no  no   0   4 3586\n148 183  bwt &gt; 2500  36 175 white nonsmoker   0  no  no   0  12 3600\n149 184  bwt &gt; 2500  22 125 white nonsmoker   0  no  no   1  13 3614\n150 185  bwt &gt; 2500  24 133 white nonsmoker   0  no  no   0   7 3614\n151 186  bwt &gt; 2500  21 134 other nonsmoker   0  no  no   2   8 3629\n152 187  bwt &gt; 2500  19 235 white    smoker   0 yes  no   0   5 3629\n153 188  bwt &gt; 2500  25  95 white    smoker   3  no yes   0   8 3637\n154 189  bwt &gt; 2500  16 135 white    smoker   0  no  no   0   2 3643\n155 190  bwt &gt; 2500  29 135 white nonsmoker   0  no  no   1   4 3651\n156 191  bwt &gt; 2500  29 154 white nonsmoker   0  no  no   1   5 3651\n157 192  bwt &gt; 2500  19 147 white    smoker   0  no  no   0   4 3651\n158 193  bwt &gt; 2500  19 147 white    smoker   0  no  no   0   4 3651\n159 195  bwt &gt; 2500  30 137 white nonsmoker   0  no  no   1   5 3699\n160 196  bwt &gt; 2500  24 110 white nonsmoker   0  no  no   1   8 3728\n161 197  bwt &gt; 2500  19 184 white    smoker   0 yes  no   0   7 3756\n162 199  bwt &gt; 2500  24 110 other nonsmoker   1  no  no   0  10 3770\n163 200  bwt &gt; 2500  23 110 white nonsmoker   0  no  no   1   4 3770\n164 201  bwt &gt; 2500  20 120 other nonsmoker   0  no  no   0   2 3770\n165 202  bwt &gt; 2500  25 241 black nonsmoker   0 yes  no   0  10 3790\n166 203  bwt &gt; 2500  30 112 white nonsmoker   0  no  no   1   5 3799\n167 204  bwt &gt; 2500  22 169 white nonsmoker   0  no  no   0   7 3827\n168 205  bwt &gt; 2500  18 120 white    smoker   0  no  no   2   6 3856\n169 206  bwt &gt; 2500  16 170 black nonsmoker   0  no  no   4   8 3860\n170 207  bwt &gt; 2500  32 186 white nonsmoker   0  no  no   2   6 3860\n171 208  bwt &gt; 2500  18 120 other nonsmoker   0  no  no   1  13 3884\n172 209  bwt &gt; 2500  29 130 white    smoker   0  no  no   2   8 3884\n173 210  bwt &gt; 2500  33 117 white nonsmoker   0  no yes   1   2 3912\n174 211  bwt &gt; 2500  20 170 white    smoker   0  no  no   0   4 3940\n175 212  bwt &gt; 2500  28 134 other nonsmoker   0  no  no   1   8 3941\n176 213  bwt &gt; 2500  14 135 white nonsmoker   0  no  no   0   8 3941\n177 214  bwt &gt; 2500  28 130 other nonsmoker   0  no  no   0   8 3969\n178 215  bwt &gt; 2500  25 120 white nonsmoker   0  no  no   2   7 3983\n179 216  bwt &gt; 2500  16  95 other nonsmoker   0  no  no   1  10 3997\n180 217  bwt &gt; 2500  20 158 white nonsmoker   0  no  no   1   6 3997\n181 218  bwt &gt; 2500  26 160 other nonsmoker   0  no  no   0   9 4054\n182 219  bwt &gt; 2500  21 115 white nonsmoker   0  no  no   1   5 4054\n183 220  bwt &gt; 2500  22 129 white nonsmoker   0  no  no   0   4 4111\n184 221  bwt &gt; 2500  25 130 white nonsmoker   0  no  no   2   9 4153\n185 222  bwt &gt; 2500  31 120 white nonsmoker   0  no  no   2   7 4167\n186 223  bwt &gt; 2500  35 170 white nonsmoker   1  no  no   1   6 4174\n187 224  bwt &gt; 2500  19 120 white    smoker   0  no  no   0   3 4238\n188 225  bwt &gt; 2500  24 116 white nonsmoker   0  no  no   1   7 4593\n189 226  bwt &gt; 2500  45 123 white nonsmoker   0  no  no   1   5 4990"
  },
  {
    "objectID": "for_instructors/how-to.html",
    "href": "for_instructors/how-to.html",
    "title": "DO NOT EDIT",
    "section": "",
    "text": "DO NOT EDIT\ndoc folder is generated automatically when render. Do not edit by hand.\n\n\nWhere are my content\nLecture notes\n\ncourse_material/course_material_overview.qmd: overview with a table. this is where you edit the links to each topic\nnotes_diagnostic_tests.qmd: this is an example of qmd notes. renders to html.\nexample.pdf: an example of pdf notes. downloadable by students directly from the browser\n\nLab content\n\nlab/code/ stores code\nlab/data/ stores data\n\n\n\nThe course website\nThe website is linked to the github repository, specifically,\n\nHome links to index.qmd\nGet started links to get_started/get_started.qmd\nCourse material links to course_material/course_material_overview.qmd\nR labs and code links to lab/overview.qmd, which links a bunch of sub material\n\nThe overall appearance is controlled by _quarto.yml.\n\n\nHow to modify the content\nThe website is made by quarto, which is a better version of Rmarkdown. The way code chunk works is exactly the same as Rmarkdown (only that it supports a few more languages in the same file).\nYou might need to install quarto and upgrade Rstudio to the latest version, to be able to render the documents by yourself.\n\nRecommended workflow\n\nClone the repo to your local\nMake a branch\ntest render index.qmd, see if the whole website renders\nchange something on the branch\ntest render the qmd files you modified. Make sure that the path are set properly so that files can be loaded properly\npush to remote\nmake a pull request, someone will take over from here\n\n\n\nDeploy changes to the website (avoid this if you are not sure!)\npush all modifications to github, the site will update itself momentarily (after the CI/CD are successful)\nYou can also let me know if you need any help with the website or text."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "You can find the official course schedule provided by University of Oslo here. If there is an error in the time and place on this page, please refer to the official schedule.\n\n\n\n\n\n\nTime and place\n\n\n\nSessions are roughly divided by AM (morning) and PM (afternoon).\n\nAM: 8:30 - 11:45\nPM: 12:45 - 16:00\n\nLocations\n\nAud13: Auditorium 13 in Domus Medica (map)\nRunde: Runde auditorium R105 (map)\n\n\n\n\nWeek 1\n\n\n\nDay\nSession\nPlace\nTopic\nCourse material\n\n\n\n\nDay 1 - Apr 8\nPM\nAud 13\nCourse introduction\nLecture\n\n\n\n\n\nData and descriptive statistics\nLecture\n\n\nDay 2 - Apr 9\nAM\nAud 13\nLab 1\nLecture, Lab notes, Code\n\n\n\n\n\nLab 2\nLecture, Lab notes, Code\n\n\n\nPM\n\nIntro to probability\nLecture\n\n\n\n\n\nDiagnostic test\nLecture\n\n\n\n\n\nLab 3\nLab notes, Code\n\n\nDay 3 - Apr 10\nAM\nAud 13\nBinomial distribution\nLecture\n\n\n\n\n\nNormal distribution\nLecture\n\n\n\n\n\nLab 4\nLab notes, Code\n\n\n\nPM\n\nStatistical inference\n\n\n\n\n\n\nt-test\n\n\n\nDay 4 - Apr 11\nAM\nAud 13\nLab 5\nLecture, Lab notes, Code\n\n\n\nPM\n\nCategorical data analysis\n\n\n\nDay 5 - Apr 12\nAM\nAud 13\nLab 6\nLecture, Lab notes, Code\n\n\n\n\n\nWeek 2\n\n\n\nDay\nSession\nPlace\nTopic\n\n\n\n\nApr 22\nAM\nRunde\nTransformations, non-parametric methods\n\n\n\nPM\nRunde\nSample size and statistical power\n\n\nApr 23\nAM\nRunde\nStudy designs\n\n\n\nPM\nRunde\nLinear regression I\n\n\nApr 24\nAM\nAud 13\nLinear regression II\n\n\n\nPM\nAud 13\nLinear regression III\n\n\n\n\n\nCourse summary\n\n\nApr 25\nAM\nRunde\nLogistic regression\n\n\n\nPM\nRunde\nSurvival analysis"
  },
  {
    "objectID": "lab_diagtest.html",
    "href": "lab_diagtest.html",
    "title": "Probability and diagnostic testing",
    "section": "",
    "text": "tp &lt;- 22\ntn &lt;- 331\nfp &lt;- 16\nfn &lt;- 3\npositives &lt;- tp + fn\npositives\n\n[1] 25\n\nnegatives &lt;- tn + fp\nnegatives\n\n[1] 347\n# sensitivity: tp / positives\ntp/positives\n\n[1] 0.88\n\n# specificity: tn / negatives\ntn/negatives\n\n[1] 0.9538905\n\n# positive predictive value ppv\n# tp / positive test\ntp / (tp+fp)\n\n[1] 0.5789474"
  },
  {
    "objectID": "lab_diagtest.html#hiv-example",
    "href": "lab_diagtest.html#hiv-example",
    "title": "Probability and diagnostic testing",
    "section": "HIV example",
    "text": "HIV example\n\n# prevalence 0.1%\n# other values\n# prevalence &lt;- 0.01\n# prevalence &lt;- 0.1\n# prevalence &lt;- 0.0001\nprevalence &lt;- 0.001\n\n\n# false positive rate 0.2%\n# fpr is 1-specificity\nspecificity &lt;- 1-0.002\nspecificity\n\n[1] 0.998\n\n# false negative rate 2%\n# fnr is 1-sensitivity\nsensitivity &lt;- 1-0.02\nsensitivity\n\n[1] 0.98\n\n\nFind positive predictive value.\n\na &lt;- sensitivity * prevalence\nb &lt;- sensitivity * prevalence + (1-specificity) * (1-prevalence)\na/b\n\n[1] 0.3290799\n\n# you can skip the step where you name a and b\n(sensitivity * prevalence) / (sensitivity * prevalence + (1-specificity) * (1-prevalence))\n\n[1] 0.3290799"
  },
  {
    "objectID": "lab/lab_distribution.html",
    "href": "lab/lab_distribution.html",
    "title": "Binomial and Normal distribution",
    "section": "",
    "text": "On this page you will find some examples and code for probability distributions."
  },
  {
    "objectID": "lab/lab_distribution.html#randomness-and-simulation",
    "href": "lab/lab_distribution.html#randomness-and-simulation",
    "title": "Binomial and Normal distribution",
    "section": "Randomness and simulation",
    "text": "Randomness and simulation\n\nsample(1:6, size = 2, replace = T)\n\n[1] 4 5\n\n# set replacement False\nsample(1:3, size = 3, replace = F)\n\n[1] 1 3 2\n\nsample(1:3, size = 3, replace = T) # replacement is allowed\n\n[1] 2 3 1\n\n\n\n# if set seed, the result is the same\n# you need to run the line set.seed(yourseed) right before\nset.seed(1)\nsample(1:6, size = 2, replace = T)\n\n[1] 1 4"
  },
  {
    "objectID": "lab/lab_distribution.html#binomial-distribution",
    "href": "lab/lab_distribution.html#binomial-distribution",
    "title": "Binomial and Normal distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nBinomial distribution has two parameters: \\(n\\) and \\(p\\). In R, they are referred to as size and prob. Consider the following process:\n\nan experiment or trial has two outcomes: 1 and 0; positive or negative; boy or girl; …\nthe probability of having 1 as outcome (or ‘success’) is \\(p\\)\nrepeat the experiment \\(n\\) times, where each one experiment does not affect the other (independent)\ncount how many times there are zero 1’s, one 1’s, …, n 1’s\n\nThe probability associated with each possibility can be described by binomial distribution (n,p).\n\nCoin toss example\nWe can start with simulating 500 random sample from a binomial distribution whose parameters (n,p) are (1, 0.5). This is like tossing a fair coin once (n=1), whose probability of having Heads and Tails are 50/50 (p=0.5). We toss it 500 times, and record each time what outcome it is.\nWe can print out the first 20 results, and visualize all the frequencies from the 500 outcomes.\n🟡 Loading\n  webR...\n\n\n  \n\n\nHere you see that the frequency for 1 and 0 outcomes are very close to 0.5, even though sometimes there are slightly more 1 than 0 and vice versa.\nThe X-axis should be only two discrete values - 0 and 1; and nothing between.\n\n\nDifferent n and p\nFor a random variable \\(X\\) and parameters \\(n, p\\), the formula of binomial distribution is\n\\[P(X = k) = {n \\choose k}p^{k}(1-p)^{n-k}\\] Here \\(k\\) is the number of success (1’s) among \\(n\\) experiments. For example, \\(n=5\\), the possible range of \\(k\\) could be 0, 1, 2, 3, 4, 5 successes. The number of successes is affected by the probability \\(p\\) for each one experiment - in general, the higher \\(p\\) is, the more likely you get larger number of successes. This can be seen from the histogram.\nYou can experiment with different combinations to see the effect of size and prob, N and P. Try the following combinations:\n\nN: 1, 5, 8, 20, 50, 100\nP: 0.15, 0.5, 0.8, 0.95\n\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "lab/lab_distribution.html#normal-distribution",
    "href": "lab/lab_distribution.html#normal-distribution",
    "title": "Binomial and Normal distribution",
    "section": "Normal distribution",
    "text": "Normal distribution\nThe normal distribution has a bell-shape, and is symmetrical. In a simulation, it might not look perfectly symmetrical; but when you have a large enough sample, it should be close enough to a bell-shaped histogram.\n\n# two parameters: mean and sd\nx01 &lt;- rnorm(1000, mean = 0, sd = 1)\nhist(x01, main = '1000 normally distributed data N(0,1)')\n\n\n\n\n\n\n\nx102 &lt;- rnorm(1000, mean = 10, sd = 2)\nhist(x102, main = '1000 normally distributed data N(10,2)')\n\n\n\n\n\n\n\n\nYou can find the summary statistics for the simulated data.\n\n# check the summary\nsummary(x102)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.213   8.734  10.009  10.027  11.305  16.391 \n\n# mean\nmean(x102)\n\n[1] 10.02688\n\n# variance and sd\nvar(x102) # (sd(x102))^2 # these two are equivalent\n\n[1] 3.846535\n\nsd(x102)\n\n[1] 1.961259\n\n\nIt can be helpful to visualize the mean on top of the histogram.\n\n# can plot the mean on the histogram to indicate the center\nhist(x102, main = '1000 normally distributed data N(10,2)')\n\n# v means verticle line, lwd means line width\nabline(v = mean(x102), col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\n\nImportant values\nFor a standard normal distribution, its 2.5% and 97.5% quantiles (\\(\\pm 1.96\\)) are frequently used. This means that\n\nprobability below -1.96 is approximately 0.025 (2.5%)\nprobability above 1.96 is approximately 0.025; below 1.96 is 1-0.025 = 0.975\nprobability between \\(\\pm 1.96\\) is approximately 95%.\n\n\n# pnorm finds the probability below a value\np1 &lt;- pnorm(-1.96, mean = 0, sd = 1)\np2 &lt;- pnorm(1.96, mean = 0, sd = 1)\nc(p1, p2) # print the values\n\n[1] 0.0249979 0.9750021\n\n1-p1 # equal to p2\n\n[1] 0.9750021\n\nqnorm(0.025, mean = 0, sd = 1) # 2.5% quantile\n\n[1] -1.959964\n\nqnorm(0.975, mean = 0, sd = 1) # 97.5%\n\n[1] 1.959964"
  },
  {
    "objectID": "lab/lab_distribution.html#bar-plot",
    "href": "lab/lab_distribution.html#bar-plot",
    "title": "Binomial and Normal distribution",
    "section": "Bar plot",
    "text": "Bar plot\nWhen you have data in groups, you can plot them in a bar plot.\n\ndd &lt;- data.frame(prob = c(0.03, 0.14, 0.13, 0.38, 0.33),\n                 n = c(3,16,15,44,38))\nrownames(dd) &lt;- c('0-17', '18-24', '25-34', '35-64', '65+')\n\n\n# we can plot two graphs side by side\n# set parameter: 1 row 2 columns\npar(mfrow = c(1, 2))\n# bar plot for counts\nbarplot(dd$n, names.arg = rownames(dd),\n        main = 'Number of killed for road accidents')\n\n\n# bar plot for probability\nbarplot(dd$prob, names.arg = rownames(dd),\n        main = 'Proportion for killed road accidents')"
  },
  {
    "objectID": "lab/lab_distribution.html#example-birth-weight",
    "href": "lab/lab_distribution.html#example-birth-weight",
    "title": "Binomial and Normal distribution",
    "section": "Example: birth weight",
    "text": "Example: birth weight\nThis is the example we used in class to illustrate normal distribution. We are going to reproduce the example in class: explore birth weight variable, bwt. Our task is to estimate the probability (or proportion) for birth weight above 4000g.\nLoad the data birth.csv (or another data format). You can use the point-and-click in Rstudio to load the dataset.\nCreate a variable called birth_weight by extracting the bwt column, using the dollar sign operator. Make a histogram of this variable.\n\n# load birth data first\n# if you forgot, check notes from day 2 (descriptive stat)\nbirth &lt;- read.csv('data/birth.csv', sep =',')\nhead(birth) # print 6 rows\n\n  id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1  4 bwt &lt;= 2500  28 120 other    smoker   1  no yes   0   0  709\n2 10 bwt &lt;= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3 11 bwt &lt;= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4 13 bwt &lt;= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5 15 bwt &lt;= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6 16 bwt &lt;= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n\n# we use the variable bwt\nbirth_weight &lt;- birth$bwt\nhist(birth_weight)\n\n\n\n\n\n\n\n\n\nBy counting\nYou can count how many data points fits your criteria: weight above 4000. Let us assume that this is strictly above, hence equal to 4000 is not considered.\n\nbirth_weight&gt;4000 # converts 189 values to T/F indicator\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n# we see 9 TRUE. you can print out the birth_weight values to check if it's the case\n# 'which' finds the indices for TRUE\nwhich(birth_weight &gt;4000)\n\n[1] 181 182 183 184 185 186 187 188 189\n\n# length counts the number of elements, here it is 9\nlength(which(birth_weight &gt;4000)==T)\n\n[1] 9\n\n\nR treats the logical values TRUE as 1 and FALSE as 0; so you can also do mathematical operation such as sum directly.\n\n# alternatively, since R codes T as 1 and F as 0\n# we can use sum() command\nsum(birth_weight&gt;4000)\n\n[1] 9\n\n# probability is 9 over total subjects\n9/189\n\n[1] 0.04761905\n\n\n\n\nUse normal approximation\nSince the variable birth_weight looks similar to a normal distribution, we can also use normal approximation for this task. First we should find out the parameters for this distribution, by estimating the mean \\(m\\) and standard deviation \\(s\\).\nAfterwards, there are two options:\n\nestimate \\(P(X&gt;4000)\\) when \\(X \\sim N(m, s)\\)\nestimate \\(P(Y&gt;\\frac{4000-m}{s})\\) when \\(Y \\sim N(0,1)\\)\n\n\n# first get the parameters mean and sd (or sqrt variance)\nm &lt;- mean(birth_weight)\ns &lt;- sd(birth_weight)\nc(m, s) # print out\n\n[1] 2944.6561  729.0224\n\n# s &lt;- sqrt(var(birth_weight))\n\n# probability of birthweight above 4000\n\n# directly from N(2944, 729)\n# lower.tail = F means it computes p(X&gt;4000) \npnorm(4000, mean = m, sd =s, lower.tail = F)\n\n[1] 0.07386235\n\n# you can also use the standard normal dist\n# whose mean and sd (var) are 0 and 1\n# can be translated as a second variable Y above 1.45 \n# see lecture notes for why this is the case!\n# hint: (4000-m)/s\npnorm(1.45, lower.tail = F)\n\n[1] 0.07352926"
  },
  {
    "objectID": "lab/lab_diagtest.html",
    "href": "lab/lab_diagtest.html",
    "title": "Probability and diagnostic testing",
    "section": "",
    "text": "In this page you will find some examples from the lecture. You are not required to do your exercises with R, however R can be used as a calculator.\n\nSensitivity = TP / P = TP / (TP+FN)\nSpecificity = TN / N = TN / (TN+FP)\nPositive predictive value PPV = TP / test positive = TP / (TP + FP)\n\nIn addition,\n\nfalse positive rate (FPR) is 1 - specificity\nfalse negative rate (FNR) is 1 - sensitivity\n\nWhen you have prevalence of the disease, the positive and negative predictive value PPV, NPV are\n\\[PPV = \\frac{sens \\times prev}{sens \\times prev + (1-spec) \\times (1 - prev)}\\]\n\\[NPV = \\frac{spec \\times prev}{(1-sens) \\times prev + spec \\times (1-prev)}\\]\n\nCoin toss example\nYou can replicate the coin toss example in the probability section. Modify \\(n\\) (number of tosses) and \\(p\\) (probability of having 1 as outcome) to see what happens.\n🟡 Loading\n  webR...\n\n\n  \n\n\n\n\nMammography example\nIn the mammography example, we are given the counts of patients that fall into one of the four categories:\n\n22 tested cancer really have cancer\n331 tested no cancer really do not have cancer\n16 tested cancer, but do not have cancer\n3 tested no cancer, but really have cancer\n\nAccording to the terminology introduced in class, we can create four variables to store the data.\n\ntp &lt;- 22\ntn &lt;- 331\nfp &lt;- 16\nfn &lt;- 3\n\nWe can find how many patients really have cancer, or not have cancer. Here positive and negative refer to the actual disease status, not thest test result.\n\n# positive means positive condition - real disease status\npositives &lt;- tp + fn\npositives\n\n[1] 25\n\nnegatives &lt;- tn + fp\nnegatives\n\n[1] 347\n\n\nNow find the sensitivity, specificity, positive predictive value.\n\n# sensitivity: tp / positives\ntp/positives\n\n[1] 0.88\n\n# specificity: tn / negatives\ntn/negatives\n\n[1] 0.9538905\n\n# positive predictive value ppv\n# tp / positive test\ntp / (tp+fp)\n\n[1] 0.5789474\n\n\n\n\nHIV example\nThe HIV example from the class demonstrates the how prevalence affects metrics of diagnostic tests.\nTo start, we set prevalence to 0.001.\n\n# prevalence 0.1%\nprevalence &lt;- 0.001\n\nWe were given some terms that are not sensitivity and specifity, but it is straightfoward to translate into terms we know.\n\n# false positive rate 0.2%\n# fpr is 1-specificity\nspecificity &lt;- 1-0.002\nspecificity\n\n[1] 0.998\n\n# false negative rate 2%\n# fnr is 1-sensitivity\nsensitivity &lt;- 1-0.02\nsensitivity\n\n[1] 0.98\n\n\nFind positive predictive value from the formula.\n\na &lt;- sensitivity * prevalence\nb &lt;- sensitivity * prevalence + (1-specificity) * (1-prevalence)\na/b\n\n[1] 0.3290799\n\n# you can skip the step where you name a and b\n(sensitivity * prevalence) / (sensitivity * prevalence + (1-specificity) * (1-prevalence))\n\n[1] 0.3290799\n\n\nNow test out different values of prevalence. Replace the value of prevalence, then run the following code by pressing the button.\nYou can try the following prevalence values: 0.01%, 0.1%, 1%, 10%.\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "lab/lab_diagtest.html#mammography-example",
    "href": "lab/lab_diagtest.html#mammography-example",
    "title": "Probability and diagnostic testing",
    "section": "Mammography example",
    "text": "Mammography example\nIn the mammography dataset, we are given the counts of patients that fall into one of the four categories:\n\n22 tested cancer really have cancer\n331 tested no cancer really do not have cancer\n16 tested cancer, but do not have cancer\n3 tested no cancer, but really have cancer\n\nAccording to the terminology introduced in class, we can create four variables to store the data.\n\ntp &lt;- 22\ntn &lt;- 331\nfp &lt;- 16\nfn &lt;- 3\n\nWe can find how many patients really have cancer, or not have cancer. Here positive and negative refer to the actual disease status, not thest test result.\n\n# positive means positive condition - real disease status\npositives &lt;- tp + fn\npositives\n\n[1] 25\n\nnegatives &lt;- tn + fp\nnegatives\n\n[1] 347\n\n\nNow find the sensitivity, specificity, positive predictive value.\n\n# sensitivity: tp / positives\ntp/positives\n\n[1] 0.88\n\n# specificity: tn / negatives\ntn/negatives\n\n[1] 0.9538905\n\n# positive predictive value ppv\n# tp / positive test\ntp / (tp+fp)\n\n[1] 0.5789474"
  },
  {
    "objectID": "lab/lab_diagtest.html#hiv-example",
    "href": "lab/lab_diagtest.html#hiv-example",
    "title": "Probability and diagnostic testing",
    "section": "HIV example",
    "text": "HIV example\nThe HIV example from the class demonstrates the how prevalence affects metrics of diagnostic tests.\nTo start, we set prevalence to 0.001.\n\n# prevalence 0.1%\nprevalence &lt;- 0.001\n\nWe were given some terms that are not sensitivity and specifity, but it is straightfoward to translate into terms we know.\n\n# false positive rate 0.2%\n# fpr is 1-specificity\nspecificity &lt;- 1-0.002\nspecificity\n\n[1] 0.998\n\n# false negative rate 2%\n# fnr is 1-sensitivity\nsensitivity &lt;- 1-0.02\nsensitivity\n\n[1] 0.98\n\n\nFind positive predictive value from the formula.\n\na &lt;- sensitivity * prevalence\nb &lt;- sensitivity * prevalence + (1-specificity) * (1-prevalence)\na/b\n\n[1] 0.3290799\n\n# you can skip the step where you name a and b\n(sensitivity * prevalence) / (sensitivity * prevalence + (1-specificity) * (1-prevalence))\n\n[1] 0.3290799\n\n\nNow test out different values of prevalence. Replace the value of prevalence, then run the following code by pressing the button.\nYou can try the following prevalence values: 0.01%, 0.1%, 1%, 10%.\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "lab/lab_distribution.html#binomial-vs-normal-distribution",
    "href": "lab/lab_distribution.html#binomial-vs-normal-distribution",
    "title": "Binomial and Normal distribution",
    "section": "Binomial vs Normal distribution",
    "text": "Binomial vs Normal distribution\nUnder certain conditions, binomial distribution can be approximated by a normal distribution. Try out different values of \\(N, P\\) to see when the approximation breaks!\nFor example, start with\n\nN: 10, 20, 50, 100\nP: 0.1, 0.5, 0.8\n\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "MF9130E - Introductory course in Statistics",
    "section": "Preparation",
    "text": "Preparation\nYou should have a working solution of R and RStudio (either installed on your own laptop, or using Posit cloud) before the course.\nPlease also familiarize yourself with the course website.\n\nGet Started provides some information about software installation, where to download data and code, and some resources.\nSchedule provides an overview of the planned sessions, and links to course material used in each session.\nR Lab and Code hosts the lab session notes collectively."
  },
  {
    "objectID": "index.html#schedule-and-course-material",
    "href": "index.html#schedule-and-course-material",
    "title": "MF9130E - Introductory course in Statistics",
    "section": "Schedule and course material",
    "text": "Schedule and course material\nOn Schedule page, you will see a list of topics and links to the material. This should be consistent with the official UiO schedule provided by University of Oslo.\nIf there is an error in the time and place on this page, please refer to the official schedule.\n\n\n\n\n\n\nNote\n\n\n\nWe will primarily use this website (instead of Canvas) for hosting material and exercises. Please let us know if you have issue accessing data files or content!"
  },
  {
    "objectID": "lab/lab_distribution.html#randomness-and-simulation-optional",
    "href": "lab/lab_distribution.html#randomness-and-simulation-optional",
    "title": "Binomial and Normal distribution",
    "section": "Randomness and simulation (optional)",
    "text": "Randomness and simulation (optional)\nA random sample is random. For example, if you throw a 6 sided dice twice, you will possibly have different outcomes. This process can be simulated with the line below.\nYou can copy and paste this line to your R console, and run it a few times. Alternatively, click on Run Code button below a few times. You should see different results every time.\n🟡 Loading\n  webR...\n\n\n  \n\n\nBy setting the seed in programming, it means you fix the process of the random data generator. This ensures that your results is reproducible regardless of when you run the line below.\nYou can try to set a different value of the seed number, say set.seed(20) and see what happens!\n🟡 Loading\n  webR...\n\n\n  \n\n\nThe randomness in the data generating process implies that if you simulate two datasets with the same parameters, the output could be different, even though both are from the same theoretical distribution.\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-1-heart",
    "href": "lab/lab_ttest.html#exercise-1-heart",
    "title": "t-test",
    "section": "Exercise 1 (heart)",
    "text": "Exercise 1 (heart)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn &lt;- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 &lt;- qt(p = 0.025, df = n-1) # -2.09\nt975 &lt;- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n# plug in the formula\nci_lower &lt;- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper &lt;- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals)."
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-2-lung-function-1",
    "href": "lab/lab_ttest.html#exercise-2-lung-function-1",
    "title": "t-test",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n# if you use rda data file: \n# load('./lab/data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data &lt;- read.csv('data/PEFH98-english.csv', sep = ',')\nhead(lung_data)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n4  21   male    179     74     520     530     540     480     510     500\n5  20   male    196     95     740     750     750     700     710     700\n6  20   male    189     83     600     575     600     600     600     640\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n4 530.0000 496.6667 513.3333\n5 746.6667 703.3333 725.0000\n6 591.6667 613.3333 602.5000\n\n# we can focus on height and gender variable only\nhead(lung_data[, c('height', 'gender')], 10)\n\n   height gender\n1     165 female\n2     185   male\n3     178   male\n4     179   male\n5     196   male\n6     189   male\n7     173   male\n8     196   male\n9     173 female\n10    173 female\n\n\nNow we need to separate the height data for based on gender. First, we do it for gender == 'female'.\n\n# for convenience, we create a variable names 'gender'\ngender &lt;- lung_data$gender\n# height for female\nheight_f &lt;- lung_data$height[gender == 'female']\n\nYou should always check whether your newly created variable is correct: for example, you can compare the first several values of height_f with the original data, to see if it is really only selecting height for females.\nAnother useful thing to do is to check how many data poinnts have been selected.\n\n# check the first few values, is it only selecting female heights?\nhead(height_f)\n\n[1] 165 173 173 169 170 172\n\n# number of females\nnf &lt;- length(height_f)  # 54\nnf\n\n[1] 54\n\n\nNow we can compute the mean and confidence interval on the newly created variable, height_f.\n\nmean(height_f) # 169.57\n\n[1] 169.5741\n\nsd(height_f) # 5.69\n\n[1] 5.692106\n\n# se_f &lt;- sd(height_f)/sqrt(54) 0.774\n\n# quantile for t distribution: pay attention to df!\nt975 &lt;- qt(p = 0.975, df = nf-1) # 2.005\n\n# 95% CI \nci_upper_f &lt;- mean(height_f) + t975 * sd(height_f)/sqrt(nf) # 171.1277\nci_lower_f &lt;- mean(height_f) - t975 * sd(height_f)/sqrt(nf) # 168.0204\nc(ci_lower_f, ci_upper_f)\n\n[1] 168.0204 171.1277\n\n# double check by running a t.test\nt.test(height_f)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 218.92, df = 53, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nBased on our calculation, the mean height for females is 169.57 cm (95% confidence interval (168.02, 171.13)).\nFor gender == 'male', we can do the same thing. Pay attention to the different degrees of freedom, because the sample size has changed.\n\n# height for male \nheight_m &lt;- lung_data$height[gender == 'male']\n# number of males\nnm &lt;- length(height_m)  # 52\n\nmean(height_m) # 181.87\n\n[1] 181.8654\n\nsd(height_m) # 5.67\n\n[1] 5.667343\n\n# se_m &lt;- sd(height_m)/sqrt(52) # 0.786\n\n# find quantile for males (pay attention to df)\nt975 &lt;- qt(p = 0.975, df = nm-1) # 2.007\n# 95% CI\nci_upper_m &lt;- mean(height_m) + t975 * sd(height_m)/sqrt(nm) # 183.44\nci_lower_m &lt;- mean(height_m) - t975 * sd(height_m)/sqrt(nm)  # 180.28\nc(ci_lower_m, ci_upper_m)\n\n[1] 180.2876 183.4432\n\n# verify by t.test\nt.test(height_m)\n\n\n    One Sample t-test\n\ndata:  height_m\nt = 231.4, df = 51, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 180.2876 183.4432\nsample estimates:\nmean of x \n 181.8654 \n\n\nBased on our calculation, the mean height for males is 181.87 cm (95% confidence interval (180.28, 183.44)).\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\nLet \\(\\mu_F\\) indicate the average height for the female students. We test the hypothesis\n\\(H_0: \\mu_F = 167\\) against \\(H_a: \\mu_F \\neq 167\\) (two sided test).\n\n# use t.test command\nt.test(height_f, mu = 167)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 3.3231, df = 53, p-value = 0.001619\nalternative hypothesis: true mean is not equal to 167\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nWe can conclude that the average height for female students differ significantly from 167 (p = 0.0016). We reject the null hypothesis.\nIf you want to verify the p-value by hand: follow the procedure below.\n\n# (optional) calculate from the formula\nt_stat &lt;- (mean(height_f) - 167)/(sd(height_f)/sqrt(nf))\nt_stat  # 3.323\n\n[1] 3.323112\n\n# compare this with t distribution with nf-1 degrees of freedom\npval_twosided &lt;- pt(q = t_stat, df = nf-1, lower.tail = F)*2\npval_twosided\n\n[1] 0.001618751\n\n\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\nHere we use paired t-test, because these two measurements are on the same subject.\n\n\n\n\n\n\nMissing data in pefsit2\n\n\n\n\n\nWhen you run mean(lung_data$pefsit2), it might return NA as a result. This is caused by missing values in this variable. You can check this from the data (click on lung_data in your environment)(row 66)\nThis does not affect t.test() as it will remove NA automatically. However this might affect other functions, such as mean().\nYou can do mean(pefsit2, na.rm = T) (remove NA). This does not remove NA from your data forever; but only for your mean computation.\nYou should check whether there are NA in your data. One option is summary(pefsit2).\n\n\n\n\npefsit1 &lt;- lung_data$pefsit1\npefsit2 &lt;- lung_data$pefsit2\n# hist(pefsit1)\n# hist(pefsit2)\n# compute mean:\nmean(pefsit1)\n\n[1] 504.6038\n\nmean(pefsit2) # this returns NA, bec subject 66 has missing data\n\n[1] NA\n\nlung_data[66, ]\n\n   age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n66  19 female    166     56     450      NA      NA      NA     425      NA\n   pefsitm pefstam pefmean\n66      NA      NA      NA\n\n# to remove this when computing mean by mean(): \nmean(pefsit2, na.rm = T) # remove NA when computing mean\n\n[1] 509.5238\n\n# (how to check if my data has NA: is.na(pefsit2); summary(pefsit2))\nsummary(pefsit2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  280.0   415.0   500.0   509.5   580.0   800.0       1 \n\n# t-test will automatically remove NA \nt.test(pefsit1, pefsit2, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsit1 and pefsit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean difference \n           -4.4 \n\n# alternatively, you can test whether the difference is equal to 0\ndiff_sit1_sit2 &lt;- pefsit1 - pefsit2\nt.test(diff_sit1_sit2, mu = 0)\n\n\n    One Sample t-test\n\ndata:  diff_sit1_sit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean of x \n     -4.4 \n\n\nWe can conclude that there is no significant difference (p = 0.12) between the first and second pef measurement.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\nWe still use paired test as both measurements are on the same subject.\n\n# compare pefsitm, pefstam (paired t-test)\npefsitm &lt;- lung_data$pefsitm\npefstam &lt;- lung_data$pefstam\n\nt.test(pefsitm, pefstam, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsitm and pefstam\nt = -3.6974, df = 104, p-value = 0.0003498\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -14.656161  -4.423204\nsample estimates:\nmean difference \n      -9.539683 \n\n\nWe can conclude that that there is a significant difference (p &lt; 0.001) between pef measured in a sitting and standing position.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference &lt;- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n# create a variable\ndiff_sitm_stam &lt;- pefsitm - pefstam\nqqnorm(diff_sitm_stam, pch = 20)\nqqline(diff_sitm_stam, col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nWhen a Q-Q plot looks like this, we can say that the normality assumption is reasonably fulfilled.\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n# this is independent two samples t-test\n# we need two variables: pefmean for men, pefmean for women\n\npefmean_f &lt;- lung_data$pefmean[gender == 'female']\npefmean_m &lt;- lung_data$pefmean[gender == 'male']\n\n# visually spot whether there is a difference\n# NOTE: there is a NA in pefmean_f; use na.rm = T to remove it \npar(mfrow = c(1, 2))\nhist(pefmean_f, main = 'pefmean (F)')\nabline(v = mean(pefmean_f, na.rm = T), col = 'red', lwd = 2)\nhist(pefmean_m, main = 'pefmean (M)')\nabline(v = mean(pefmean_m), col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nFrom the histogram for female and male students, it can be seen that the difference between the two mean pef measurements differ. We can test it using t-test for two independent samples.\n\n\n\n\n\n\nEqual variance assumption\n\n\n\n\n\nFor two sample t-test, there is an assumption of equal variance in two groups. In R, t.test() automatically checks whether this is fulfilled. If not, it returns result from Welch’s t-test.\nIf you want to force t.test() to use equal variance, specify var.equal = T.\nThe ways to interpret results are the same.\n\n\n\n\n# two sample t-test (Welch)\nt.test(pefmean_f, pefmean_m, paired = F)\n\n\n    Welch Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.425, df = 90.28, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.6366 -150.3941\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n# force equal variance\nt.test(pefmean_f, pefmean_m, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.468, df = 103, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.4907 -150.5401\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n\nFrom both results you can see that the difference in means between two genders is very big. The p-values are very small (p &lt; 0.001), can reject the null hypothesis that the sample means from two groups are equal.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?\n\npar(mfrow = c(1, 2))\nqqnorm(pefmean_f, pch = 20, main = 'Q-Q plot: pefmean (F)')\nqqline(pefmean_f, col = 'red', lwd = 2)\n\nqqnorm(pefmean_m, pch = 20, main = 'Q-Q plot: pefmean (M)')\nqqline(pefmean_m, col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nThe normality assumption is fulfilled."
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-1-weight-1",
    "href": "lab/lab_eda_part1.html#exercise-1-weight-1",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 1 (weight)",
    "text": "Exercise 1 (weight)\n\n1a\nGenerate a variable named weight, with the following measurements:\n50 75 70 74 95 83 65 94 66 65 65 75 84 55 73 68 72 67 53 65\n\nweight &lt;- c(50, 75, 70, 74, 95, \n            83, 65, 94, 66, 65, \n            65, 75, 84, 55, 73, \n            68, 72, 67, 53, 65)\n\n\n\n1b\nMake a simple descriptive analysis of the variable, what are the mean, median, maximum, minimum and quantiles?\nHow to interpret the data?\n\nmean(weight)\n\n[1] 70.7\n\nmedian(weight)\n\n[1] 69\n\nmax(weight)\n\n[1] 95\n\nmin(weight)\n\n[1] 50\n\n# alternatively, \nsummary(weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0    65.0    69.0    70.7    75.0    95.0 \n\n\n\n\n1c\nMake a histogram.\n\nhist(weight)\n\n\n\n\n\n\n\n\n\n\n1d\nMake a boxplot. What do the two dots on the top represent?\n\nboxplot(weight)\n\n\n\n\n\n\n\n\nThey are the largest two points in the dataset. These are outliers in this data.\n\n\n\n\n\n\n(Optional) Outliers in box plot\n\n\n\n\n\nThere are different ways to define outliers. The default box plot in R determines points beyond \\(Q_1 - 1.5\\times IQR\\) and \\(Q_3 + 1.5\\times IQR\\) are outliers. You can check the values of quartiles using summary(), and read up the documentation ?boxplot()."
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-2-lung-function-1",
    "href": "lab/lab_eda_part1.html#exercise-2-lung-function-1",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nDownload and open PEFH98-english.dta into R.\nIf you have problem with .dta data format, you can also use PEFH98-english.csv.\nPay attention to how gender is coded. We might have to modify it.\n\n# locate your datafile, set the path to your data\n# if you use rda data file: \n# load('./lab/data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data &lt;- read.csv('data/PEFH98-english.csv', sep = ',')\nhead(lung_data)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n4  21   male    179     74     520     530     540     480     510     500\n5  20   male    196     95     740     750     750     700     710     700\n6  20   male    189     83     600     575     600     600     600     640\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n4 530.0000 496.6667 513.3333\n5 746.6667 703.3333 725.0000\n6 591.6667 613.3333 602.5000\n\n\n\n\n2b)\nHow many observations are there (number of subjects)? How do you get a list of variable names from your dataset?\n\nnrow(lung_data)\n\n[1] 106\n\ncolnames(lung_data)\n\n [1] \"age\"     \"gender\"  \"height\"  \"weight\"  \"pefsit1\" \"pefsit2\" \"pefsit3\"\n [8] \"pefsta1\" \"pefsta2\" \"pefsta3\" \"pefsitm\" \"pefstam\" \"pefmean\"\n\n\nMake a histogram for each of the following variables. Compute means, and interpret the results.\nheight\nweight\nage\npefsitm\npefstam\n\nhist(lung_data$height)\n\n\n\n\n\n\n\n\nWe repeat it for the other 4 variables. We can put them more compactly,\n\npar(mfrow = c(2, 2)) \n# we use this line to display (2 rows 2 columns)\n# by default it is 1 row 1 column\n# run this line to set it back to default:\n# par(mfrow = c(1, 1))\nhist(lung_data$weight)\nhist(lung_data$age)\nhist(lung_data$pefsitm)\nhist(lung_data$pefstam)\n\n\n\n\n\n\n\n\n\n\n2c)\nMake histograms for the variables height and pefmean for men and women separately. Also try to make boxplots.\nWhat conclusion can you draw?\n\nheight_f &lt;- lung_data$height[lung_data$gender == 'female']\nheight_m &lt;- lung_data$height[lung_data$gender == 'male']\n\npar(mfrow = c(1,2)) # plot in parallel\nhist(height_f)\nhist(height_m)\n\n\n\n\n\n\n\n# we can make it more customized\n# add axis limit, title and xaxis name\npar(mfrow = c(1,2)) # plot in parallel\nhist(height_f, main = 'Height: female', xlab = 'Height (cm)',\n     xlim = c(150, 200))\nhist(height_m, main = 'Height: male', xlab = 'Height (cm)',\n     xlim = c(150, 200))\n\n\n\n\n\n\n\n\nSimilarly, histogram for pefmean can be done in the same way.\n\npefmean_f &lt;- lung_data$pefmean[lung_data$gender == 'female']\npefmean_m &lt;- lung_data$pefmean[lung_data$gender == 'male']\n\npar(mfrow = c(1,2)) # plot in parallel\nhist(pefmean_f)\nhist(pefmean_m)\n\n\n\n\n\n\n\n\nNow we can make some boxplots\n\npar(mfrow = c(1, 2))\nboxplot(height ~ gender, data = lung_data, main = 'Height vs Gender')\n\n# it is also possible to remove the frame\nboxplot(pefmean ~ gender, data = lung_data, frame = F, main = 'PEFmean vs gender')\n\n\n\n\n\n\n\n\n\n\n2d)\nMake three scatterplots to compare\n\npefmean with height\npefmean with weight\npefmean with age\n\nWhat association do you see?\n\n# pefmean height\nplot(lung_data$pefmean, lung_data$height)\n\n\n\n\n\n\n\n# it is possible to customize \nplot(lung_data$pefmean, lung_data$height, \n     main = 'PEF mean vs height', \n     xlab = 'PEF mean', ylab = 'Height',\n     pch = 20)\n\n\n\n\n\n\n\n# pch: plotting symbols\n\npch = 20 is setting the symbol to small solid dots. You can try different values, from 0 to 25. Read more\n\npar(mfrow = c(1, 2))\n# pefmean weight\nplot(lung_data$pefmean, lung_data$weight, \n     main = 'PEF mean vs weight', \n     xlab = 'PEF mean', ylab = 'Weight',\n     pch = 20)\n\n# pefmean age\nplot(lung_data$pefmean, lung_data$age, \n     main = 'PEF mean vs age', \n     xlab = 'PEF mean', ylab = 'Age',\n     pch = 20)"
  },
  {
    "objectID": "lab/lab_intro_rstudio.html#rstudio-and-posit",
    "href": "lab/lab_intro_rstudio.html#rstudio-and-posit",
    "title": "Getting started in RStudio",
    "section": "RStudio and Posit",
    "text": "RStudio and Posit\nRStudio is a free and open-source IDE (integrated development environment). RStudio IDE is developed by Posit, previously RStudio PBC.\nIt is convenient to download RStudio to your laptop or desktop; or use RStudio Cloud (need internet connection).\nWhen you open Rstudio, you will see something like this.\n\n\n\n\n\n\n\nEmpty bottom right panel\n\n\n\nYou might NOT see anything in the bottom right panel, because you might not have a Project yet.\nDon’t worry, we will learn how to create it."
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-1-lung-function-1",
    "href": "lab/lab_categorical.html#exercise-1-lung-function-1",
    "title": "Categorical data analysis",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe investigate whether having a high value of pefmean is associated with gender. First, create a new variable highpef that indicates whether pefmean is above 500.\n\n# if you use rda data file: \n# load('./data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data &lt;- read.csv('data/PEFH98-english.csv', sep = ',')\n# head(lung_data)\n\n# examine variable pefmean\n# visualise the distribution\nhist(lung_data$pefmean)\nabline(v = 500, col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nThere are multiple ways to do it. Here we show two of them.\nIn option 1, we use the function ifelse(). It will create a binary vector, and fill in different output based on whether the condition was true or not.\n\n# option 1\n# code new variable: highpef with cutoff = 500\n# this is a binary variable, 1 means yes (higher than 500), 0 means no\nhighpef &lt;- ifelse(lung_data$pefmean &gt; 500, '1', '0')\n\n# check if it makes sense\nhead(lung_data$pefmean)\n\n[1] 405.0000 491.6667 505.0000 513.3333 725.0000 602.5000\n\nhead(highpef)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\nIn option 2, we create a vector with a pre-specified value, then replace the elements for given indices.\n\n# option 2 \n# create a vector with all '0's\n# then set the elements above 500 in 'pefmean' as '1'\n# (replacement with index)\n\nn &lt;- length(lung_data$pefmean) # 106\nhighpef_alt &lt;- rep('0', n) # repeat '0' 106 times\nhighpef_alt[lung_data$pefmean &gt; 500] &lt;- '1'\nhead(highpef_alt)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\n\n\n1b)\nWe investigate the association between these two variables via an appropriate table analysis. Make a table of highpef vs gender, which counts the number of subjects having each one of the four combinations in a 2 by 2 contingency table.\nYou can do this with table() command: it counts how many observations equals to each unique value in the data. When the data takes 2 values (1/0, yes or no), table() counts the numbers in each category.\n\n# take out gender variable so we don't need to use $ any more\ngender &lt;- lung_data$gender\n\n# we can count how many subjects are in each category for the two variables\ntable(gender)\n\ngender\nfemale   male \n    54     52 \n\ntable(highpef)\n\nhighpef\n 0  1 \n54 51 \n\n\nUsing table() for two variables produces the cross tabulation: \\(2 \\times 2\\) combinations. The first element is put as rows, and second element is put as columns.\n\n# now we count how many subjects are in the combinations \ntable_gender_highpef &lt;- table(gender, highpef)\ntable_gender_highpef\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\n\n\n\n\n\n\n\nCross tabulation, order of exposure and outcome\n\n\n\n\n\nYou should be aware that table() does not automatically produce the contingency table which puts exposed (group 1) for two outcomes at the first row; neither does it put positive outcome (outcome = yes) for exposed/unexposed group in the first column.\ntable() only counts the combination. It is up to you to make sense which numbers are \\(d1, d0, h1, h0\\)!\n\n\n\n\n\n1c)\nCompute risk ratio and odds ratio (only the point estimates, no confidence interval). Here the exposure is gender (we assume that the reference group is female: unexposed), and outcome is highpef. Interpret your results.\n\n\n\n\n\n\nRisk ratio and odds ratio\n\n\n\n\n\n\n\n\n\nOutcome (yes)\nOutcome (no)\n\n\n\n\nExposed\nd1\nh1\n\n\nUnexposed\nd0\nh0\n\n\n\nRisk ratio: \\(\\frac{d1/(d1+h1)}{d0/(d0+h0)}\\)\nOdds ratio: \\(\\frac{d1/h1}{d0/h0} = \\frac{d1 \\times h0}{d0 \\times h1}\\)\n\n\n\nIt could help by drawing a table before computing the metrics.\n\n\n\n\nOutcome (highpef = 1)\nOutcome (highpef = 0)\n\n\n\n\nExposed (gender = male)\n46\n6\n\n\nUnexposed (gender = female)\n5\n48\n\n\n\n\n# first use formula (only point estimates)\n# risk ratio (relative risk)\n# risk in exposed / risk in unexposed\n\nrisk_exposed &lt;- 46/(46+6)  # 0.885\nrisk_unexposed &lt;- 5/(5+48) # 0.094\nrr &lt;- risk_exposed/risk_unexposed\nrr  # 9.37\n\n[1] 9.376923\n\n# odds ratio\n# odds of event in exposed group / odds of event in non-exposed group\nodds_exposed &lt;- 46/6  # 7.667\nodds_unexposed &lt;- 5/48  # 0.104\nor &lt;- odds_exposed/odds_unexposed\nor # 73.6\n\n[1] 73.6\n\n\n\n\n1d)\nCarry out a chi-square analysis to assess the strength of association. Interpret your results.\nWe do a chi-square test with chisq.test() function. To read more about what is required as input, you can use ?chisq.test() to get the documentation.\n\n# note: \n# we have not distinguished between exposure and outcome\n# by default, 'table' function puts the first input in rows (gender)\n# the order does not affect the result of chi-square test\n\nchisq.test(table_gender_highpef)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_gender_highpef\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\nIf you want to know what are the observed and expected numbers used for the chi-square test, you can create a variable (let us call it ctest), and access the elements using $.\n\nctest &lt;- chisq.test(table_gender_highpef)\n# you can extract elements of ctest using $\nctest$observed # observed data\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\nctest$expected # expected data (under null, no association)\n\n        highpef\ngender          0        1\n  female 27.25714 25.74286\n  male   26.74286 25.25714\n\n\n\n\n\n\n\n\nChi-square test measures strength of association\n\n\n\nIt is worth pointing out that chisq.test() does not distinguish between exposure and outcome variables. If you reverse the variable order, the test-statistic and p-value is exactly the same.\nThis is why you should always be clear in your mind what is your exposure and outcome, and report risk ratio and/or odds ratio.\n\n\n\n# reverse the order of the two variables: highpef firsst, gender second\ntable_highpef_gender &lt;- table(highpef, gender)\nchisq.test(table_highpef_gender)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_highpef_gender\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\n\n\nOptional: use R package to compute RR and OR\n(This is for people who are interested, and able to load additional R packages and read the documentations. If this is too much: skip it)\nYou can verify if this is correct with some R packages that implement risk ratio and odds ratio.\n\n\n\n\n\n\nBe careful with the input format when using packages\n\n\n\nDifferent packages can have different requirements for how your data input should look like. Always check documentation to get the correct result.\nThis is also true with other softwares like STATA.\nComparison of different packages in computing risk ratio: read this discussion\nDocumentation: epitools Documentation: epiR\n\n\nThe first function we use is epitab from epitools package.\n\n# install.packages('epitools')\n\n# match the expected data format\n# col: outcome -, outcome +\n# row: exposure -, exposure +\ntb1 &lt;- matrix(c(48, 5, 6, 46), byrow = T, ncol = 2)\n# tb1\nepitools::epitab(tb1, method = 'riskratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 riskratio    lower    upper\n  Exposed1       48 0.9056604        5 0.09433962  1.000000       NA       NA\n  Exposed2        6 0.1153846       46 0.88461538  9.376923 4.048485 21.71842\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\nepitools::epitab(tb1, method = 'oddsratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 oddsratio    lower    upper\n  Exposed1       48 0.8888889        5 0.09803922       1.0       NA       NA\n  Exposed2        6 0.1111111       46 0.90196078      73.6 21.00627 257.8735\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\nThe second function we use is epi.2by2 from epiR package.\n\n# install.packages('epiR')\n\n# match the expected data format\n# col: outcome +, outcome -\n# row: exposure +, exposure -\ntb2 &lt;- matrix(c(46, 6, 5, 48), byrow = T, ncol = 2)\n# tb2\nepiR::epi.2by2(tb2, method = 'cohort.count')\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           46            6         52     88.46 (76.56 to 95.65)\nExposed -            5           48         53       9.43 (3.13 to 20.66)\nTotal               51           54        105     48.57 (38.70 to 58.53)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 9.38 (4.05, 21.72)\nInc odds ratio                                 73.60 (21.01, 257.87)\nAttrib risk in the exposed *                   79.03 (67.31, 90.75)\nAttrib fraction in the exposed (%)            89.34 (75.30, 95.40)\nAttrib risk in the population *                39.14 (26.76, 51.52)\nAttrib fraction in the population (%)         80.58 (57.91, 91.04)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 65.624 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-2-birth-data-1",
    "href": "lab/lab_categorical.html#exercise-2-birth-data-1",
    "title": "Categorical data analysis",
    "section": "Exercise 2 (birth data)",
    "text": "Exercise 2 (birth data)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n2a)\nDoes smoking influence the birth weight of a baby?\nMake a histogram and a box plot, and find the median and mean for birth weight in grams (bwt) for the two groups of mothers: one group that smokes, and one that does not smoke durinng the pregnancy. (Hint: use variable smk for this task)\n\n# if you are using .rda: click on the data icon\n\n# load data\nbirth &lt;- read.csv('data/birth.csv', sep = ',')\nhead(birth)\n\n  id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1  4 bwt &lt;= 2500  28 120 other    smoker   1  no yes   0   0  709\n2 10 bwt &lt;= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3 11 bwt &lt;= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4 13 bwt &lt;= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5 15 bwt &lt;= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6 16 bwt &lt;= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n\n# take the variables\nbirthweight &lt;- birth$bwt\nsmoker &lt;- birth$smk\n\n# get to know how many smoker and non-smoker\ntable(smoker)\n\nsmoker\nnonsmoker    smoker \n      115        74 \n\n# histogram for all (both categories together)\nhist(birthweight)\n\n\n\n\n\n\n\n# separate birthweight based on smoking\nbw_smoker &lt;- birthweight[smoker == 'smoker']\nbw_nonsmoker &lt;- birthweight[smoker == 'nonsmoker']\n\n# can produce summary statistics\nsummary(bw_smoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2370    2776    2773    3246    4238 \n\nsummary(bw_nonsmoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1021    2509    3100    3055    3622    4990 \n\n\nVisualise with histogram and boxplot\n\npar(mfrow = c(1, 2))\n# histogram for smokers\n# (you can try to add sample means for each group)\n# set the range for axis limits so they look comparable\nhist(bw_smoker, main = 'birthweight: smoker mothers', xlim = c(600, 5000))\nhist(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', xlim = c(600, 5000))\n\n\n\n\n\n\n\n# boxplot \nboxplot(bw_smoker, main = 'birthweight: smoker mothers', ylim = c(600, 5000))\nboxplot(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', ylim = c(600, 5000))\n\n\n\n\n\n\n\n\nFrom the histograms and boxplots, you can spot that there seems to be slightly smaller values of baby birth weight for smoker mothers; although we can not draw a concrete conclusion by looking at the graphs. Therefore, you should carry out a hypothesis test to be more certain.\n\n\n2b)\nAre the distributions for the two groups normal? Can we conclude that smoking has an effect on the weight of a newborn? Use a t-test.\n\nqqnorm(bw_smoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_smoker)\n\n\n\n\n\n\n\nqqnorm(bw_nonsmoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_nonsmoker)\n\n\n\n\n\n\n\n# two samples t-test \nt.test(bw_smoker, bw_nonsmoker, paired = F) # welch\n\n\n    Welch Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.7095, df = 170, p-value = 0.00743\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -486.95979  -76.46677\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\nt.test(bw_smoker, bw_nonsmoker, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -492.73382  -70.69274\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\n\nFrom the Q-Q plots, you can consider that the normality assumption is fulfilled.\nThe t-test (either Welch’s two sample t-test; or two sample t-test for equal variances) indicates that there is significant difference between the two groups. Birth weight of babies from smoker mothers are significantly lower than those from non-smoker mothers (p=0.009 if using the second test).\n\n\n2c)\nDo mothers of hypertension have a tendency to have babies with a lower birth weight? (The variable for hypertension is HT)\nMake a histogram of birth weight for the two groups of mothers with and without hypertension. What do you observe?\n\n# take out the variables\n# head(birth)\nhypertension &lt;- birth$ht\n\n\n# separate birthweight based on hypertension\nbw_hypertension &lt;- birthweight[hypertension == 'yes']\nbw_nohypertension &lt;- birthweight[hypertension == 'no']\n\n# can produce summary statistics\nsummary(bw_hypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1135    1775    2495    2537    3232    3790 \n\nsummary(bw_nohypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2424    2992    2972    3475    4990 \n\npar(mfrow = c(1, 2))\n# can set an x axis limit so they are comparable\nhist(bw_hypertension, main = 'birthweight: hypertension', xlim = c(600, 5000))\nhist(bw_nohypertension, main = 'birthweight: no hypertension', xlim = c(600, 5000))\n\n\n\n\n\n\n\n# boxplot \nboxplot(bw_hypertension, main = 'birthweight: hypertension', ylim = c(600, 5000))\nboxplot(bw_nohypertension, main = 'birthweight: no hypertension', ylim = c(600, 5000))\n\n\n\n\n\n\n\n\nYou can read (from the y-axis of histograms) that baby birth weight for hypertension mother group have very few observations compared to those with no hypertension mothers. This can not be spotted in the boxplot.\nThis suggests that when you carry out exploratory analysis, it’s always useful to do it in different ways.\n\n# check normality\nqqnorm(bw_hypertension, main = 'birthweight: hypertension')\nqqline(bw_hypertension)\n\n\n\n\n\n\n\nqqnorm(bw_nohypertension, main = 'birthweight: no hypertension')\nqqline(bw_nohypertension)\n\n\n\n\n\n\n\n# can check the standard deviation to see if you need welch or equal variance t-test\nc(sd(bw_hypertension), sd(bw_nohypertension))\n\n[1] 917.3405 709.2265\n\n# two samples t-test \n# t.test(bw_hypertension, bw_nohypertension, paired = F) # welch\nt.test(bw_hypertension, bw_nohypertension, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_hypertension and bw_nohypertension\nt = -2.0192, df = 187, p-value = 0.04489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -861.09734  -10.02413\nsample estimates:\nmean of x mean of y \n 2536.750  2972.311 \n\n\nThe data in hypertension group (even though scarce) fall roughly on the straight line in the QQ plot. The data on the no hyper tension group (much more data) also fall on the straight line; this suggests that the normal assumption can be assumed to be fulfilled.\nWe have checked whether the variances between the two groups are very different (rule of thumb: twice standard deviation). We can see that they do not differ by that much; so we can use the t-test with equal variance.\n\n\n2d)\nAnalyse the effect of smoking on birth weight in a table (categorical analysis): compute the relative risk (risk ratio) and carry out a test to assess the strength of association.\nAlso do it for hypertension (instead of smoking).\nWhat can you conclude?\n\n# smoking on birth weight\n# cross tabulation\nlow_bw &lt;- birth$low\ntable(low_bw)\n\nlow_bw\nbwt &lt;= 2500  bwt &gt; 2500 \n         59         130 \n\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_smk &lt;- table(smoker, low_bw)\ntb_bw_smk\n\n           low_bw\nsmoker      bwt &lt;= 2500 bwt &gt; 2500\n  nonsmoker          29         86\n  smoker             30         44\n\n# compute your risk ratio!\n\nchisq.test(tb_bw_smk)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_smk\nX-squared = 4.2359, df = 1, p-value = 0.03958\n\n\nExposure variable is smoker, and outcome is low birth weight of baby.\nRisk in exposed group (smoker): 30/(30+44) = 0.405\nRisk in unexposed group (nonsmoker): 29/(29+86) = 0.252\nRelative risk (risk ratio): 0.405/0.252 = 1.61\nWith the chi-square test which has a p value of 0.03, you can say that there is evidence that smoking during preganancy affects the risk of low birth weight. (We have not computed the confidence interval here, but you should report them. Use an R package)\n\n(Optional)\nThe situation for hypertension is slightly tricker.\n\n# hypertension\ntable(hypertension)\n\nhypertension\n no yes \n177  12 \n\n# cross tabulation\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_ht &lt;- table(hypertension, low_bw)\ntb_bw_ht\n\n            low_bw\nhypertension bwt &lt;= 2500 bwt &gt; 2500\n         no           52        125\n         yes           7          5\n\n# compute your risk ratio!\n\nExposure variable is hypertension, and outcome is low birth weight of baby.\nRisk in exposed group (hypertension): 7/(7+5) = 0.583\nRisk in unexposed group (no hypertension): 52/(52+125) = 0.293\nRelative risk (risk ratio): 0.405/0.252 = 1.99\n\n# carry out a chi-square test\nchisq.test(tb_bw_ht)\n\nWarning in chisq.test(tb_bw_ht): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_ht\nX-squared = 3.1431, df = 1, p-value = 0.07625\n\n# can specify the computation: \nchisq.test(tb_bw_ht, correct = F) # remove the continuity correction\n\nWarning in chisq.test(tb_bw_ht, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = 1, p-value = 0.03619\n\nchisq.test(tb_bw_ht, simulate.p.value = T) # simulate p values\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = NA, p-value = 0.05047\n\n\nYou can see that the conclusion for this chi-square test is problematic, as the data is very imbalanced. The p-value is around 0.05, so you are not able to reject the null hypothesis (of no association) with full confidence. You should report your confidence interval along with the point estimate, and also report your p-value even if it is not significantly small.\nYou should also double check with results computed in R packages that have implemented relative risk."
  }
]