[
  {
    "objectID": "course_material/course_material_overview.html",
    "href": "course_material/course_material_overview.html",
    "title": "Course material",
    "section": "",
    "text": "On this page you’ll find a list of material used in this course.\nFor the exercises and lab notes we use during the lab sessions, please check the R Lab and Code."
  },
  {
    "objectID": "course_material/course_material_overview.html#week-1",
    "href": "course_material/course_material_overview.html#week-1",
    "title": "Course material",
    "section": "Week 1",
    "text": "Week 1\n\n\n\nTime\nTopic\nLecture notes\nLab\nOther\n\n\n\n\nApril 24 PM\nCourse introduction\nSlides\n\n\n\n\n\nDescriptive statistics\nSlides, K&S chapter 2-4, Aalen chapter 1-2\n\nPaper 1, Paper 2, Paper 3\n\n\nApril 25 AM\nIntroduction to R and Rstudio\nSlides\nIntro to RStudio, Intro to R\n\n\n\n\nLab session\nSlides\nDescriptive statistics (EDA I)\n\n\n\nApril 25 PM\nProbability, diagnosistic tests\nIntro slides Probability Diagnostic tests\n\n\n\n\n\nStatistical distributions\nDistributions\n\n\n\n\nApril 26 AM\nLab session\n\nCOVID-19 tests, Simulations\n\n\n\nApril 26 PM\nStatistical inference, confidence intervals, t-test\nOverview, Sample mean, Confidence interval, Hypothesis testing\n\n\n\n\nApril 27 AM\nLab session\nSlides\nt-test\n\n\n\nApril 27 PM\nCategorical data, proportions, table analysis\nOverview, Proportions, Compare proportions, Table analysis\n\n\n\n\nApril 28 AM\nLab session\nSlides\nCategorical data analysis"
  },
  {
    "objectID": "course_material/course_material_overview.html#week-2",
    "href": "course_material/course_material_overview.html#week-2",
    "title": "Course material",
    "section": "Week 2",
    "text": "Week 2\n\n\n\n\n\n\n\n\n\n\nTime\nTopic\nLecture notes\nLab\nOther\n\n\n\n\nMay 8 AM\nExploratory data analysis, transformation\n\n\n\n\n\n\nNon-parametric methods\n\n\n\n\n\nMay 8 PM\nSample size, statistical power\n\n\n\n\n\nMay 9 AM\nStudy design, principles of clinical trials\n\n\n\n\n\nMay 9 PM\nLinear regression I\n\n\n\n\n\nMay 10 AM\nLinear regression II\n\n\n\n\n\nMay 10 PM\nSurvival analysis"
  },
  {
    "objectID": "lab/lab_eda_part1.html",
    "href": "lab/lab_eda_part1.html",
    "title": "Exploratory data analysis (Part I)",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-1-weight",
    "href": "lab/lab_eda_part1.html#exercise-1-weight",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 1 (weight)",
    "text": "Exercise 1 (weight)\n\n1a\nGenerate a variable named weight, with the following measurements:\n50 75 70 74 95 83 65 94 66 65 65 75 84 55 73 68 72 67 53 65\n\nweight <- c(50, 75, 70, 74, 95, \n            83, 65, 94, 66, 65, \n            65, 75, 84, 55, 73, \n            68, 72, 67, 53, 65)\n\n\n\n1b\nMake a simple descriptive analysis of the variable, what are the mean, median, maximum, minimum and quantiles?\nHow to interpret the data?\n\nmean(weight)\n\n[1] 70.7\n\nmedian(weight)\n\n[1] 69\n\nmax(weight)\n\n[1] 95\n\nmin(weight)\n\n[1] 50\n\n# alternatively, \nsummary(weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0    65.0    69.0    70.7    75.0    95.0 \n\n\n\n\n1c\nMake a histogram.\n\nhist(weight)\n\n\n\n\n\n\n1d\nMake a boxplot. What do the two dots on the top represent?\n\nboxplot(weight)\n\n\n\n\nThey are the largest two points in the dataset. These are outliers in this data.\n\n\n\n\n\n\n(Optional) Outliers in box plot\n\n\n\n\n\nThere are different ways to define outliers. The default box plot in R determines points beyond \\(Q_1 - 1.5\\times IQR\\) and \\(Q_3 + 1.5\\times IQR\\) are outliers. You can check the values of quartiles using summary(), and read up the documentation ?boxplot()."
  },
  {
    "objectID": "lab/lab_eda_part1.html#exercise-2-lung-function",
    "href": "lab/lab_eda_part1.html#exercise-2-lung-function",
    "title": "Exploratory data analysis (Part I)",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nDownload and open PEFH98-english.dta into R.\nIf you have problem with .dta data format, you can also use PEFH98-english.csv.\nPay attention to how gender is coded. We might have to modify it.\n\n# locate your datafile, set the path to your data\n# if you use rda data file: \n# load('./lab/data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data <- read.csv('data/PEFH98-english.csv', sep = ',')\nhead(lung_data)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n4  21   male    179     74     520     530     540     480     510     500\n5  20   male    196     95     740     750     750     700     710     700\n6  20   male    189     83     600     575     600     600     600     640\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n4 530.0000 496.6667 513.3333\n5 746.6667 703.3333 725.0000\n6 591.6667 613.3333 602.5000\n\n\n\n\n2b)\nHow many observations are there (number of subjects)? How do you get a list of variable names from your dataset?\n\nnrow(lung_data)\n\n[1] 106\n\ncolnames(lung_data)\n\n [1] \"age\"     \"gender\"  \"height\"  \"weight\"  \"pefsit1\" \"pefsit2\" \"pefsit3\"\n [8] \"pefsta1\" \"pefsta2\" \"pefsta3\" \"pefsitm\" \"pefstam\" \"pefmean\"\n\n\nMake a histogram for each of the following variables. Compute means, and interpret the results.\nheight\nweight\nage\npefsitm\npefstam\n\nhist(lung_data$height)\n\n\n\n\nWe repeat it for the other 4 variables. We can put them more compactly,\n\npar(mfrow = c(2, 2)) \n# we use this line to display (2 rows 2 columns)\n# by default it is 1 row 1 column\n# run this line to set it back to default:\n# par(mfrow = c(1, 1))\nhist(lung_data$weight)\nhist(lung_data$age)\nhist(lung_data$pefsitm)\nhist(lung_data$pefstam)\n\n\n\n\n\n\n2c)\nMake histograms for the variables height and pefmean for men and women separately. Also try to make boxplots.\nWhat conclusion can you draw?\n\nheight_f <- lung_data$height[lung_data$gender == 'female']\nheight_m <- lung_data$height[lung_data$gender == 'male']\n\npar(mfrow = c(1,2)) # plot in parallel\nhist(height_f)\nhist(height_m)\n\n\n\n# we can make it more customized\n# add axis limit, title and xaxis name\npar(mfrow = c(1,2)) # plot in parallel\nhist(height_f, main = 'Height: female', xlab = 'Height (cm)',\n     xlim = c(150, 200))\nhist(height_m, main = 'Height: male', xlab = 'Height (cm)',\n     xlim = c(150, 200))\n\n\n\n\nSimilarly, histogram for pefmean can be done in the same way.\n\npefmean_f <- lung_data$pefmean[lung_data$gender == 'female']\npefmean_m <- lung_data$pefmean[lung_data$gender == 'male']\n\npar(mfrow = c(1,2)) # plot in parallel\nhist(pefmean_f)\nhist(pefmean_m)\n\n\n\n\nNow we can make some boxplots\n\npar(mfrow = c(1, 2))\nboxplot(height ~ gender, data = lung_data, main = 'Height vs Gender')\n\n# it is also possible to remove the frame\nboxplot(pefmean ~ gender, data = lung_data, frame = F, main = 'PEFmean vs gender')\n\n\n\n\n\n\n2d)\nMake three scatterplots to compare\n\npefmean with height\npefmean with weight\npefmean with age\n\nWhat association do you see?\n\n# pefmean height\nplot(lung_data$pefmean, lung_data$height)\n\n\n\n# it is possible to customize \nplot(lung_data$pefmean, lung_data$height, \n     main = 'PEF mean vs height', \n     xlab = 'PEF mean', ylab = 'Height',\n     pch = 20)\n\n\n\n# pch: plotting symbols\n\npch = 20 is setting the symbol to small solid dots. You can try different values, from 0 to 25. Read more\n\npar(mfrow = c(1, 2))\n# pefmean weight\nplot(lung_data$pefmean, lung_data$weight, \n     main = 'PEF mean vs weight', \n     xlab = 'PEF mean', ylab = 'Weight',\n     pch = 20)\n\n# pefmean age\nplot(lung_data$pefmean, lung_data$age, \n     main = 'PEF mean vs age', \n     xlab = 'PEF mean', ylab = 'Age',\n     pch = 20)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the course",
    "section": "",
    "text": "The aim of the course is to make the participants acquainted with basic statistical ideas and methods. No special previous knowledge of mathematics or statistics is assumed. The statistical software R and the RStudio environment will be used in many of the exercises. Analysis of examples from biomedical research will be emphasized.\n\nContact\n\nManuela Zucknick: manuela.zucknick@medisin.uio.no\nValeria Vitelli: valeria.vitelli@medisin.uio.no\nAlvaro Kohn Luque: a.k.luque@medisin.uio.no\nChi Zhang: chi.zhang@medisin.uio.no"
  },
  {
    "objectID": "lab/overview.html",
    "href": "lab/overview.html",
    "title": "R Lab and Code",
    "section": "",
    "text": "Welcome! Here you will find material for R lab and code for this course. Lecture notes, videos and other resources can be found in Course material page."
  },
  {
    "objectID": "lab/overview.html#useful-resources",
    "href": "lab/overview.html#useful-resources",
    "title": "R Lab and Code",
    "section": "Useful resources",
    "text": "Useful resources\nList of commands that are useful for this course: list of commands\nBook (Wickham et al) R for Data Science (2e)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MF9130E - Introductory course in Statistics",
    "section": "",
    "text": "Welcome!\nYou are on the course website for MF9130E - Introductory Course in Statistics.\n\nThe course is intended for students and researchers who are interested in statistics and R programming, with applications in medical and healthcare data. No previous programming experience is required to participate in this course.\nThis website is developed by the instructors of the course, hosted for free and public access on Github. The course github repository can be accessed here.\nYou can check the course page by UiO for information related to applications, evaluations and other administrative matters.\nAbout home exam: information regarding home exam will be talked about in the class on the first and last days. Assignment tasks for home exam will be put in Canvas, and you should submit your assignment in Inspera. More information about the exam\n\n\nPreparation\nYou should have a working solution of R (either installed on your own laptop, or using Posit cloud) before the course.\nIt would also be helpful if you familiarize yourself with the course website.\n\nGet Started provides some information about software installation, where to download data and code, and some resources.\nCourse material provides an overview of the material and corresponding links for each session.\nR Lab and Code hosts the lab session notes.\n\n\n\n\nSchedule\nYou can find the official course schedule provided by University of Oslo here. If there is an error in the time and place on this page, please refer to the official schedule.\n\nWeek 1\n\n\n\n\n\n\n\n\n\nDate\nTime\nTopic\nPlace\n\n\n\n\nApr 24\n12:45-16:00\nLecture: course introduction, data and descriptive statistics\nDME, Lille auditorium\n\n\nApr 25\n08:30-11:45\nGuided lab session: introduction to R, descriptive statistics with R\nDME, Auditorium 13\n\n\nApr 25\n12:45-16:00\nLecture: probability, Bayes law, diagnostic tests, distributions\nDME, Auditorium 13\n\n\nApr 26\n08:30-11:45\nGuided lab session\nDME, Store auditorium\n\n\nApr 26\n12:45-16:00\nLecture: statistical inference, hypothesis testing, confidence intervals, t-tests\nDME, Store auditorium\n\n\nApr 27\n08:30-11:45\nGuided lab session\nDME, Runde auditorium R105\n\n\nApr 27\n12:45-16:00\nLecture: categorical data analysis\nDME, Runde auditorium R105\n\n\nApr 28\n08:30-11:45\nGuided lab session\nDME, Store Auditorium\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\n\n\nDate\nTime\nTopic\nPlace\n\n\n\n\nMay 8\n08:30-11:45\nLecture and lab: exploratory data analysis, transformation, non-parametric methods\nDME, Lille auditorium\n\n\nMay 8\n12:45-16:00\nLecture and lab: sample size, statistical power\nDME, Lille auditorium\n\n\nMay 9\n08:30-11:45\nLecture: study designs, principle of clinical trials\nHelga Engs hus Auditorium 3\n\n\nMay 9\n12:45-16:00\nLecture and lab: regression I\nHelga Engs hus Auditorium 3\n\n\nMay 10\n08:30-11:45\nLecture: regression II\nDME, Auditorium 13\n\n\nMay 10\n12:45-16:00\nLecture and lab: regression II\nDME, Auditorium 13\n\n\nMay 11\n08:30-11:45\nLecture and lab: regression III\nDME, Auditorium 13\n\n\nMay 11\n12:45-16:00\nLecture and lab: survival analysis, course summary\nDME, Auditorium 13"
  },
  {
    "objectID": "course_material/notes_probability.html",
    "href": "course_material/notes_probability.html",
    "title": "Probability",
    "section": "",
    "text": "Topics:\nBooks and resources:"
  },
  {
    "objectID": "course_material/notes_probability.html#basic-concepts",
    "href": "course_material/notes_probability.html#basic-concepts",
    "title": "Probability",
    "section": "Basic concepts",
    "text": "Basic concepts\nA probability expresses a potential for something to happen.\nIt is an assessment of uncertainty in a situation or event.\nIt corresponds to the concept of risk in medicine.\n\nBrief history\nBlaise Pascal (17th century) was the founder of probability theory, the set of basic rules for doing probability calculations. His work was motivated by dice and card games.\nAndrey Kolmogorov formulated the exact probability rules as late as 1933.\n\n\nTwo definitions of probability\nFrequentist definition: Proportions of times (or frequency) that some event occurs in a large number of similar repeated trials.\nBayesian definition: Degree of belief in the occurrence of an event.\n\n\n\n\n\n\nObservation\n\n\n\n\n\nIn this course we focus in the frequentist definition, the most widely used. However, we will see at the end of this section that the Bayesian definition is important as it constitutes the foundation of the Bayesian approach to statistics, after Thomas Bayes (18th century).\n\n\n\n\n\nLaw of Large Numbers\n“As an experiment is repeated over and over, the observed frequency approaches the true probability”.\n\n\n\n\n\n\nExample: Coin tossing\n\n\n\n\n\nThis figure shows the frequency of heads in up to 200 simulated coin tosses. We can see that the frequency approaches the value 0.5 as the number of tosses grows.\n\n\n\n\nThe frequentist view of probability interprets the frequency of an event in a large number of experiments as its probability.\n\n\n\n\n\n\nExample: Births in Norway\n\n\n\n\n\nGiving birth to a girl or a boy is perceived as a random event. This table contains the frequencies of occurrence of the event “giving birth to a girl” in Norway during 2019-2022\n\nSource: Statistisk sentralbyrå\n\n\n\n\n\n\n\n\n\nYear\nTotal number of births\nNumber of boys\nNumber of girls\nPercentage of girls\n\n\n\n\n2022\n51480\n26445\n25035\n48.6\n\n\n2021\n56060\n28684\n27376\n48.8\n\n\n2020\n52979\n27063\n25916\n48.9\n\n\n2019\n54495\n28042\n26453\n48.5\n\n\n\nThe probability of giving birth to a girl in Norway is approximately 0.49"
  },
  {
    "objectID": "course_material/notes_probability.html#probability-calculations",
    "href": "course_material/notes_probability.html#probability-calculations",
    "title": "Probability",
    "section": "Probability calculations",
    "text": "Probability calculations\n\nStochastic trial, events and sample space.\nA stochastic trial is characterized by an uncertain outcome.\nAll possible outcomes in a stochastic trial make up the sample space.\nAn event can be a single outcome, or a collection of single outcomes.\nEach event has a probability of ocurrence between 0 and 1. A probability equal to 0 means that the event can never occur, and equal to 1 means that the event is certeinly occuring.\nThe sum of all probabilities in a sample space equals 1.\n\n\n\n\n\n\nExamples: Stochastic trials\n\n\n\n\n\nDice tossing\n\nSample space: {1,2,3,4,5,6}\nEvents:\n\nEven number of eyes: {2,4.6}\nMore than 3 eyes: {4,5,6}\n\n\nChild birth\n\nSample space: {B,G}\nEvents:\n\nHaving a girl: {G}\nNot having a girl: {B}\n\n\nDiastolic blood pressure\n\nSample space: {40,41,…,119,120}\nEvent:\n\nHypertension: {91,92, …, 120}\n\n\n\n\n\n\n\nVenn diagram\nVenn diagrams are often used to illustrate events. In the figures below A and B represent different events and S is the sample space.\n\n\n\n\n\n\n\n\n\n\n\n\nOperators on events:\n\nUnion: \\(A \\cup B\\)\nIntersection: \\(A \\cap B\\)\nComplement: \\(\\bar{A}\\)\n\n\n\n\n\n\n\n\n Union: \\(A \\cup B\\)\n Intersection: \\(A \\cap B\\)\n\n\n\n\n\n\n\n\n\n\n Complement: \\(\\bar{A}\\)\n Combining operators: \\(A \\cap \\bar{B}\\)\n\n\n\n\n\nProbability calculation rules\nThe probability of an event \\(A\\) is denoted by \\(P(A)\\). It has a value between 0 and 1. The probability over the whole sample space equals 1.\nComplement rule\n\\[P(A) + P(\\bar{A}) = 1\\]\nAdditive rule\nThe occurrence of at least one of the events \\(A\\) or \\(B\\) is \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] For disjoint events \\(A\\) and \\(B\\), \\(P(A \\cap B) = 0\\). Hence \\[P(A \\cup B) = P(A) + P(B)\\]\nMultiplicative rule\nProbability of independent events \\(A\\) and \\(B\\) can be multiplied\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\n\n\n\n\nExample: Child gender in two births\n\n\n\n\n\nWhat is the probability of giving birth to at least one girl?\n\nWe assume the probability of giving birth to a girl is 0.5.\nWe assume that the two births are independent events.\n\n\\[\n\\begin{aligned}\nP(\\text{at least one girl}) & =P(\\text{1st child is girl}) + P(\\text{2nd child is girl})-P(\\text{both are girls}) \\\\\n& = 1/2 + 1/2 - 1/2 \\times 1/2 = 3/4\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nExample: Tossing two dices\n\n\n\n\n\n\nThe two tosses are independent events.\nThere are 36 equally possible outcomes.\nOnly one outcome corresponds to the double 6 event.\n\nWhat is the probability of getting 6 in the first AND the second throw? \\[\nP(\\text{6 in the first AND the second throw}) = 1/6 \\times 1/6 = 1/36\n\\]\nWhat is the probability of getting at least one 6 in two throws? \\[\n\\begin{aligned}\n    P(\\text{at least one 6 in 2 trows}) &=P(\\text{6 in the first}) +         P(\\text{6 in the second}) - P(\\text{6 in both})\\\\\n    &=1/6 +1/6 - 1/36 =0.31\n\\end{aligned}\n\\]\nWhat is the probability of not getting any 6 in two throws? \\[\n\\begin{aligned}\n    P(\\text{not getting any 6 in 2 trows}) &= 1 - P(\\text{at least one 6 in 2 trows})\\\\\n    &= 1 - 0.31 = 0.69\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#conditional-probability",
    "href": "course_material/notes_probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhat is the probability of getting the outcome \\(A\\) given that the event \\(B\\) has occur? For example, what is the risk of becoming sick from COVID-19 given that your spouse already is?\nThe idea to define such a conditional probability of \\(A\\) given \\(B\\), denoted \\(P(A|B)\\), is to consider \\(B\\) as the new sample space and rescale the probability of events in \\(B\\), such that the new sample space has probability 1:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\n\n\nExercise: Weather forecast\n\n\n\n\n\nThis table contains the frequency of joint events: weather forecast and actual weather.\n\n\n\n\nsunny forecast\ncloudy forecast\nrainy forecast\n\n\nsunny weather\n0.3\n0.05\n0.05\n\n\ncloudy weather\n0.04\n0.2\n0.02\n\n\nrainy weather\n0.1\n0.04\n0.2\n\n\n\n\nWhich is the probability of a sunny day?\nWhich is the probability that the forecast is wrong?\nWhich is the probability of rain when the forecast is sunny?\n\nHint: remember to use the special case of the additive rule, holding when A and B are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\nStochastic independence\nThe events \\(A\\) and \\(B\\) are independent if \\(P(A|B) = P(A)\\)\nInterpretation: probability of \\(A\\) is the same if we also know that \\(B\\) has occurred.\n\n\n\n\n\n\nExamples: independence\n\n\n\n\n\nConsider the following:\n\n\\(A\\): Probability of me having the condition\n\\(B\\): Probability of my partner having the same condition\n\nCondition 1: Diabetes, \\(P(A|B) = P(A)\\)\nCondition 2: COVID-19, \\(P(A|B) \\neq P(A)\\)\n\n\n\nProbability calculations can be simplified if there is stochastic independence:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = P(A)\\]\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\n\n\n\n\nExample: Child birth (revisited)\n\n\n\n\n\nTwo children are born, which is the probability that genders are different?\n\\[\\begin{aligned}\nP(\\text{girl and boy}) & =P(\\text{girl then boy}) + P(\\text{boy then girl})\\\\\n& = P(\\text{first is girl}) \\times P(\\text{second is boy}) +\\\\\n&~~~~+ P(\\text{first is boy}) \\times P(\\text{second is girl})\\\\\n& = 1/2 \\times 1/2 + 1^/2 \\times 1/2 = 0.50\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nExample: Dices (revisited)\n\n\n\n\n\nThree dices are tossed, which is the probability of all showing sixes?\n\\[P(\\text{three 6})=1/6 + 1/6 + 1/6 = 1/216 = 0.004\\]\n\n\n\n\n\n\n\n\n\nExercise: Side effects of medication\n\n\n\n\n\nA treatment has a side effect which has a risk of 1/1000. Which is the probability that the side effect does not occur among 1000 patients?\nHint: think about independence."
  },
  {
    "objectID": "course_material/notes_probability.html#total-probability",
    "href": "course_material/notes_probability.html#total-probability",
    "title": "Probability",
    "section": "Total probability",
    "text": "Total probability\nThe law of total probability expresses the probability of an outcome, \\(P(A)\\), which can be realized via two distinct events \\(B\\) and \\(\\bar{B}\\):\n\\[P(A) = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\\]\n\n\n\n\n\n\nDerivation of the law of total probability\n\n\n\n\n\nAny event \\(A\\) can be divided in two with regard to another event \\(B\\):\n\\[A = (A \\cap B) \\cup (A \\cap \\bar{B})\\]\n\nBecase the two events \\((A \\cap B)\\) and \\((A \\cap \\bar{B})\\) are disjunct, we can write:\n\\[\nP(A) = P(A \\cap B) + P(A \\cap \\bar{B})\n\\] Using the multiplicative rule, we get the law of total probability:\n\\[\nP(A) = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\n\\]\n\n\n\n\n\n\n\n\n\nExample: Gender of twins\n\n\n\n\n\n\nWe want to find the probability of two twins having the same gender.\nMonozygotic twins have the same gender, while dizygotic twins are like any other siblings.\nWe have to take into consideration if the twins are monozygotic or not. For that we use the law of total probability.\n\n\\(A\\) = Both twins have the same gender.\n\\(B\\) = The twins are monozygotic.\n\nWe want to find \\(P(A)\\)\nWe assume that the probability of twins being monozygotic, \\(P(B)\\), is 1/3.\nThe law of total probability give us:\n\n\\[\n\\begin{aligned}\nP(A) & = P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})\\\\\n& = 1 \\cdot 1/3 + 1/2 \\cdot 2/3 = 0.67\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#bayes-theorem-also-called-bayes-law",
    "href": "course_material/notes_probability.html#bayes-theorem-also-called-bayes-law",
    "title": "Probability",
    "section": "Bayes’ theorem (also called Bayes’ law)",
    "text": "Bayes’ theorem (also called Bayes’ law)\nGiven two events \\(A\\) and \\(B\\), Bayes theorem states that:\n\\[\nP(B|A)=\\frac{P(A|B)P(B)}{P(A)}\n\\]\n\n\n\n\n\n\nExplain\n\n\n\n\n\nIt was first formulated by Thomas Bayes (1702-1761)\nBayes law relates the conditional probabilities \\(P(B|A)\\) and \\(P(A|B)\\)\n\n\n\n\n\n\n\n\n\nExample Gender of twins (revisited)\n\n\n\n\n\nWhat if we want to find the probability that two twins of the same gender are monozygotic?\nIn other words, what is \\(P(B|A)\\)?\nWe can use Bayes’ law and the law of total probability\n\\[\n\\begin{aligned}P(B|A) & =\\frac{P(A|B)P(B)}{P(A)}\\\\       & =\\frac{P(A|B)P(B)}{P(A|B)P(B) + P(A| \\bar{B})P( \\bar{B})}\\\\       & = \\frac{1 \\cdot 1/3}{1 \\cdot 1/3 + 1/2 \\cdot 2/3} = 0.5\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "course_material/notes_probability.html#bayesian-statistics",
    "href": "course_material/notes_probability.html#bayesian-statistics",
    "title": "Probability",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nIn the Bayesian definition of probability, the size of the probability of a given event represents ones degree of belief in the occurrence of the even.\nWhere does Bayes come in? Bayes’ law is used to calculate such probabilities base on on our prior belief and available data:\n\\[\nP(\\theta|data) =\\frac{P(data|\\theta)P(\\theta)}{P(data)}\n\\]\n\n\\(\\theta\\) refers to the parameters in your model (mean, variance, etc.)\nThe prior distribution \\(P(\\theta)\\) is where you put in your prior beliefs\nWhat you want to estimte is the so called posterior distribution \\(P(\\theta|data)\\), the probability distribution of the model parameters given your data.\nThe more data you have, the more will it dominate over your prior belief.\nWhen youy have prior knowledge about your problem, you get to actually use this information\nWhen you know little (or nothing) of your problme, there are anyway many methodological advantages in using Bayesian statistics\nBayesian statistics is not really relevant for simpler problems like in this course, but you will probaly at some point come across articles using Bayesian approaches."
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html",
    "href": "course_material/notes_diagnostic_tests.html",
    "title": "Evaluation of Diagnostic Tests",
    "section": "",
    "text": "Topics:\nBook and resources:"
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html#confusion-matrix",
    "href": "course_material/notes_diagnostic_tests.html#confusion-matrix",
    "title": "Evaluation of Diagnostic Tests",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nDenote the subjects with the condition as \\(P\\), and subjects without the condition as \\(N\\).\nTotal population: \\(P+N\\)\nPrevalence (of the condition) is defined as: \\(\\frac{P}{P+N}\\)\n\n\n\n\n\n\n\n\n\n\nPredicted Positive\nPredicted Negative\nTotal\n\n\n\n\nWith condition\nTrue Positive TP\nFalse Negative FN\n\\(P = TP + FN\\)\n\n\nWithout condition\nFalse Positive FP\nTrue Negative TN\n\\(N=FP + TN\\)\n\n\nTotal\n\\(P_{\\text{predicted}}=TP+FP\\)\n\\(N_{\\text{predicted}}=FN + TN\\)\n\n\n\n\nSensitivity of a diagnostic test is the probability of revealing that a person has the condition. It is also known as true positive rate (TPR), as it is the proportion of true positives.\n\\[\\text{Sensitivity}= \\frac{TP}{P} = \\frac{TP}{TP+FN}\\]\nSpecificity is the probability of revealing that a person does not have the condition (i.e. healthy). It is also known as true negative rate (TNR), as it is the proportion of true negatives.\n\\[\\text{Specificity} = \\frac{TN}{N} = \\frac{TN}{TN + FP}\\]\nPositive predictive value (PPV): probability that the person with the condition was given a positive test result\n\\[PPV = \\frac{TP}{P_{predicted}} = \\frac{TP}{TP + FP}\\]\nNegative predictive value (NPV): probability that the person without the condition (i.e. healthy) was give a negative test result.\n\\[NPV = \\frac{TN}{N_{predicted}} = \\frac{TN}{TN + FN}\\]\n\n\n\n\n\n\nExample: Mammography\n\n\n\n\n\n(From the Norwegian Medical Journal, 1990) 372 women with a lump in the breast initially classified as malign or benign were referred to a surgical clinic for a final diagnosis.\n\n\n\n\nMammography malign\nMammography benign\n\n\n\n\nFinal diagnosis malign\n22\n3\n\n\nFinal diagnosis benign\n16\n331\n\n\n\nWe identify the positive test result (mammography malign), and the positive condition (final diagnosis malign). Then we can compute the four following probabilities from the table:\nSensitivity: \\(22/(22+3) = 0.88\\)\nSpecificity: \\(331/(331+16) = 0.95\\)\nPositive predictive value: \\(22/(22+16) = 0.58\\)\nNegative predictive value: \\(331/(331+3) = 0.99\\)"
  },
  {
    "objectID": "course_material/notes_diagnostic_tests.html#diagnostic-tests-and-prevalence",
    "href": "course_material/notes_diagnostic_tests.html#diagnostic-tests-and-prevalence",
    "title": "Evaluation of Diagnostic Tests",
    "section": "Diagnostic tests and prevalence",
    "text": "Diagnostic tests and prevalence\nThe concepts in diagnostic testing can be formulated in the form of conditional probabilities:\n\nSensitivity: \\(P(\\text{pos}|\\text{ill})\\)\nSpecificity: \\(P(\\text{neg}|\\text{healthy})\\)\nPositive predictive value: \\(P(\\text{ill}|\\text{pos})\\)\nNegative predictive value: \\(P(\\text{healthy}|\\text{neg})\\)\n\nBayes’ theorem can be applied to compute PPV and NPV from sensitivity, specificity and prevalence:\n\\[PPV = \\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})}\\]\n\\[NPV = \\frac{\\text{spec} \\cdot (1-\\text{prev}) }{(1-\\text{sens}) \\cdot \\text{prev} + \\text{spec} \\cdot (1-\\text{prev})}\\]\n\n\n\n\n\n\nDerivation of the PPV formula using Bayes’ law\n\n\n\n\n\n\\[\n\\begin{aligned}PPV = P(\\text{ill}|\\text{pos}) & = \\frac{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill})}{P(\\text{pos})}\\\\                               & = \\frac{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill})}{P(\\text{pos}|\\text{ill}) \\cdot P(\\text{ill}) +  P(\\text{pos}|\\text{healthy}) \\cdot P(\\text{healthy})}\\\\ & =\\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})}\\end{aligned}\n\\]\nExercise: Try to derive the formula for \\(NPV\\) in a similar way.\n\n\n\n\n\n\n\n\n\nExample: HIV testing\n\n\n\n\n\nIn a test for the HIV virus, the result can be:\n\nPositive: the test shows antibodies.\nNegative: the test does not show antibodies.\n\nBut the test result may be wrong:\n\nA false positive might come from antibodies from related virus, but not HIV.\nA false negative might be due to the fact that antibodies are not yet produced in sufficient quantity, hence are not detected by the test.\n\nWe assume that the sensitivity of the HIV test is 98%. We know that the specificity of the HIV test is 99.8%. We assume that the prevalence of HIV in a given population is 0.1%.\nWhat is the probability of a person having HIV, if he got a positive test result?\n\\(PPV = \\frac{\\text{sens} \\cdot \\text{prev}}{\\text{sens} \\cdot \\text{prev} + (1-\\text{spec}) \\cdot (1-\\text{prev})} = \\frac{0.98 \\times 0.001}{0.98 \\times 0.001 + 0.002 \\times 0.999} = 0.329\\)\nWhat is the probability of a person not having HIV, if he got a negative test result?\n\\(NPV = \\frac{\\text{spec} \\cdot (1-\\text{prev}) }{(1-\\text{sens}) \\cdot \\text{prev} + \\text{spec} \\cdot (1-\\text{prev})} = \\frac{0.998 \\times 0.999}{0.98 \\times 0.001 + 0.998 \\times 0.999} = 0.999\\)\nLet us see from 100 000 persons, what are the theoretical results of the test?\n\nNumber of HIV infected (positive condition): \\(100000 \\times 0.001 = 100\\)\nTrue positives: \\(100 \\times 0.98 = 98\\)\nFalse negatives: 2\nNumber of not HIV infected (negative condition): \\(100000-100 = 99900\\)\nTrue negatives: \\(99900 \\times 0.998 = 99700\\)\nFalse positives: \\(200\\)\n\nNote that from 298 positive tests, only 98 persons have HIV. How would these numbers change if the prevalence would be lower? and if it would be higher?"
  },
  {
    "objectID": "lab/lab_survival.html",
    "href": "lab/lab_survival.html",
    "title": "Survival analysis",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_ttest.html",
    "href": "lab/lab_ttest.html",
    "title": "t-test",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-1-heart-data",
    "href": "lab/lab_ttest.html#exercise-1-heart-data",
    "title": "t-test",
    "section": "Exercise 1 (heart data)",
    "text": "Exercise 1 (heart data)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n# enter the data\nheart <- c(11.5, 14.75, 13.75, 10.5, 14.75,\n           13.5, 10.75, 9.5, 11.75, 12, \n           10.5, 11.75, 10, 14.5, 12, \n           11, 14, 15, 11.5, 10.25)\n\nYou can either compute the sample mean by summing each data point, and divide by sample size; or use R command mean().\n\n# compute the mean\nsum_heart <- 11.5 + 14.75 + 13.75 + 10.5 + 14.75 + \n  13.5 + 10.75 + 9.5 + 11.75 + 12 + \n  10.5 + 11.75 + 10 + 14.5 + 12 + \n  11 + 14 + 15 + 11.5 + 10.25\n\n# this is the sum \nsum_heart\n\n[1] 243.25\n\n# sample size: 20\nn <- 20\n# if we do not know the size, can find out with length(heart)\nsum_heart/n\n\n[1] 12.1625\n\n# formula: sum(heart)/length(heart)\nmean(heart)  # should be the same as above\n\n[1] 12.1625\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn <- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n[1] 1.779405\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n[1] 1.779405\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 <- qt(p = 0.025, df = n-1) # -2.09\nt975 <- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n[1] -2.093024  2.093024\n\n# plug in the formula\nci_lower <- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper <- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n[1] 11.32971 12.99529\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 30.568, df = 19, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\npar(mfrow = c(1, 2)) # this code puts two plots side by side\n# histogram\nhist(heart, breaks = 10, main = 'Histogram of heart data')\nabline(v = mean(heart), col = 'red', lwd = 2)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\n\n# qqplot \nqqnorm(heart, pch = 20)\nqqline(heart, lwd = 2)\n\n\n\n\n\n# boxplot \npar(mfrow = c(1, 2))\n# compare with 11\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11, col = 'forestgreen', lwd = 3)\n\n\n# compare with 11.5\nboxplot(heart, horizontal = T, main = 'Compare with mean = 11.5')\nabline(v = mean(heart), col = 'red', lwd = 3)\nabline(v = c(ci_lower, ci_upper), col = 'red', lwd = 2,\n       lty = 'dashed')\nabline(v = 11.5, col = 'forestgreen', lwd = 3)\n\n\n\n\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals).\n\n# H0: mu = 11; H1: mu != 11\nt.test(heart, mu = 11, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 2.9217, df = 19, p-value = 0.008751\nalternative hypothesis: true mean is not equal to 11\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625 \n\n# H0: mu = 11.5; H1: mu != 11.5\nt.test(heart, mu = 11.5, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  heart\nt = 1.665, df = 19, p-value = 0.1123\nalternative hypothesis: true mean is not equal to 11.5\n95 percent confidence interval:\n 11.32971 12.99529\nsample estimates:\nmean of x \n  12.1625"
  },
  {
    "objectID": "lab/lab_ttest.html#exercise-2-lung-function",
    "href": "lab/lab_ttest.html#exercise-2-lung-function",
    "title": "t-test",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n# if you use rda data file: \n# load('./lab/data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data <- read.csv('data/PEFH98-english.csv', sep = ',')\nhead(lung_data)\n\n  age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n1  20 female    165     50     400     400     410     410     410     400\n2  20   male    185     75     480     460     510     520     500     480\n3  21   male    178     70     490     540     560     470     500     470\n4  21   male    179     74     520     530     540     480     510     500\n5  20   male    196     95     740     750     750     700     710     700\n6  20   male    189     83     600     575     600     600     600     640\n   pefsitm  pefstam  pefmean\n1 403.3333 406.6667 405.0000\n2 483.3333 500.0000 491.6667\n3 530.0000 480.0000 505.0000\n4 530.0000 496.6667 513.3333\n5 746.6667 703.3333 725.0000\n6 591.6667 613.3333 602.5000\n\n# we can focus on height and gender variable only\nhead(lung_data[, c('height', 'gender')], 10)\n\n   height gender\n1     165 female\n2     185   male\n3     178   male\n4     179   male\n5     196   male\n6     189   male\n7     173   male\n8     196   male\n9     173 female\n10    173 female\n\n\nNow we need to separate the height data for based on gender. First, we do it for gender == 'female'.\n\n# for convenience, we create a variable names 'gender'\ngender <- lung_data$gender\n# height for female\nheight_f <- lung_data$height[gender == 'female']\n\nYou should always check whether your newly created variable is correct: for example, you can compare the first several values of height_f with the original data, to see if it is really only selecting height for females.\nAnother useful thing to do is to check how many data poinnts have been selected.\n\n# check the first few values, is it only selecting female heights?\nhead(height_f)\n\n[1] 165 173 173 169 170 172\n\n# number of females\nnf <- length(height_f)  # 54\nnf\n\n[1] 54\n\n\nNow we can compute the mean and confidence interval on the newly created variable, height_f.\n\nmean(height_f) # 169.57\n\n[1] 169.5741\n\nsd(height_f) # 5.69\n\n[1] 5.692106\n\n# se_f <- sd(height_f)/sqrt(54) 0.774\n\n# quantile for t distribution: pay attention to df!\nt975 <- qt(p = 0.975, df = nf-1) # 2.005\n\n# 95% CI \nci_upper_f <- mean(height_f) + t975 * sd(height_f)/sqrt(nf) # 171.1277\nci_lower_f <- mean(height_f) - t975 * sd(height_f)/sqrt(nf) # 168.0204\nc(ci_lower_f, ci_upper_f)\n\n[1] 168.0204 171.1277\n\n# double check by running a t.test\nt.test(height_f)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 218.92, df = 53, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nBased on our calculation, the mean height for females is 169.57 cm (95% confidence interval (168.02, 171.13)).\nFor gender == 'male', we can do the same thing. Pay attention to the different degrees of freedom, because the sample size has changed.\n\n# height for male \nheight_m <- lung_data$height[gender == 'male']\n# number of males\nnm <- length(height_m)  # 52\n\nmean(height_m) # 181.87\n\n[1] 181.8654\n\nsd(height_m) # 5.67\n\n[1] 5.667343\n\n# se_m <- sd(height_m)/sqrt(52) # 0.786\n\n# find quantile for males (pay attention to df)\nt975 <- qt(p = 0.975, df = nm-1) # 2.007\n# 95% CI\nci_upper_m <- mean(height_m) + t975 * sd(height_m)/sqrt(nm) # 183.44\nci_lower_m <- mean(height_m) - t975 * sd(height_m)/sqrt(nm)  # 180.28\nc(ci_lower_m, ci_upper_m)\n\n[1] 180.2876 183.4432\n\n# verify by t.test\nt.test(height_m)\n\n\n    One Sample t-test\n\ndata:  height_m\nt = 231.4, df = 51, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 180.2876 183.4432\nsample estimates:\nmean of x \n 181.8654 \n\n\nBased on our calculation, the mean height for males is 181.87 cm (95% confidence interval (180.28, 183.44)).\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\nLet \\(\\mu_F\\) indicate the average height for the female students. We test the hypothesis\n\\(H_0: \\mu_F = 167\\) against \\(H_a: \\mu_F \\neq 167\\) (two sided test).\n\n# use t.test command\nt.test(height_f, mu = 167)\n\n\n    One Sample t-test\n\ndata:  height_f\nt = 3.3231, df = 53, p-value = 0.001619\nalternative hypothesis: true mean is not equal to 167\n95 percent confidence interval:\n 168.0204 171.1277\nsample estimates:\nmean of x \n 169.5741 \n\n\nWe can conclude that the average height for female students differ significantly from 167 (p = 0.0016). We reject the null hypothesis.\nIf you want to verify the p-value by hand: follow the procedure below.\n\n# (optional) calculate from the formula\nt_stat <- (mean(height_f) - 167)/(sd(height_f)/sqrt(nf))\nt_stat  # 3.323\n\n[1] 3.323112\n\n# compare this with t distribution with nf-1 degrees of freedom\npval_twosided <- pt(q = t_stat, df = nf-1, lower.tail = F)*2\npval_twosided\n\n[1] 0.001618751\n\n\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\nHere we use paired t-test, because these two measurements are on the same subject.\n\n\n\n\n\n\nMissing data in pefsit2\n\n\n\n\n\nWhen you run mean(lung_data$pefsit2), it might return NA as a result. This is caused by missing values in this variable. You can check this from the data (click on lung_data in your environment)(row 66)\nThis does not affect t.test() as it will remove NA automatically. However this might affect other functions, such as mean().\nYou can do mean(pefsit2, na.rm = T) (remove NA). This does not remove NA from your data forever; but only for your mean computation.\nYou should check whether there are NA in your data. One option is summary(pefsit2).\n\n\n\n\npefsit1 <- lung_data$pefsit1\npefsit2 <- lung_data$pefsit2\n# hist(pefsit1)\n# hist(pefsit2)\n# compute mean:\nmean(pefsit1)\n\n[1] 504.6038\n\nmean(pefsit2) # this returns NA, bec subject 66 has missing data\n\n[1] NA\n\nlung_data[66, ]\n\n   age gender height weight pefsit1 pefsit2 pefsit3 pefsta1 pefsta2 pefsta3\n66  19 female    166     56     450      NA      NA      NA     425      NA\n   pefsitm pefstam pefmean\n66      NA      NA      NA\n\n# to remove this when computing mean by mean(): \nmean(pefsit2, na.rm = T) # remove NA when computing mean\n\n[1] 509.5238\n\n# (how to check if my data has NA: is.na(pefsit2); summary(pefsit2))\nsummary(pefsit2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  280.0   415.0   500.0   509.5   580.0   800.0       1 \n\n# t-test will automatically remove NA \nt.test(pefsit1, pefsit2, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsit1 and pefsit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean difference \n           -4.4 \n\n# alternatively, you can test whether the difference is equal to 0\ndiff_sit1_sit2 <- pefsit1 - pefsit2\nt.test(diff_sit1_sit2, mu = 0)\n\n\n    One Sample t-test\n\ndata:  diff_sit1_sit2\nt = -1.5781, df = 104, p-value = 0.1176\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.929056  1.129056\nsample estimates:\nmean of x \n     -4.4 \n\n\nWe can conclude that there is no significant difference (p = 0.12) between the first and second pef measurement.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\nWe still use paired test as both measurements are on the same subject.\n\n# compare pefsitm, pefstam (paired t-test)\npefsitm <- lung_data$pefsitm\npefstam <- lung_data$pefstam\n\nt.test(pefsitm, pefstam, paired = T)\n\n\n    Paired t-test\n\ndata:  pefsitm and pefstam\nt = -3.6974, df = 104, p-value = 0.0003498\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -14.656161  -4.423204\nsample estimates:\nmean difference \n      -9.539683 \n\n\nWe can conclude that that there is a significant difference (p < 0.001) between pef measured in a sitting and standing position.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference <- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n# create a variable\ndiff_sitm_stam <- pefsitm - pefstam\nqqnorm(diff_sitm_stam, pch = 20)\nqqline(diff_sitm_stam, col = 'red', lwd = 2)\n\n\n\n\nWhen a Q-Q plot looks like this, we can say that the normality assumption is reasonably fulfilled.\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n# this is independent two samples t-test\n# we need two variables: pefmean for men, pefmean for women\n\npefmean_f <- lung_data$pefmean[gender == 'female']\npefmean_m <- lung_data$pefmean[gender == 'male']\n\n# visually spot whether there is a difference\n# NOTE: there is a NA in pefmean_f; use na.rm = T to remove it \npar(mfrow = c(1, 2))\nhist(pefmean_f, main = 'pefmean (F)')\nabline(v = mean(pefmean_f, na.rm = T), col = 'red', lwd = 2)\nhist(pefmean_m, main = 'pefmean (M)')\nabline(v = mean(pefmean_m), col = 'red', lwd = 2)\n\n\n\n\nFrom the histogram for female and male students, it can be seen that the difference between the two mean pef measurements differ. We can test it using t-test for two independent samples.\n\n\n\n\n\n\nEqual variance assumption\n\n\n\n\n\nFor two sample t-test, there is an assumption of equal variance in two groups. In R, t.test() automatically checks whether this is fulfilled. If not, it returns result from Welch’s t-test.\nIf you want to force t.test() to use equal variance, specify var.equal = T.\nThe ways to interpret results are the same.\n\n\n\n\n# two sample t-test (Welch)\nt.test(pefmean_f, pefmean_m, paired = F)\n\n\n    Welch Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.425, df = 90.28, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.6366 -150.3941\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n# force equal variance\nt.test(pefmean_f, pefmean_m, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pefmean_f and pefmean_m\nt = -12.468, df = 103, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -207.4907 -150.5401\nsample estimates:\nmean of x mean of y \n 425.2987  604.3141 \n\n\nFrom both results you can see that the difference in means between two genders is very big. The p-values are very small (p < 0.001), can reject the null hypothesis that the sample means from two groups are equal.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?\n\npar(mfrow = c(1, 2))\nqqnorm(pefmean_f, pch = 20, main = 'Q-Q plot: pefmean (F)')\nqqline(pefmean_f, col = 'red', lwd = 2)\n\nqqnorm(pefmean_m, pch = 20, main = 'Q-Q plot: pefmean (M)')\nqqline(pefmean_m, col = 'red', lwd = 2)\n\n\n\n\nThe normality assumption is fulfilled."
  },
  {
    "objectID": "lab/lab_categorical.html",
    "href": "lab/lab_categorical.html",
    "title": "Categorical data analysis",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-1-lung-function",
    "href": "lab/lab_categorical.html#exercise-1-lung-function",
    "title": "Categorical data analysis",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe investigate whether having a high value of pefmean is associated with gender. First, create a new variable highpef that indicates whether pefmean is above 500.\n\n# if you use rda data file: \n# load('./data/PEFH98-english.rda')\n\n# if you use csv: \nlung_data <- read.csv('data/PEFH98-english.csv', sep = ',')\n# head(lung_data)\n\n# examine variable pefmean\n# visualise the distribution\nhist(lung_data$pefmean)\nabline(v = 500, col = 'red', lwd = 2)\n\n\n\n\nThere are multiple ways to do it. Here we show two of them.\nIn option 1, we use the function ifelse(). It will create a binary vector, and fill in different output based on whether the condition was true or not.\n\n# option 1\n# code new variable: highpef with cutoff = 500\n# this is a binary variable, 1 means yes (higher than 500), 0 means no\nhighpef <- ifelse(lung_data$pefmean > 500, '1', '0')\n\n# check if it makes sense\nhead(lung_data$pefmean)\n\n[1] 405.0000 491.6667 505.0000 513.3333 725.0000 602.5000\n\nhead(highpef)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\nIn option 2, we create a vector with a pre-specified value, then replace the elements for given indices.\n\n# option 2 \n# create a vector with all '0's\n# then set the elements above 500 in 'pefmean' as '1'\n# (replacement with index)\n\nn <- length(lung_data$pefmean) # 106\nhighpef_alt <- rep('0', n) # repeat '0' 106 times\nhighpef_alt[lung_data$pefmean > 500] <- '1'\nhead(highpef_alt)\n\n[1] \"0\" \"0\" \"1\" \"1\" \"1\" \"1\"\n\n\n\n\n1b)\nWe investigate the association between these two variables via an appropriate table analysis. Make a table of highpef vs gender, which counts the number of subjects having each one of the four combinations in a 2 by 2 contingency table.\nYou can do this with table() command: it counts how many observations equals to each unique value in the data. When the data takes 2 values (1/0, yes or no), table() counts the numbers in each category.\n\n# take out gender variable so we don't need to use $ any more\ngender <- lung_data$gender\n\n# we can count how many subjects are in each category for the two variables\ntable(gender)\n\ngender\nfemale   male \n    54     52 \n\ntable(highpef)\n\nhighpef\n 0  1 \n54 51 \n\n\nUsing table() for two variables produces the cross tabulation: \\(2 \\times 2\\) combinations. The first element is put as rows, and second element is put as columns.\n\n# now we count how many subjects are in the combinations \ntable_gender_highpef <- table(gender, highpef)\ntable_gender_highpef\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\n\n\n\n\n\n\n\nCross tabulation, order of exposure and outcome\n\n\n\n\n\nYou should be aware that table() does not automatically produce the contingency table which puts exposed (group 1) for two outcomes at the first row; neither does it put positive outcome (outcome = yes) for exposed/unexposed group in the first column.\ntable() only counts the combination. It is up to you to make sense which numbers are \\(d1, d0, h1, h0\\)!\n\n\n\n\n\n1c)\nCompute risk ratio and odds ratio (only the point estimates, no confidence interval). Here the exposure is gender (we assume that the reference group is female: unexposed), and outcome is highpef. Interpret your results.\n\n\n\n\n\n\nRisk ratio and odds ratio\n\n\n\n\n\n\n\n\n\nOutcome (yes)\nOutcome (no)\n\n\n\n\nExposed\nd1\nh1\n\n\nUnexposed\nd0\nh0\n\n\n\nRisk ratio: \\(\\frac{d1/(d1+h1)}{d0/(d0+h0)}\\)\nOdds ratio: \\(\\frac{d1/h1}{d0/h0} = \\frac{d1 \\times h0}{d0 \\times h1}\\)\n\n\n\nIt could help by drawing a table before computing the metrics.\n\n\n\n\nOutcome (highpef = 1)\nOutcome (highpef = 0)\n\n\n\n\nExposed (gender = male)\n46\n6\n\n\nUnexposed (gender = female)\n5\n48\n\n\n\n\n# first use formula (only point estimates)\n# risk ratio (relative risk)\n# risk in exposed / risk in unexposed\n\nrisk_exposed <- 46/(46+6)  # 0.885\nrisk_unexposed <- 5/(5+48) # 0.094\nrr <- risk_exposed/risk_unexposed\nrr  # 9.37\n\n[1] 9.376923\n\n# odds ratio\n# odds of event in exposed group / odds of event in non-exposed group\nodds_exposed <- 46/6  # 7.667\nodds_unexposed <- 5/48  # 0.104\nor <- odds_exposed/odds_unexposed\nor # 73.6\n\n[1] 73.6\n\n\n\n\n1d)\nCarry out a chi-square analysis to assess the strength of association. Interpret your results.\nWe do a chi-square test with chisq.test() function. To read more about what is required as input, you can use ?chisq.test() to get the documentation.\n\n# note: \n# we have not distinguished between exposure and outcome\n# by default, 'table' function puts the first input in rows (gender)\n# the order does not affect the result of chi-square test\n\nchisq.test(table_gender_highpef)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_gender_highpef\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\nIf you want to know what are the observed and expected numbers used for the chi-square test, you can create a variable (let us call it ctest), and access the elements using $.\n\nctest <- chisq.test(table_gender_highpef)\n# you can extract elements of ctest using $\nctest$observed # observed data\n\n        highpef\ngender    0  1\n  female 48  5\n  male    6 46\n\nctest$expected # expected data (under null, no association)\n\n        highpef\ngender          0        1\n  female 27.25714 25.74286\n  male   26.74286 25.25714\n\n\n\n\n\n\n\n\nChi-square test measures strength of association\n\n\n\nIt is worth pointing out that chisq.test() does not distinguish between exposure and outcome variables. If you reverse the variable order, the test-statistic and p-value is exactly the same.\nThis is why you should always be clear in your mind what is your exposure and outcome, and report risk ratio and/or odds ratio.\n\n\n\n# reverse the order of the two variables: highpef firsst, gender second\ntable_highpef_gender <- table(highpef, gender)\nchisq.test(table_highpef_gender)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_highpef_gender\nX-squared = 62.498, df = 1, p-value = 2.667e-15\n\n\n\n\nOptional: use R package to compute RR and OR\n(This is for people who are interested, and able to load additional R packages and read the documentations. If this is too much: skip it)\nYou can verify if this is correct with some R packages that implement risk ratio and odds ratio.\n\n\n\n\n\n\nBe careful with the input format when using packages\n\n\n\nDifferent packages can have different requirements for how your data input should look like. Always check documentation to get the correct result.\nThis is also true with other softwares like STATA.\nComparison of different packages in computing risk ratio: read this discussion\nDocumentation: epitools Documentation: epiR\n\n\nThe first function we use is epitab from epitools package.\n\n# install.packages('epitools')\n\n# match the expected data format\n# col: outcome -, outcome +\n# row: exposure -, exposure +\ntb1 <- matrix(c(48, 5, 6, 46), byrow = T, ncol = 2)\n# tb1\nepitools::epitab(tb1, method = 'riskratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 riskratio    lower    upper\n  Exposed1       48 0.9056604        5 0.09433962  1.000000       NA       NA\n  Exposed2        6 0.1153846       46 0.88461538  9.376923 4.048485 21.71842\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\nepitools::epitab(tb1, method = 'oddsratio')\n\n$tab\n          Outcome\nPredictor  Disease1        p0 Disease2         p1 oddsratio    lower    upper\n  Exposed1       48 0.8888889        5 0.09803922       1.0       NA       NA\n  Exposed2        6 0.1111111       46 0.90196078      73.6 21.00627 257.8735\n          Outcome\nPredictor      p.value\n  Exposed1          NA\n  Exposed2 2.16777e-17\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\nThe second function we use is epi.2by2 from epiR package.\n\n# install.packages('epiR')\n\n# match the expected data format\n# col: outcome +, outcome -\n# row: exposure +, exposure -\ntb2 <- matrix(c(46, 6, 5, 48), byrow = T, ncol = 2)\n# tb2\nepiR::epi.2by2(tb2, method = 'cohort.count')\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           46            6         52     88.46 (76.56 to 95.65)\nExposed -            5           48         53       9.43 (3.13 to 20.66)\nTotal               51           54        105     48.57 (38.70 to 58.53)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 9.38 (4.05, 21.72)\nOdds ratio                                     73.60 (21.01, 257.87)\nAttrib risk in the exposed *                   79.03 (67.31, 90.75)\nAttrib fraction in the exposed (%)            89.34 (75.30, 95.40)\nAttrib risk in the population *                39.14 (26.76, 51.52)\nAttrib fraction in the population (%)         80.58 (57.91, 91.04)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 65.624 Pr>chi2 = <0.001\nFisher exact test that OR = 1: Pr>chi2 = <0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "lab/lab_distributions.html",
    "href": "lab/lab_distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "In this R lab, we will practice with probability distributions."
  },
  {
    "objectID": "lab/lab_distributions.html#binomial-distribution",
    "href": "lab/lab_distributions.html#binomial-distribution",
    "title": "Probability distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA random variable \\(X\\) is said to be a binomial variable with parameters \\(n\\) and \\(p\\) if\n\\[ P(X=x) = \\binom n x p^{x}(1-p)^{n-x}\\]\nThis is often writen \\(X \\sim \\text{Binom}(n,p)\\). Let us compute the probabilities of a binomial distribution \\(X \\sim \\text{Binom}(5,0.2)\\) using R.\n\n# first, we store the parameters of the distribution in variables n and p\nn = 5\np = 0.2\n\nx = 0:n # we create a vector with discrete values from 0 to n\n\nbinom_coef=choose(n, x) # we compute the binomial coefficients for each x \n\n# we compute binomial probabilities for all x values\nbinom_prob= binom_coef* p^x * (1 - p)^{n-x} \nprint(binom_prob) # we print all probabilities\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\nThis was a good exercise, but R has a built-in function to compute the probabilities of a binomial distribution directly: dbinom(x,n,p). Let us use that function to verify that the probabilities that we computed were correct.\n\nbinom_prob = dbinom(x,n,p)\nprint(binom_prob)\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\nWe can plot those probabilities with the barplot command to see the probability density function (PDF).\n\nbarplot(names= x,\n        height = binom_prob,\n        main = \"PDF of X~Binom(5,0.2)\",\n        xlab='X',\n        ylab='Probability')"
  },
  {
    "objectID": "lab/lab_distributions.html#simulations-of-a-binomial-distribution",
    "href": "lab/lab_distributions.html#simulations-of-a-binomial-distribution",
    "title": "Probability distributions",
    "section": "Simulations of a binomial distribution",
    "text": "Simulations of a binomial distribution\nNext we will simulate N samples from a binomial distribution with the command rbinom(N, n, p) and look at the frequency distribution of the outcomes.\n\nN=100\n# we draw N samples from a binomial distribution with parameters n and p and store them in variable binom_sim\nbinom_sim = rbinom(N,n,p)\n\n# we compute the frequency distribution of the samples and store them in a variable\ndata = table(binom_sim)\nprint(data)\n\nbinom_sim\n 0  1  2  3 \n44 35 18  3 \n\n# we can plot the frequencies in a bar plot\nbarplot(data,ylab=\"frequencies\")\n\n\n\n# we can plot the frequencies in a pie chart\npie(data)\n\n\n\n\nNext, we will compute the relative frequencies by dividing the observed frequencies by the total number of draws N. For high N, the relative frequencies should approach the theoretical probabilities of the binomial distribution.\n\nbinom_sim_relfreq=data/N\nprint(binom_sim_relfreq)\n\nbinom_sim\n   0    1    2    3 \n0.44 0.35 0.18 0.03 \n\nbarplot(binom_sim_relfreq,ylab=\"relative frequency\")"
  },
  {
    "objectID": "lab/lab_distributions.html#normal-distribution",
    "href": "lab/lab_distributions.html#normal-distribution",
    "title": "Probability distributions",
    "section": "Normal distribution",
    "text": "Normal distribution\nThe probability density function of the normal distribution \\(X \\sim \\text{N}(\\mu,\\sigma)\\), where \\(\\mu\\) denotes the mean and \\(\\sigma\\) the standard deviation, is given by:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\text{exp}(- \\frac{(x-\\mu)^2}{2\\sigma^2})\n\\]\nWe can define and plot this function in R to see the characteristic bell shape of the normal distribution.\n\n# first we set the values for the mean and the sd \nmu=0\nsigma=1\n\n# next, we define the function f\nf_norm=function(x){(1/(sigma*sqrt(2*pi))) * exp((-(x-mu)^2)/(2*sigma^2))}\n#note that this function is already defined in R as dnorm(x), we could use that instead\n\n# we plot the function f\ncurve(f_norm,\n      xlim=c(-5,5),\n      main=\"Normal density plot\",\n      xlab=\"X values\",\n      ylab=\"Density\"\n      )\n\n\n\n\nNext, we will compute \\(P(X \\leq 0)\\). For that we can compute the area under the curve from \\(-\\infty\\) to \\(1\\) using the function integrate.\n\nintegrate(f_norm,-Inf, 0)\n\n0.5 with absolute error < 4.7e-05\n\n\nUnsurprisingly, R has a built-in function to compute such probabilities: pnorm(x,mean, sd). Let us use that function to verify that the probability that we computed was correct.\n\npnorm(0, mean=0, sd=1)\n\n[1] 0.5\n\n\nConversely, we can find the value of \\(x\\) so that \\(P(X\\leq x) = 0.5\\) using the command qnorm(prob,mean,sd).\n\nqnorm(0.5,mean=0,sd=1)\n\n[1] 0\n\n\nNow let us find the value of \\(x\\) so that \\(P(-x\\leq X\\leq x) = 0.95\\). As the distribution is symmetric with respect to zero, we could instead find the value of \\(x\\) so that \\(P( X\\leq x) = 0.025\\), or equivalently, the value of \\(x\\) so that \\(P( X\\geq x) = 0.025\\). We can compute those values again using the command pnorm.\n\nqnorm(0.025,mean=0,sd=1) # find x so that P(X<=x) = 0.025\n\n[1] -1.959964\n\nqnorm(0.025,mean=0,sd=1, lower.tail = FALSE) # find x so that P(X>=x) = 0.025\n\n[1] 1.959964\n\n\nLet us verify that \\(P(-1.96\\leq X\\leq 1.96) = 0.95\\).\n\npnorm(1.96,0,1) - pnorm(-1.96,0,1)\n\n[1] 0.9500042\n\n\nIt is important to remember where the value 1.96 comes from, we will see it later in the course!"
  },
  {
    "objectID": "lab/lab_distributions.html#simulations-of-the-normal-distribution",
    "href": "lab/lab_distributions.html#simulations-of-the-normal-distribution",
    "title": "Probability distributions",
    "section": "Simulations of the normal distribution",
    "text": "Simulations of the normal distribution\nTo simulate samples of the normal distribution in R, we use the command rnorm(N, mean, sd). If we want to simulate samples of the standard normal distribution \\(N(0,1)\\), we can simply use the command rnorm(N).\n\nnorm_sim=rnorm(500)\nhist(norm_sim, \n     freq = FALSE, #This is needed to plot the relative frequencies instead of the frequencies, so that the total area of the histogram is one.\n     main= \"Histrogram of normal data\",\n     xlab=\"X values\")\n\n\n\n\nThe larger the number of samples, the the closer the histogram would look to a bell shape. We can also check if our data are normally distributed using the command qqnorm(data) that generates a QQPlot. These plots display the data quantiles against the theoretical quantiles of the normal distribution. Therefore, normally distributed data should lie close to the identity line \\(y=x\\).\n\nqqnorm(norm_sim) # we generate the QQPlot for the data simulated data"
  },
  {
    "objectID": "lab/lab_distributions.html#approximation-of-the-binomial-distribution",
    "href": "lab/lab_distributions.html#approximation-of-the-binomial-distribution",
    "title": "Probability distributions",
    "section": "Approximation of the binomial distribution",
    "text": "Approximation of the binomial distribution\nHere we will compute the probability density function of binomial distributions with parameters \\(p=0.2\\) and parameter \\(n\\) variying from 2 to 30.\n\np=0.3\n\npar(mfrow=c(2,2))    # set the plotting area into a 2*2 matrix\n#first plot with n=2\nx = 0:2\nbarplot(names= x,\n        height = dbinom(x,2,p),\n        main = \"PDF of X~Binom(2,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#second plot with n=5\nx = 0:5\nbarplot(names= x,\n        height = dbinom(x,5,p),\n        main = \"PDF of X~Binom(5,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#third plot with n=15\nx = 0:15\nbarplot(names= x,\n        height = dbinom(x,15,p),\n        main = \"PDF of X~Binom(15,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n#fourth plot with n=30\nx = 0:30\nbarplot(names= x,\n        height = dbinom(x,30,p),\n        main = \"PDF of X~Binom(30,0.3)\",\n        xlab='X',\n        ylab='Probability')\n\n\n\n\nWe observe that the higher \\(n\\) is, the closer the PDF of the binomial distribution looks to a bell shape. From the graphs above, it seems that \\(X_1 \\sim \\text{Binom}(30,0.3)\\) can be approximated by a normal distribution \\(X_2 \\sim \\text{N}(30*0.3,\\sqrt{30*0.3*0.7})\\). But do the probabilities of anologous events also match? Next, we compare \\(P(X_1 = 10)\\) and \\(P(9.5 \\leq X_2 \\leq 10.5)\\).\n\ndbinom(x=10,30,0.3)\n\n[1] 0.1415617\n\nmu=30*0.3\nsigma=sqrt(30*0.3*0.7) \npnorm(10.5,mu,sigma) - pnorm(9.5,mu,sigma)\n\n[1] 0.1460026"
  },
  {
    "objectID": "lab/lab_distributions.html#exercise-when-the-normal-distribution-wont-work",
    "href": "lab/lab_distributions.html#exercise-when-the-normal-distribution-wont-work",
    "title": "Probability distributions",
    "section": "Exercise: When the normal distribution won’t work",
    "text": "Exercise: When the normal distribution won’t work\nCheck if a Binomial distribution with \\(p=0.01\\) and \\(n=30\\) can be approximated by a normal distribution. Why?"
  },
  {
    "objectID": "lab/lab_distributions.html#central-limit-theorem",
    "href": "lab/lab_distributions.html#central-limit-theorem",
    "title": "Probability distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nWe will simulate 1000 samples of size \\(N=100\\) of the binomial distribution with parameters \\(p=0.01\\) and \\(n=30\\) (it could be any other distribution). We will compute the mean of each sample and see that the sample mean is a random variable that is normally distributed.\n\nsample_means=replicate(n = 1000, mean(rbinom(100,30,0.01)))\nhist(sample_means, breaks=20)\n\n\n\nqqnorm(sample_means)"
  },
  {
    "objectID": "lab/lab_distributions.html#exercise-sample-mean-with-small-sample-size",
    "href": "lab/lab_distributions.html#exercise-sample-mean-with-small-sample-size",
    "title": "Probability distributions",
    "section": "Exercise: Sample mean with small sample size",
    "text": "Exercise: Sample mean with small sample size\nRepeat the previous simulations, but this time take a very small sample size. Is the sample mean also normally distributed? Why?"
  },
  {
    "objectID": "lab/ex_categorical.html",
    "href": "lab/ex_categorical.html",
    "title": "Exercise - Categorical data analysis",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_categorical.html#exercise-1-lung-function",
    "href": "lab/ex_categorical.html#exercise-1-lung-function",
    "title": "Exercise - Categorical data analysis",
    "section": "Exercise 1 (lung function)",
    "text": "Exercise 1 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sitting position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n1a)\nWe investigate whether having a high value of pefmean is associated with gender.\nFirst, create a new variable highpef that indicates whether pefmean is above 500.\n\n\n1b)\nWe investigate the association between these two variables via an appropriate table analysis. Make a table of highpef vs gender, which counts the number of subjects having each one of the four combinations in a 2 by 2 contingency table.\n\n\n1c)\nCompute risk ratio and odds ratio (only the point estimates, no confidence interval). Here the exposure is gender (we assume that the reference group is female: unexposed), and outcome is highpef. Interpret your results.\n\n\n\n\n\n\nRisk ratio and odds ratio\n\n\n\n\n\n\n\n\n\nOutcome (yes)\nOutcome (no)\n\n\n\n\nExposed\na\nb\n\n\nUnexposed\nc\nd\n\n\n\nRisk ratio: \\(\\frac{a/(a+b)}{c/(c+d)}\\)\nOdds ratio: \\(\\frac{a/b}{c/d} = \\frac{ad}{bc}\\)\n\n\n\n\n\n1d)\nCarry out a chi-square analysis to assess the strength of association. Interpret your results."
  },
  {
    "objectID": "lab/ex_categorical.html#exercise-2-birth-data",
    "href": "lab/ex_categorical.html#exercise-2-birth-data",
    "title": "Exercise - Categorical data analysis",
    "section": "Exercise 2 (birth data)",
    "text": "Exercise 2 (birth data)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n2a)\nDoes smoking influence the birth weight of a baby?\nMake a histogram and a box plot, and find the median and mean for birth weight in grams (bwt) for the two groups of mothers: one group that smokes, and one that does not smoke durinng the pregnancy. (Hint: use variable smk for this task)\n\n\n2b)\nAre the distributions for the two groups normal? Can we conclude that smoking has an effect on the weight of a newborn? Use a t-test.\n\n\n2c)\nDo mothers of hypertension have a tendency to have babies with a lower birth weight? (The variable for hypertension is ht)\nMake a histogram of birth weight for the two groups of mothers with and without hypertension. What do you observe?\n\n\n2d)\nAnalyse the effect of smoking on birth weight in a table (categorical analysis): compute the relative risk (risk ratio) and carry out a test to assess the strength of association.\nAlso do it for hypertension (instead of smoking).\nWhat can you conclude?"
  },
  {
    "objectID": "lab/ex_ttest.html",
    "href": "lab/ex_ttest.html",
    "title": "Exercise - t-test",
    "section": "",
    "text": "Datasets\nR script"
  },
  {
    "objectID": "lab/ex_ttest.html#exercise-1-heart",
    "href": "lab/ex_ttest.html#exercise-1-heart",
    "title": "Exercise - t-test",
    "section": "Exercise 1 (heart)",
    "text": "Exercise 1 (heart)\nThe weight of the hearts of 20 men with age between 25 and 55 years have been evaluated, and is given below (in ounces, 1 ounce = 28g)\n11.50 14.75 13.75 10.50 14.75 13.50 10.75 9.50 11.75 12.00\n10.50 11.75 10.00 14.50 12.00 11.00 14.00 15.00 11.50 10.25\n\n1a)\nCreate a variable in R, and enter the data. Compute the mean weight of the hearts based on the formula; then verify it with R function.\n\n\n\n\n\n\nFormula: mean\n\n\n\nThe mean of data \\(X = (x_1, x_2, ... x_n)\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^N x_i\\)\n\n\n\n\n1b)\nGo through the procedure of computing the 95% confidence interval for the sample mean from the formula. Verify the computed confidence interval with R function t.test(heart).\n\n\n\n\n\n\nFormula: confidence interval for the mean\n\n\n\n\n\n95% confidence interval for the sample mean \\(\\bar{X}\\) is\n\\((\\bar{X} - t' \\times \\frac{s}{\\sqrt{n}}, \\bar{X} + t' \\times \\frac{s}{\\sqrt{n}})\\)\nwhere \\(s\\) is the (empirical) standard deviation, \\(s^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}\\)\n(\\(t' = 2.09\\) for \\(n = 20\\))\n\n\n\n\n# sample size n of this data\nn <- 20  # length(heart)\n\n# 1. compute (empirical) standard deviation\n# formula (sd): sqrt(1/(n-1) * (sum(xi - xbar)^2))\n\nsqrt(1/(n-1) * (sum((heart - mean(heart))^2))) # be careful with brackets\n\n# alternatively, use sd() function. these two numbers should match\nsd(heart) # 1.779\n\n# 2. 95% CI (quantile from t-distribution)\n# formula: CI = xbar +- t(n-1, alpha/2) * sd/sqrt(n)\n# translate into format R can understand:\n\nt025 <- qt(p = 0.025, df = n-1) # -2.09\nt975 <- qt(p = 0.975, df = n-1) # 2.09\nc(t025, t975) # symmetric around 0, so can pick any one\n\n# plug in the formula\nci_lower <- mean(heart) - t975 * sd(heart)/sqrt(n)\nci_upper <- mean(heart) + t975 * sd(heart)/sqrt(n)\nc(ci_lower, ci_upper)\n\n# verify by doing a one-sample t-test\n# by default, it tests whether mean is 0\nt.test(heart)\n\n\n\n1c)\nWe would like to know whether the mean heart weight is equal to 11 and 11.5.\nBefore you carry out hypothesis tests, you can gain useful insight by visualization. You can try to use histogram, Q-Q plot and boxplot, and compare the mean heart weight with 11 and 11.5.\nWhat can you conclude?\n\n\n1d)\nFormulate hypothesis tests. Decide whether you need one-sided or two-sided test, and state your conclusion (with p-values and confidence intervals)."
  },
  {
    "objectID": "lab/ex_ttest.html#exercise-2-lung-function",
    "href": "lab/ex_ttest.html#exercise-2-lung-function",
    "title": "Exercise - t-test",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (1 is female, 2 is male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n\n\n\n\n\nLung function data\n\n\n\nThis is the same dataset we have used in EDA (part I) from two days ago. You can use either PEFH98-english.rda, or PEFH98-english.csv data.\nIf you forgot how to load the data, you can refresh your knowledge by reading these notes (Example 2)\n\n\n\n2a)\nOpen PEFH98-english (in either csv or rda format) into R.\nDetermine the mean height for females. Calculate the standard deviation and a 95% confidence interval.\nDo the same for males.\n\n\n2b)\nAssume that the average height for female students a few years ago was 167 cm. Does the height in the present material differ significantly?\nFormulate a null hypothesis, calculate a test statistic and p-value.\n\n\n2c)\nDo the values pefsit1 and pefsit2 differ significantly? Calculate a test statistic, the mean difference, and the 95% confidence interval, and formulate a conclusion. Interpret the results.\n\n\n2d)\nCarry out a t-test to decide whether pefsitm annd pefstam are significantly different.\n\n\n2e)\nAre the assumptions of the previous test reasonably fulfilled?\nYou should check that the difference between pefsitm and pefstam is normally distributed.\n\n\n\n\n\n\nExpand to read hint\n\n\n\n\n\nYou can create a new variable called difference by difference <- pefsitm - pefstam, then check the normality of difference by looking at histogram (hist(difference)), or QQ plot (qnorm difference)\n\n\n\n\n\n2f)\nCarry out a t-test to evaluate whether pefmean is significantly different for men and women.\nInterpret the results, and formulate a conclusion including p-value and confidence interval.\n\n\n2g)\nAre the assumptions fulfilled in the previous test (pefmean vs gender)?"
  },
  {
    "objectID": "lab/lab_ttest.html#equal-variance-assumption",
    "href": "lab/lab_ttest.html#equal-variance-assumption",
    "title": "t-test",
    "section": "Equal variance assumption",
    "text": "Equal variance assumption\nFor two sample t-test, there is an assumption of equal variance in two groups. In R, t.test() automatically checks whether this is fulfilled. If not, it returns result from Welch’s t-test.\nIf you want to force t.test() to use equal variance, specify var.equal = T.\nThe ways to interpret results are the same."
  },
  {
    "objectID": "lab/list_of_commands.html",
    "href": "lab/list_of_commands.html",
    "title": "List of commands",
    "section": "",
    "text": "Data\n\nData import\n\nread.csv('data/your_data.csv', sep = ',')\n\n\n\nCreate variables\nCreate a numeric variable\n\na <- 3\na  # return the value in console\nreturn(a)\n\nNumeric, character, logical variables\n\nclass(a)\n\nb <- 'hadley'\nclass(b)\n\nc <- TRUE\nclass(c)\n\n\n\nData structure (vector, matrix)\nCreate vectors and matrices\n\nnum_vector <- c(1, 2, 3, 4, 5)\nchar_vector <- c('student_a', 'student_b', 'student_c')\nlogical_vector <- c(T, F, T, F)\n\n# matrix\nmatrix_1 <- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = T)\n\n# matrix by combining vectors\nvec1 <- c(1, 2)\nvec2 <- c(3, 4)\n\nmatrix_c <- cbind(vec1, vec2) # bind by columnn\nmatrix_r <- rbind(vec2, vec2) # bind by row\n\n\n\n\nData exploration of a data.frame\nCreate a data.frame\n\nmini_data <- data.frame(\n  age = c(20, 50, 32), \n  sex = c('male', 'female', 'male'), \n  has_covid = c(T, T, F)\n)\n\nGet the column (feature) names, dimension, number of rows (observation) and columns\n\ncolnames(mini_data)\ndim(mini_data)\nnrow(mini_data)\nncol(mini_data)\n\nSelect a variable (age) from the data\n\nmini_data$age\nmini_data['age']\nmini_data[, 1] # first column, which is 'age'\n\nFilter a variable based on another (for example, age for females (sex == 'female'))\n\nmini_data$age[mini_data$sex == 'female']\n\n# you can also break down the process:\nage <- mini_data$age\nsex <- mini_data$sex\nage[sex == 'female']\n\n\n\nDescriptive statistics\nContinuous variables\n\n# continuous variable x\nsummary(x)\nmin(x)\nmax(x)\nmean(x)\nmedian(x)\nquantile(x, 0.95)\nIQR(x) # interquartile range\n\nCategorical variables: count and percentage\n\n# continuous variable z\n# subjects per category in x\ntable(x)\n# percentage\ntable(x)/length(x) \n\n\n\nVisualisation\nWe let x, y be two continuous variables, and z be categorical. To create histogram, boxplot, scatterplot, you can use the following commands,\n\nhist(x) # histogram\nboxplot(x) # boxplot \nboxplot(x ~ z, data = data) # boxplot for two variables, where z is categorical\nplot(x,y)  # scatter plot of x, y\n\n\n\nHypothesis tests\n\nt-test\n\n# one sample (default tests against 0, conf.level 0.95)\nt.test(x)\n\n# one sample\nt.test(x, mu = your_value, conf.level = 0.95)\n\n# paired samples\nt.test(x1, x2, paired = T, conf.level = 0.95)\nt.test(x1-x2, conf.level = 0.95) # equivalent to one sample\n\n# two independent samples\nt.test(x, y, conf.level = 0.95)\n\n# check normal assumption\nqqnorm(x)\nqqline(x)\n\n\n\nz-test, chi-square tests and table analysis\n\n# test proportion: whether 123 success in 1000 equals prob = 0.15\nprop.test(x = 123, n = 1000, p = 0.15)\nbinom.test(x = 123, n = 1000, p = 0.15)\n\n# create binary variable\n# compare your continuous values against threshold\n# assign \"yes\" to those higher; otherwise, assign \"no\"\nhigh_value <- ifelse(your_values > threshold, \"yes\", \"no\")\n\n# count each category\ntable(x) # x is categorical! \n\n# cross tabulation (2 variables)\ntable(x,y) # x, y are categorical\n\n# chi.squared test\n# tb is a 2 by 2 table (matrix) with counts\nchisq.test(tb)\n\n\n(Content for Week 2 will come soon!)\n\n\nnon-parametric tests\n\n\n\nRegression\n\nLinear regression\n\n\nLogistic regression\n\n\nSurvival analysis"
  },
  {
    "objectID": "lab/lab_categorical.html#exercise-2-birth-data",
    "href": "lab/lab_categorical.html#exercise-2-birth-data",
    "title": "Categorical data analysis",
    "section": "Exercise 2 (birth data)",
    "text": "Exercise 2 (birth data)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n2a)\nDoes smoking influence the birth weight of a baby?\nMake a histogram and a box plot, and find the median and mean for birth weight in grams (bwt) for the two groups of mothers: one group that smokes, and one that does not smoke durinng the pregnancy. (Hint: use variable smk for this task)\n\n# if you are using .rda: click on the data icon\n\n# load data\nbirth <- read.csv('data/birth.csv', sep = ',')\nhead(birth)\n\n  id         low age lwt   eth       smk ptl  ht  ui fvt ttv  bwt\n1  4 bwt <= 2500  28 120 other    smoker   1  no yes   0   0  709\n2 10 bwt <= 2500  29 130 white nonsmoker   0  no yes   2   0 1021\n3 11 bwt <= 2500  34 187 black    smoker   0 yes  no   0   0 1135\n4 13 bwt <= 2500  25 105 other nonsmoker   1 yes  no   0   0 1330\n5 15 bwt <= 2500  25  85 other nonsmoker   0  no yes   0   4 1474\n6 16 bwt <= 2500  27 150 other nonsmoker   0  no  no   0   5 1588\n\n# take the variables\nbirthweight <- birth$bwt\nsmoker <- birth$smk\n\n# get to know how many smoker and non-smoker\ntable(smoker)\n\nsmoker\nnonsmoker    smoker \n      115        74 \n\n# histogram for all (both categories together)\nhist(birthweight)\n\n\n\n# separate birthweight based on smoking\nbw_smoker <- birthweight[smoker == 'smoker']\nbw_nonsmoker <- birthweight[smoker == 'nonsmoker']\n\n# can produce summary statistics\nsummary(bw_smoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2370    2776    2773    3246    4238 \n\nsummary(bw_nonsmoker)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1021    2509    3100    3055    3622    4990 \n\n\nVisualise with histogram and boxplot\n\npar(mfrow = c(1, 2))\n# histogram for smokers\n# (you can try to add sample means for each group)\n# set the range for axis limits so they look comparable\nhist(bw_smoker, main = 'birthweight: smoker mothers', xlim = c(600, 5000))\nhist(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', xlim = c(600, 5000))\n\n\n\n# boxplot \nboxplot(bw_smoker, main = 'birthweight: smoker mothers', ylim = c(600, 5000))\nboxplot(bw_nonsmoker, main = 'birthweight: nonsmoker mothers', ylim = c(600, 5000))\n\n\n\n\nFrom the histograms and boxplots, you can spot that there seems to be slightly smaller values of baby birth weight for smoker mothers; although we can not draw a concrete conclusion by looking at the graphs. Therefore, you should carry out a hypothesis test to be more certain.\n\n\n2b)\nAre the distributions for the two groups normal? Can we conclude that smoking has an effect on the weight of a newborn? Use a t-test.\n\nqqnorm(bw_smoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_smoker)\n\n\n\nqqnorm(bw_nonsmoker, main = 'QQplot: birthweight: smoker mothers')\nqqline(bw_nonsmoker)\n\n\n\n# two samples t-test \nt.test(bw_smoker, bw_nonsmoker, paired = F) # welch\n\n\n    Welch Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.7095, df = 170, p-value = 0.00743\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -486.95979  -76.46677\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\nt.test(bw_smoker, bw_nonsmoker, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_smoker and bw_nonsmoker\nt = -2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -492.73382  -70.69274\nsample estimates:\nmean of x mean of y \n 2773.243  3054.957 \n\n\nFrom the Q-Q plots, you can consider that the normality assumption is fulfilled.\nThe t-test (either Welch’s two sample t-test; or two sample t-test for equal variances) indicates that there is significant difference between the two groups. Birth weight of babies from smoker mothers are significantly lower than those from non-smoker mothers (p=0.009 if using the second test).\n\n\n2c)\nDo mothers of hypertension have a tendency to have babies with a lower birth weight? (The variable for hypertension is HT)\nMake a histogram of birth weight for the two groups of mothers with and without hypertension. What do you observe?\n\n# take out the variables\n# head(birth)\nhypertension <- birth$ht\n\n\n# separate birthweight based on hypertension\nbw_hypertension <- birthweight[hypertension == 'yes']\nbw_nohypertension <- birthweight[hypertension == 'no']\n\n# can produce summary statistics\nsummary(bw_hypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1135    1775    2495    2537    3232    3790 \n\nsummary(bw_nohypertension)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    709    2424    2992    2972    3475    4990 \n\npar(mfrow = c(1, 2))\n# can set an x axis limit so they are comparable\nhist(bw_hypertension, main = 'birthweight: hypertension', xlim = c(600, 5000))\nhist(bw_nohypertension, main = 'birthweight: no hypertension', xlim = c(600, 5000))\n\n\n\n# boxplot \nboxplot(bw_hypertension, main = 'birthweight: hypertension', ylim = c(600, 5000))\nboxplot(bw_nohypertension, main = 'birthweight: no hypertension', ylim = c(600, 5000))\n\n\n\n\nYou can read (from the y-axis of histograms) that baby birth weight for hypertension mother group have very few observations compared to those with no hypertension mothers. This can not be spotted in the boxplot.\nThis suggests that when you carry out exploratory analysis, it’s always useful to do it in different ways.\n\n# check normality\nqqnorm(bw_hypertension, main = 'birthweight: hypertension')\nqqline(bw_hypertension)\n\n\n\nqqnorm(bw_nohypertension, main = 'birthweight: no hypertension')\nqqline(bw_nohypertension)\n\n\n\n# can check the standard deviation to see if you need welch or equal variance t-test\nc(sd(bw_hypertension), sd(bw_nohypertension))\n\n[1] 917.3405 709.2265\n\n# two samples t-test \n# t.test(bw_hypertension, bw_nohypertension, paired = F) # welch\nt.test(bw_hypertension, bw_nohypertension, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  bw_hypertension and bw_nohypertension\nt = -2.0192, df = 187, p-value = 0.04489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -861.09734  -10.02413\nsample estimates:\nmean of x mean of y \n 2536.750  2972.311 \n\n\nThe data in hypertension group (even though scarce) fall roughly on the straight line in the QQ plot. The data on the no hyper tension group (much more data) also fall on the straight line; this suggests that the normal assumption can be assumed to be fulfilled.\nWe have checked whether the variances between the two groups are very different (rule of thumb: twice standard deviation). We can see that they do not differ by that much; so we can use the t-test with equal variance.\n\n\n2d)\nAnalyse the effect of smoking on birth weight in a table (categorical analysis): compute the relative risk (risk ratio) and carry out a test to assess the strength of association.\nAlso do it for hypertension (instead of smoking).\nWhat can you conclude?\n\n# smoking on birth weight\n# cross tabulation\nlow_bw <- birth$low\ntable(low_bw)\n\nlow_bw\nbwt <= 2500  bwt > 2500 \n         59         130 \n\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_smk <- table(smoker, low_bw)\ntb_bw_smk\n\n           low_bw\nsmoker      bwt <= 2500 bwt > 2500\n  nonsmoker          29         86\n  smoker             30         44\n\n# compute your risk ratio!\n\nchisq.test(tb_bw_smk)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_smk\nX-squared = 4.2359, df = 1, p-value = 0.03958\n\n\nExposure variable is smoker, and outcome is low birth weight of baby.\nRisk in exposed group (smoker): 30/(30+44) = 0.405\nRisk in unexposed group (nonsmoker): 29/(29+86) = 0.252\nRelative risk (risk ratio): 0.405/0.252 = 1.61\nWith the chi-square test which has a p value of 0.03, you can say that there is evidence that smoking during preganancy affects the risk of low birth weight. (We have not computed the confidence interval here, but you should report them. Use an R package)\n\n(Optional)\nThe situation for hypertension is slightly tricker.\n\n# hypertension\ntable(hypertension)\n\nhypertension\n no yes \n177  12 \n\n# cross tabulation\n# distinguish case/non case, exposed/unexposed on your own!\ntb_bw_ht <- table(hypertension, low_bw)\ntb_bw_ht\n\n            low_bw\nhypertension bwt <= 2500 bwt > 2500\n         no           52        125\n         yes           7          5\n\n# compute your risk ratio!\n\nExposure variable is hypertension, and outcome is low birth weight of baby.\nRisk in exposed group (hypertension): 7/(7+5) = 0.583\nRisk in unexposed group (no hypertension): 52/(52+125) = 0.293\nRelative risk (risk ratio): 0.405/0.252 = 1.99\n\n# carry out a chi-square test\nchisq.test(tb_bw_ht)\n\nWarning in chisq.test(tb_bw_ht): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb_bw_ht\nX-squared = 3.1431, df = 1, p-value = 0.07625\n\n# can specify the computation: \nchisq.test(tb_bw_ht, correct = F) # remove the continuity correction\n\nWarning in chisq.test(tb_bw_ht, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = 1, p-value = 0.03619\n\nchisq.test(tb_bw_ht, simulate.p.value = T) # simulate p values\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tb_bw_ht\nX-squared = 4.388, df = NA, p-value = 0.05197\n\n\nYou can see that the conclusion for this chi-square test is problematic, as the data is very imbalanced. The p-value is around 0.05, so you are not able to reject the null hypothesis (of no association) with full confidence. You should report your confidence interval along with the point estimate, and also report your p-value even if it is not significantly small.\nYou should also double check with results computed in R packages that have implemented relative risk."
  },
  {
    "objectID": "lab/lab_regression.html",
    "href": "lab/lab_regression.html",
    "title": "Regression analysis",
    "section": "",
    "text": "Linear regression\nTBC\n\n\nLogistic regression\n(Dataset birth might be switched out)\n\n# load data\nbirth_data <- haven::read_dta('data/birth.dta')\n\n# print the first rows of the data set\nhead(birth_data)\n\n# A tibble: 6 × 12\n     id low            age   lwt eth       smk   ptl ht      ui        fvt   ttv\n  <dbl> <dbl+lbl>    <dbl> <dbl> <dbl+l> <dbl> <dbl> <dbl+l> <dbl+l> <dbl> <dbl>\n1     4 1 [bwt < 25…    28   120 3 [oth…     1     1 0 [no]  1 [yes]     0     0\n2    10 1 [bwt < 25…    29   130 1 [whi…     0     0 0 [no]  1 [yes]     2     0\n3    11 1 [bwt < 25…    34   187 2 [bla…     1     0 1 [yes] 0 [no]      0     0\n4    13 1 [bwt < 25…    25   105 3 [oth…     0     1 1 [yes] 0 [no]      0     0\n5    15 1 [bwt < 25…    25    85 3 [oth…     0     0 0 [no]  1 [yes]     0     4\n6    16 1 [bwt < 25…    27   150 3 [oth…     0     0 0 [no]  0 [no]      0     5\n# … with 1 more variable: bwt <dbl>\n\n\n\nFit a logistic regression model for low and age\n\n# low ~ age \n\nlr_low_age <- glm(low ~ age, data = birth_data, family = 'binomial')\nsummary(lr_low_age)\n\n\nCall:\nglm(formula = low ~ age, family = \"binomial\", data = birth_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0402  -0.9018  -0.7754   1.4119   1.7800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# the result regresssion coefficient is not odds ratio\nlr_low_age$coefficients\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# regression coefficients can also be extracted in this way:\ncoefficients(lr_low_age)\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# coef(lr_low_age)\n\n# confidence interval for age and intercept\nconfint(lr_low_age)\n\n                 2.5 %      97.5 %\n(Intercept) -1.0336270 1.847399861\nage         -0.1150799 0.008986436\n\n\n\n# to get odds ratio, exponentiate \nexp(coef(lr_low_age))\n\n(Intercept)         age \n  1.4690000   0.9501333 \n\nexp(confint(lr_low_age))\n\n                2.5 %   97.5 %\n(Intercept) 0.3557144 6.343305\nage         0.8912949 1.009027"
  },
  {
    "objectID": "lab/ex_logisticreg.html",
    "href": "lab/ex_logisticreg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/ex_linearreg.html",
    "href": "lab/ex_linearreg.html",
    "title": "Linear regression",
    "section": "",
    "text": "Datasets\nR Script"
  },
  {
    "objectID": "lab/lab_linearreg.html",
    "href": "lab/lab_linearreg.html",
    "title": "Regression analysis",
    "section": "",
    "text": "Linear regression\nTBC\n\n\nLogistic regression\n(Dataset birth might be switched out)\n\n# load data\nbirth_data <- haven::read_dta('data/birth.dta')\n\n# print the first rows of the data set\nhead(birth_data)\n\n# A tibble: 6 × 12\n     id low            age   lwt eth       smk   ptl ht      ui        fvt   ttv\n  <dbl> <dbl+lbl>    <dbl> <dbl> <dbl+l> <dbl> <dbl> <dbl+l> <dbl+l> <dbl> <dbl>\n1     4 1 [bwt < 25…    28   120 3 [oth…     1     1 0 [no]  1 [yes]     0     0\n2    10 1 [bwt < 25…    29   130 1 [whi…     0     0 0 [no]  1 [yes]     2     0\n3    11 1 [bwt < 25…    34   187 2 [bla…     1     0 1 [yes] 0 [no]      0     0\n4    13 1 [bwt < 25…    25   105 3 [oth…     0     1 1 [yes] 0 [no]      0     0\n5    15 1 [bwt < 25…    25    85 3 [oth…     0     0 0 [no]  1 [yes]     0     4\n6    16 1 [bwt < 25…    27   150 3 [oth…     0     0 0 [no]  0 [no]      0     5\n# … with 1 more variable: bwt <dbl>\n\n\n\nFit a logistic regression model for low and age\n\n# low ~ age \n\nlr_low_age <- glm(low ~ age, data = birth_data, family = 'binomial')\nsummary(lr_low_age)\n\n\nCall:\nglm(formula = low ~ age, family = \"binomial\", data = birth_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0402  -0.9018  -0.7754   1.4119   1.7800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# the result regresssion coefficient is not odds ratio\nlr_low_age$coefficients\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# regression coefficients can also be extracted in this way:\ncoefficients(lr_low_age)\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# coef(lr_low_age)\n\n# confidence interval for age and intercept\nconfint(lr_low_age)\n\n                 2.5 %      97.5 %\n(Intercept) -1.0336270 1.847399861\nage         -0.1150799 0.008986436\n\n\n\n# to get odds ratio, exponentiate \nexp(coef(lr_low_age))\n\n(Intercept)         age \n  1.4690000   0.9501333 \n\nexp(confint(lr_low_age))\n\n                2.5 %   97.5 %\n(Intercept) 0.3557144 6.343305\nage         0.8912949 1.009027"
  },
  {
    "objectID": "lab/ex_survival.html",
    "href": "lab/ex_survival.html",
    "title": "Exercises - Survival analysis",
    "section": "",
    "text": "Datasets\n\nExercise 1: melanoma(rda link, csv link)\nExercise 2: liggetid (rda link, csv link)\n\nR script\n\n\nExercise 1: Melanoma\nThe data concern 205 patients with malignant melanoma who were operated at the university hospital in Odense, Denmark, in the period 1962-77. The patients were followed up to death or censored at the end of the study. We shall study the effect on survival of the patient’s gender and tumor thickness.\nThe variables included in the dataset are:\n\nstatus (1=death from disease, 2=censored, 4=death from other causes)\nlifetime (years) from operation\nulceration of tumor (1=yes, 2=no)\ntumor thickness in 1/100 mm\ngender (f=1, m=2)\nage at operation in years\ngrouped tumor thickness (1: 0-1 mm, 2: 2-5 mm, 3: 5+ mm)\nlogarithm of tumor thickness\n\n\n1a)\nThe “event of interest” is defined as “death from disease”. Pre-process the variables that R will use for basic survival analysis as shown in class.\n\n\n1b)\nMake a Kaplan-Meier plot of the survival curve. What are the estimated probabilities of surviving 1 year, 2 years, 5 years?\n\n\n1c)\nCompare survival according to gender. Test the difference with a logrank test.\n\n\n1d)\nStudy survival according to grouped tumor thickness by comparing survival curves in each group and test with a logrank test.\n\n\n\nExercise 2: length of hospital stay\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n2a)\nAnalyze the relationship between the variables liggetid and kjoenn with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n\n2b)\nAnalyze the relationship between the variables liggetid and slag with a Kaplan-Meier plot. Test the difference with a log-rank test."
  },
  {
    "objectID": "lab/lab_logistic_reg.html",
    "href": "lab/lab_logistic_reg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Datasets\n\nExercise 1: birth (rda link, csv link)\nExercise 2: framingham\n\n[R Script]\n\n\nLogistic regression\n(Dataset birth might be switched out)\n\n# load data\nbirth_data <- haven::read_dta('data/birth.dta')\n\n# print the first rows of the data set\nhead(birth_data)\n\n# A tibble: 6 × 12\n     id low            age   lwt eth       smk   ptl ht      ui        fvt   ttv\n  <dbl> <dbl+lbl>    <dbl> <dbl> <dbl+l> <dbl> <dbl> <dbl+l> <dbl+l> <dbl> <dbl>\n1     4 1 [bwt < 25…    28   120 3 [oth…     1     1 0 [no]  1 [yes]     0     0\n2    10 1 [bwt < 25…    29   130 1 [whi…     0     0 0 [no]  1 [yes]     2     0\n3    11 1 [bwt < 25…    34   187 2 [bla…     1     0 1 [yes] 0 [no]      0     0\n4    13 1 [bwt < 25…    25   105 3 [oth…     0     1 1 [yes] 0 [no]      0     0\n5    15 1 [bwt < 25…    25    85 3 [oth…     0     0 0 [no]  1 [yes]     0     4\n6    16 1 [bwt < 25…    27   150 3 [oth…     0     0 0 [no]  0 [no]      0     5\n# … with 1 more variable: bwt <dbl>\n\n\n\nFit a logistic regression model for low and age\n\n# low ~ age \n\nlr_low_age <- glm(low ~ age, data = birth_data, family = 'binomial')\nsummary(lr_low_age)\n\n\nCall:\nglm(formula = low ~ age, family = \"binomial\", data = birth_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0402  -0.9018  -0.7754   1.4119   1.7800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# the result regresssion coefficient is not odds ratio\nlr_low_age$coefficients\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# regression coefficients can also be extracted in this way:\ncoefficients(lr_low_age)\n\n(Intercept)         age \n 0.38458192 -0.05115294 \n\n# coef(lr_low_age)\n\n# confidence interval for age and intercept\nconfint(lr_low_age)\n\n                 2.5 %      97.5 %\n(Intercept) -1.0336270 1.847399861\nage         -0.1150799 0.008986436\n\n\n\n# to get odds ratio, exponentiate \nexp(coef(lr_low_age))\n\n(Intercept)         age \n  1.4690000   0.9501333 \n\nexp(confint(lr_low_age))\n\n                2.5 %   97.5 %\n(Intercept) 0.3557144 6.343305\nage         0.8912949 1.009027"
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-1-blood-pressure",
    "href": "lab/ex_linearreg.html#exercise-1-blood-pressure",
    "title": "Linear regression",
    "section": "Exercise 1 (blood pressure)",
    "text": "Exercise 1 (blood pressure)\nThe dataset bp contains data on 20 healthy adults on two variables, Age and Blood pressure. We will explore the relationship between these two variables.\n\n1a)\nLoad the dataset. Find the correlation between age and blood pressure, and test if it is significant. Compute a 95% confidence interval for the regresion parameter.\nAlso find the squared correlation coefficient between age and blood pressure. What does it mean?\n\n\n1b)\nWhat is the blood pressure for a person at age 40? For a person at age 75? Comment."
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-2-lung-function",
    "href": "lab/ex_linearreg.html#exercise-2-lung-function",
    "title": "Linear regression",
    "section": "Exercise 2 (lung function)",
    "text": "Exercise 2 (lung function)\nLung function has been measured on 106 medical students. Peak expiratory flow rate (PEF, measured in liters per minute) was measured three times in a sittinng position, and three times in a standing position.\nThe variables are\n\nAge (years)\nGender (female, male)\nHeight (cm)\nWeight (kg)\nPEF measured three times in a sitting position (pefsit1, pefsit2, pefsit3)\nPEF measured three times in a standing position (pefsta1, pefsta2, pefsta3)\nMean of the three measurements made in a sitting position (pefsitm)\nMean of the three measurements made in a standing position (pefstam)\nMean of all six PEF values (pefmean)\n\n\n2a)\nMake a scatter plot of pefsit2 versus pefsit1; and make a separate scatter plot of pefsit1 versus weight. Insert a regression line on top of the scatterplots.\n\n\n2b)\nCompute the correlation between pefsit1 and pefsit2; and between pefsit1 and weight.\nWhy is the correlation between the first pair closer to 1 than the second pair?\n(You can get the pair-wise correlation between many other pairs of variables using cor(your_data)).\n\n\n2c)\nCarry out two regression analysis:\n\npefsit2 as dependent variable, pefsit1 as independent variable;\npefsit1 as dependent variable, weight as independent variable.\n\nInterpret the results in relation to the scatter plots.\n\n\n2d)\nMake residual analysis for the analyses you did before. Interpret the results.\n\n\n2e)\nMake a regression analysis with pefsit1 as dependent variable, and sex and weight as independent variables. Assess the model fit. Interpret the results.\n\n\n2f)\nMake a regression analysis with pefmean as dependent variable, and try out combinations of sex, height, weight as independent variables.\nWhich variables would you include in your final analysis? How much variation is explained? Assess the model fit, and interpret the results."
  },
  {
    "objectID": "lab/ex_linearreg.html#exercise-3-length-of-hospital-stay",
    "href": "lab/ex_linearreg.html#exercise-3-length-of-hospital-stay",
    "title": "Linear regression",
    "section": "Exercise 3 (length of hospital stay)",
    "text": "Exercise 3 (length of hospital stay)\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\n3a)\nCreate a box plot for length of hospital stay for men and women.\n\n\n3b)\nWe want to explain the variation in lengths of hospital stay. We will look at the independent variables kjoenn (gender) and slag (stroke).\nRun a regression analysis using the dependent variable liggetid. Also perform a residual analysis. What do you think about this analysis?\n\n\n3c)\nDo the same analysis, but on the log-transformed data. The transformed variable already exists in the dataset, lnliggti. Comment on the results."
  },
  {
    "objectID": "lab/ex_logisticreg.html#exercise-1-birth",
    "href": "lab/ex_logisticreg.html#exercise-1-birth",
    "title": "Logistic regression",
    "section": "Exercise 1 (birth)",
    "text": "Exercise 1 (birth)\nWe shall analyse the data set given in the file birth.dta. In a study in Massachusetts, USA, birth weight was measured for the children of 189 women. The main variable in the study was birth weight, BWT, which is an important indicator of the condition of a newborn child. Low birth weight (below 2500 g) may be a medical risk factor. A major question is whether smoking during pregnancy influences the birth weight. One has also studied whether a number of other factors are related to birth weight, such as hypertension in the mother.\nThe variables of the study are\n\nIdentification number (ID)\nLow birth weight\nAge of the mother (AGE)\nWeight (in pounds) at last menstruaal period (LWT)\nEthnicity, 1 means White, 2 means African American, 3 meaans other (ETH)\nSmoking status, 1 means current smoker, 0 means not smoking during pregnancy (SMK)\nHistory of premature labbour, values 0, 1, 2… (PTL)\nHistory of hypertension, 1 is yes, 0 is no (HT)\nUterine irritability, 1 is yes, 0 is no (UI)\nFirst trimester visits, values 0, 1, 2… (FTV)\nThird trimster visits, values 0, 1, 2… (TTV)\nBirth weight (BWT)\n\n\n1a)\nPerform a logistic regression analysis with low as dependent variable, and age as independent variable. Interpret the results.\n\n\n1b)\nPerform a logistic regression analysis with low as dependent variable and smk as independent variable.\n\n\n1c)\nPerform a logistic regression with low as dependent variable, and eth as independent variable. Be careful with the reference category.\n\n\n1d)\nPerform a logistic regression with low as dependent variable, and age,smk and eth as independent variables.\n\n\n1e)\nBased on the above analysis, set up a result table which reports:\n\nodds ratios OR (unadjusted, and adjusted)\n95% confidence intervals for OR\np-values for OR\n\nMake sure you know how to interpret the table."
  },
  {
    "objectID": "lab/ex_logisticreg.html#exercise-2-framingham",
    "href": "lab/ex_logisticreg.html#exercise-2-framingham",
    "title": "Logistic regression",
    "section": "Exercise 2 (framingham)",
    "text": "Exercise 2 (framingham)\nWe use the data from the Framingham study, framingham. The dataset contains a selection of n = 500 men aged 31 to 65 years.\nThe response variable is FIRSTCHD, and this is equal to 1 if the individual has coronary heart disease and 0 otherwise.\nWe have four explanatory variables:\n\nMEANSBP, the average systolic blood pressure (mmHg) of two blood pressure measurements;\nSMOKE which is smoking (1 = yes, 0 = no);\nCHOLESTEROL which is serum cholesterol in mg/dl;\nAGE (age in years).\n\n\n2a)\nAnalyse the relationship between firstchd and smoke in a logistic regression model.\n\n\n2b)\nAnalyse the relationship between firstchd and meansbp in a logistic regression model.\n\n\n2c)\nInclude also the other two explanatory variables in a logisic regression model. Interpret the results."
  },
  {
    "objectID": "lab/lab_eda_part2.html#some-advanced-visualisation-ggplot2",
    "href": "lab/lab_eda_part2.html#some-advanced-visualisation-ggplot2",
    "title": "Exploratory data analysis (Part II)",
    "section": "some advanced visualisation (ggplot2)",
    "text": "some advanced visualisation (ggplot2)"
  },
  {
    "objectID": "lab/lab_eda_part2.html#missing-data",
    "href": "lab/lab_eda_part2.html#missing-data",
    "title": "Exploratory data analysis (Part II)",
    "section": "missing data",
    "text": "missing data"
  },
  {
    "objectID": "lab/lab_survival.html#exercise-1-melanoma",
    "href": "lab/lab_survival.html#exercise-1-melanoma",
    "title": "Survival analysis",
    "section": "Exercise 1: Melanoma",
    "text": "Exercise 1: Melanoma\nThe data concern 205 patients with malignant melanoma who were operated at the university hospital in Odense, Denmark, in the period 1962-77. The patients were followed up to death or censored at the end of the study. We shall study the effect on survival of the patient’s gender and tumor thickness.\nThe variables included in the dataset are:\n\nstatus (1=death from disease, 2=censored, 4=death from other causes)\nlifetime (years) from operation\nulceration of tumor (1=yes, 2=no)\ntumor thickness in 1/100 mm\ngender (f=1, m=2)\nage at operation in years\ngrouped tumor thickness (1: 0-1 mm, 2: 2-5 mm, 3: 5+ mm)\nlogarithm of tumor thickness\n\nFirst we load the dataset, and check what columns are there.\n\n# if you do not have survival package, install it by \n# install.packages('survival')\nlibrary(survival)\n\nmelanoma <- read.csv('data/melanoma.csv')\n\nhead(melanoma)\n\n  status   lifetime ulceration tumor_thickness gender age\n1      4 0.02739726        Yes            6.76      m  76\n2      4 0.08219178         No            0.65      m  56\n3      2 0.09589041         No            1.34      m  41\n4      4 0.27123288         No            2.90      f  71\n5      1 0.50684932        Yes           12.08      m  52\n6      1 0.55890411        Yes            4.84      m  28\n  grouped_tumor_thickness logarithm_of_tumor_thickness\n1                   5+ mm                    1.9110229\n2                  0-2 mm                   -0.4307829\n3                  0-2 mm                    0.2926696\n4                  2-5 mm                    1.0647107\n5                   5+ mm                    2.4915512\n6                  2-5 mm                    1.5769147\n\ncolnames(melanoma)\n\n[1] \"status\"                       \"lifetime\"                    \n[3] \"ulceration\"                   \"tumor_thickness\"             \n[5] \"gender\"                       \"age\"                         \n[7] \"grouped_tumor_thickness\"      \"logarithm_of_tumor_thickness\"\n\n\n\n1a)\nThe “event of interest” is defined as “death from disease”. Pre-process the variables that R will use for basic survival analysis as shown in class.\nWe only need “death from melanoma” or else, hence we need to recode the status variable which originally had three categories.\n\n# if status == 1, code 1; otherwise, code 0\n# this indicates whether deaths from melanoma\ndeath <- ifelse(melanoma$status == 1, 1, 0)\n\n# check if the labels are correct\ntable(death)\n\ndeath\n  0   1 \n148  57 \n\ntable(melanoma$status)\n\n\n  1   2   4 \n 57 134  14 \n\n\nYou can see that the category melanoma$status == 1 should match death == 1, and melanoma$status of 2 and 4 are merged together and coded as death == 0.\n\n\n1b)\nMake a Kaplan-Meier plot of the survival curve. What are the estimated probabilities of surviving 1 year, 2 years, 5 years?\n\n# take lifetime variable\nlifetime <- melanoma$lifetime\n\nkm_fit <- survfit(Surv(lifetime, death) ~ 1)\nplot(km_fit)\n\n# add title and text\ntitle(main = 'Kaplan-Meier survival estimate', \n      xlab = 'Time', \n      ylab = 'Survival probability')\n\n\n\n\nNow we check only time 1, 2, 5.\n\n# time 1, 2, 5\ntme <- c(1, 2, 5)\nsummary(km_fit, times = tme)\n\nCall: survfit(formula = Surv(lifetime, death) ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    193       6    0.970  0.0120        0.947        0.994\n    2    183       9    0.925  0.0187        0.889        0.962\n    5    122      30    0.769  0.0303        0.712        0.831\n\n\n\n\n1c)\nCompare survival according to gender. Test the difference with a logrank test.\n\n# gender == 1: female; gender == 2: male\ngender <- melanoma$gender\n\nkm_fit_gender <- survfit(Surv(lifetime, death) ~ gender)\nplot(km_fit_gender, col = c('blue', 'red'))\ntitle(main = 'Kaplan-Meier survival estimate: stratify by gender', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'bottomleft', \n       legend = c('Female', 'Male'), \n       lty = c('solid', 'solid'), \n       col = c('blue','red'))\n\n\n\n\nNow we test the difference with log-rank test\n\n# test difference with logrank test \nsurvdiff(Surv(lifetime, death) ~ gender)\n\nCall:\nsurvdiff(formula = Surv(lifetime, death) ~ gender)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\ngender=f 126       28     37.1      2.25      6.47\ngender=m  79       29     19.9      4.21      6.47\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\n\n\n\n1d)\nStudy survival according to grouped tumor thickness by comparing survival curves in each group and test with a logrank test.\nFirst we do some data processing\n\n# take out the variable\ngrouped_tumor_thickness <- melanoma$grouped_tumor_thickness\n\n# get an idea how many categories\ntable(grouped_tumor_thickness)\n\ngrouped_tumor_thickness\n0-2 mm 2-5 mm  5+ mm \n   109     64     32 \n\n\nNow we plot the KM plot, and carry out a log-rank test\n\nkm_fit_tumor <- survfit(Surv(lifetime, death) ~ grouped_tumor_thickness)\nplot(km_fit_tumor, col = c('blue', 'red', 'forestgreen'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by tumor thickness', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'bottomleft', \n        legend = c('0-2 mm', '2-5 mm', '5+ mm'), \n        lty = c('solid', 'solid', 'solid'), \n        col = c('blue','red', 'forestgreen'))\n\n\n\n# test difference with logrank test \nsurvdiff(Surv(lifetime, death) ~ grouped_tumor_thickness)\n\nCall:\nsurvdiff(formula = Surv(lifetime, death) ~ grouped_tumor_thickness)\n\n                                 N Observed Expected (O-E)^2/E (O-E)^2/V\ngrouped_tumor_thickness=0-2 mm 109       13    33.75     12.75     31.36\ngrouped_tumor_thickness=2-5 mm  64       30    16.39     11.30     15.88\ngrouped_tumor_thickness=5+ mm   32       14     6.86      7.42      8.45\n\n Chisq= 31.6  on 2 degrees of freedom, p= 1e-07"
  },
  {
    "objectID": "lab/lab_survival.html#exercise-2-length-of-hospital-stay",
    "href": "lab/lab_survival.html#exercise-2-length-of-hospital-stay",
    "title": "Survival analysis",
    "section": "Exercise 2: Length of hospital stay",
    "text": "Exercise 2: Length of hospital stay\nThe data was collected at the Geriatric Department at Ullevål Sykehus. Below is a description of the data set liggetid. The file includes the following variables:\n\nYear of birth (faar)\nMonth of birth (fmaan)\nDay of birth (fdag)\nYear of hospital admission (innaar)\nMonth of admission (innmaan)\nDay of admission (inndag)\nYear of discharge from hospital (utaar)\nMonth of discharge (utmaan)\nDay of discharge (utdag)\nGender, where 1 = male and 0 = female (kjoenn)\nAdmission from, where 1 = home, 2 = Div. of Medicine, 3 = Div. of Surgery, 4 = Other division, 5 = Other hospital, 6 = Nursing home (kom_fra)\nStroke, where 1 = yes, 0 = no (slag)\nAge (alder)\nHospital stay, in days (liggetid)\nLogarithm of hospital stay (lnliggti)\nComes from Div. of Medicine (kom_fra2)\nComes from Div. of Surgery (kom_fra3)\nComes from Other division (kom_fra4)\nComes from Other hospital (kom_fra5)\nComes from Nursing home (kom_fra6)\nCensoring variable (censor)\n\nThe variable liggetid time is calculated from the innaar, innmaan, inndag, utaar, utmaan and utdag variables.\n\nliggetid <- read.csv('data/liggetid.csv')\nhead(liggetid)\n\n  faar fmaan fdag innaar innmaan inndag utaar utmaan utdag kjoenn kom_fra slag\n1 1906     3    4   1987       3      5    87      3    18 kvinne       1    0\n2 1891     4    3   1987       3      6    87      3    23 kvinne       1    0\n3 1908     9    6   1987       3     10    87      3    16 kvinne       1    0\n4 1910     1   11   1987       3     11    87      3    25   mann       1    1\n5 1907     1    6   1987       3     13    87      3    30 kvinne       1    0\n6 1901    12   19   1987       3     13    87      4     2 kvinne       1    0\n  alder liggetid lnliggti kom_fra2 kom_fra3 kom_fra4 kom_fra5 kom_fra6 status\n1    81       13 2.564949        0        0        0        0        0      1\n2    96       17 2.833213        0        0        0        0        0      1\n3    79        6 1.791759        0        0        0        0        0      1\n4    77       14 2.639057        0        0        0        0        0      1\n5    80       17 2.833213        0        0        0        0        0      1\n6    86       20 2.995732        0        0        0        0        0      1\n\ncolnames(liggetid)\n\n [1] \"faar\"     \"fmaan\"    \"fdag\"     \"innaar\"   \"innmaan\"  \"inndag\"  \n [7] \"utaar\"    \"utmaan\"   \"utdag\"    \"kjoenn\"   \"kom_fra\"  \"slag\"    \n[13] \"alder\"    \"liggetid\" \"lnliggti\" \"kom_fra2\" \"kom_fra3\" \"kom_fra4\"\n[19] \"kom_fra5\" \"kom_fra6\" \"status\"  \n\n# all status are 1 \nstatus <- liggetid$status\n\n\n2a)\nAnalyze the relationship between the variables liggetid and kjoenn with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n# length of stay (los) vs gender\nlos <- liggetid$liggetid\nhead(los)\n\n[1] 13 17  6 14 17 20\n\n# take out gender variable\ngender <- liggetid$kjoenn\nhead(gender)\n\n[1] \"kvinne\" \"kvinne\" \"kvinne\" \"mann\"   \"kvinne\" \"kvinne\"\n\n# fit km \nkm_liggetid_gender <- survfit(Surv(los, status) ~ gender)\n\nplot(km_liggetid_gender, col = c('blue', 'red'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by gender', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'topright', \n        legend = c('Female', 'Male'), \n        lty = c('solid', 'solid'), \n        col = c('blue','red'))\n\n\n\n\nNow we do log-rank test\n\n# log rank test \nsurvdiff(Surv(los, status) ~ gender)\n\nCall:\nsurvdiff(formula = Surv(los, status) ~ gender)\n\nn=1138, 1 observation deleted due to missingness.\n\n                N Observed Expected (O-E)^2/E (O-E)^2/V\ngender=kvinne 714      714      799      9.06      31.9\ngender=mann   424      424      339     21.36      31.9\n\n Chisq= 31.9  on 1 degrees of freedom, p= 2e-08 \n\n\n\n\n2b)\nAnalyze the relationship between the variables liggetid and slag with a Kaplan-Meier plot. Test the difference with a log-rank test.\n\n# length of stay vs stroke\n# slag == 1: yes, slag == 2: no\nstroke <- liggetid$slag\n\n# fit km \nkm_liggetid_stroke <- survfit(Surv(los, status) ~ stroke)\n\nplot(km_liggetid_stroke, col = c('blue', 'red'))\n\ntitle(main = 'Kaplan-Meier survival estimate: stratify by stroke', \n      xlab = 'Time', \n      ylab = 'Survial probability')\n\n# add legend\nlegend( 'topright', \n        legend = c('Stroke: yes', 'Stroke: no'), \n        lty = c('solid', 'solid'), \n        col = c('blue','red'))\n\n\n\n\n\n# log rank test\nsurvdiff(Surv(los, status) ~ stroke)\n\nCall:\nsurvdiff(formula = Surv(los, status) ~ stroke)\n\nn=1054, 85 observations deleted due to missingness.\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nstroke=0 891      891      864     0.814      4.61\nstroke=1 163      163      190     3.714      4.61\n\n Chisq= 4.6  on 1 degrees of freedom, p= 0.03"
  }
]